{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model name is: cardiffnlp/twitter-roberta-base-hate\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer\n",
        "from src.config import NUM_TYPES, NUM_MANIFESTATIONS, MODEL_NAMES\n",
        "from src.data import load_data, prepare_datasets\n",
        "from src.model import SharedMTLModel\n",
        "from src.metrics import compute_metrics, make_compute_metrics_fn\n",
        "from src.training import compute_pos_weights, get_training_args, get_early_stopping_callback\n",
        "from src.predict import predict_dev_set\n",
        "from src.logging_utils import log_experiment_results\n",
        "from src.thresholds import find_optimal_thresholds\n",
        "\n",
        "lang = \"eng\"\n",
        "trial_id = \"MTL_Thresholds_1\"\n",
        "model_name = MODEL_NAMES[-1]\n",
        "print(f\"model name is: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc190c2fdecd46a883333799397cc040",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac4f68e67a474d71b4d7ae4e401c9f6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83d5047e7dac4e8a9fe2873a88f772ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f78f8f7c9e747dabad2a0712e77219c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_1, train_2, train_3 = load_data(lang)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
        "train_dataset, val_dataset = prepare_datasets(train_1, train_2, train_3, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-hate and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "pos_weight_2 = compute_pos_weights(train_2, train_2.columns[2:])\n",
        "pos_weight_3 = compute_pos_weights(train_3, train_3.columns[2:])\n",
        "model = SharedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS, pos_weight_2, pos_weight_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [930/930 02:46, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Subtask 1/f1 Macro</th>\n",
              "      <th>Subtask 2/f1 Macro</th>\n",
              "      <th>Subtask 3/f1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.702832</td>\n",
              "      <td>0.784170</td>\n",
              "      <td>0.287911</td>\n",
              "      <td>0.483007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.634600</td>\n",
              "      <td>0.673045</td>\n",
              "      <td>0.806074</td>\n",
              "      <td>0.330054</td>\n",
              "      <td>0.483277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.518200</td>\n",
              "      <td>0.691198</td>\n",
              "      <td>0.809569</td>\n",
              "      <td>0.362043</td>\n",
              "      <td>0.486907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.414300</td>\n",
              "      <td>0.743279</td>\n",
              "      <td>0.821937</td>\n",
              "      <td>0.415100</td>\n",
              "      <td>0.513092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.357800</td>\n",
              "      <td>0.786966</td>\n",
              "      <td>0.827781</td>\n",
              "      <td>0.449159</td>\n",
              "      <td>0.515605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>0.852470</td>\n",
              "      <td>0.820208</td>\n",
              "      <td>0.417301</td>\n",
              "      <td>0.514900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.924248</td>\n",
              "      <td>0.825195</td>\n",
              "      <td>0.484550</td>\n",
              "      <td>0.526183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.263400</td>\n",
              "      <td>0.904617</td>\n",
              "      <td>0.832345</td>\n",
              "      <td>0.459740</td>\n",
              "      <td>0.519913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.242300</td>\n",
              "      <td>0.949772</td>\n",
              "      <td>0.822327</td>\n",
              "      <td>0.491964</td>\n",
              "      <td>0.523328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.246200</td>\n",
              "      <td>0.951907</td>\n",
              "      <td>0.824955</td>\n",
              "      <td>0.480425</td>\n",
              "      <td>0.521746</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results (before threshold optimization): \n",
            "subtask_1 f1_macro: 0.8323 \n",
            "subtask_2 f1_macro: 0.4597 \n",
            "subtask_3 f1_macro: 0.5199\n"
          ]
        }
      ],
      "source": [
        "training_args = get_training_args(trial_id)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    callbacks=[get_early_stopping_callback()]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\n",
        "    \"Validation Results (before threshold optimization):\",\n",
        "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
        "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
        "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b37d95d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal thresholds found:\n",
            "  Subtask 1: [0.20000000000000004]\n",
            "  Subtask 2: [0.1, 0.7000000000000002, 0.7500000000000002, 0.8000000000000002, 0.9000000000000002]\n",
            "  Subtask 3: [0.8000000000000002, 0.8000000000000002, 0.7500000000000002, 0.20000000000000004, 0.7500000000000002, 0.6000000000000002]\n",
            "\n",
            "Validation Results (after threshold optimization): \n",
            "subtask_1 f1_macro: 0.8329 \n",
            "subtask_2 f1_macro: 0.5174 \n",
            "subtask_3 f1_macro: 0.5499\n"
          ]
        }
      ],
      "source": [
        "val_predictions = trainer.predict(val_dataset)\n",
        "val_logits = val_predictions.predictions\n",
        "if isinstance(val_logits, tuple):\n",
        "    val_logits = val_logits[0]\n",
        "\n",
        "val_labels = val_predictions.label_ids\n",
        "thresholds = find_optimal_thresholds(val_logits, val_labels, NUM_TYPES, NUM_MANIFESTATIONS)\n",
        "\n",
        "print(\"Optimal thresholds found:\")\n",
        "print(f\"  Subtask 1: {thresholds['subtask_1']}\")\n",
        "print(f\"  Subtask 2: {thresholds['subtask_2']}\")\n",
        "print(f\"  Subtask 3: {thresholds['subtask_3']}\")\n",
        "\n",
        "eval_results_optimized = compute_metrics(val_predictions, thresholds=thresholds)\n",
        "eval_results_optimized = {f\"eval_{k}\": v for k, v in eval_results_optimized.items()}\n",
        "print(\n",
        "    \"\\nValidation Results (after threshold optimization):\",\n",
        "    f\"\\nsubtask_1 f1_macro: {eval_results_optimized['eval_subtask_1/f1_macro']:.4f}\",\n",
        "    f\"\\nsubtask_2 f1_macro: {eval_results_optimized['eval_subtask_2/f1_macro']:.4f}\",\n",
        "    f\"\\nsubtask_3 f1_macro: {eval_results_optimized['eval_subtask_3/f1_macro']:.4f}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Experiment results logged to logs.json (trial_id: MTL_Thresholds)\n",
            "  - subtask_1: eng\n",
            "  - subtask_2: eng\n",
            "  - subtask_3: eng\n"
          ]
        }
      ],
      "source": [
        "log_experiment_results(\n",
        "    eval_results_optimized,\n",
        "    trial_id,\n",
        "    lang,\n",
        "    model_name,\n",
        "    training_args,\n",
        "    NUM_TYPES,\n",
        "    NUM_MANIFESTATIONS,\n",
        "    thresholds=thresholds\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved for all 3 dev sets with Logical Gating and optimized thresholds applied.\n"
          ]
        }
      ],
      "source": [
        "output_1, output_2, output_3 = predict_dev_set(trainer, tokenizer, lang, trial_id, thresholds=thresholds)\n",
        "print(\"Predictions saved for all 3 dev sets with Logical Gating and optimized thresholds applied.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
