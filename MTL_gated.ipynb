{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf088005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from logs import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# CONFIG\n",
    "NUM_TYPES = 5\n",
    "NUM_MANIFESTATIONS = 6\n",
    "datasets_merge = False\n",
    "lang = \"eng\"\n",
    "trial_id = \"000002\"\n",
    "model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\"]\n",
    "model_name = model_names[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"./dev_phase/subtask1/train/\" + lang + \".csv\")\n",
    "train_2 = pd.read_csv(\"./dev_phase/subtask2/train/\" + lang + \".csv\")\n",
    "train_3 = pd.read_csv(\"./dev_phase/subtask3/train/\" + lang + \".csv\")\n",
    "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/\" + lang + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
    "    self.texts=texts\n",
    "    self.labels=labels\n",
    "    self.tokenizer= tokenizer\n",
    "    self.max_length = max_length # Store max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    text=self.texts[idx]\n",
    "    label=self.labels[idx]\n",
    "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
    "\n",
    "    # Ensure consistent tensor conversion for all items\n",
    "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "    item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "# Prepare label columns separately for each task, fallback to the correct columns per train DataFrame\n",
    "def get_label_columns(df):\n",
    "    return [col for col in df.columns if col not in ['id', 'text']]\n",
    "\n",
    "# Split indices once and reuse for all datasets to ensure same split\n",
    "n_samples = len(train_1)\n",
    "indices = np.arange(n_samples)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "if datasets_merge:\n",
    "    # Merge all datasets on 'id'\n",
    "    merged = train_1.merge(train_2, on=['id', 'text'], how='outer', suffixes=('_1', '_2'))\n",
    "    # For the third, avoid duplicate columns of 'text', so drop redundant one, or merge only on id\n",
    "    merged = merged.merge(train_3, on=['id', 'text'], how='outer', suffixes=('', '_3'))\n",
    "    # Get label columns: all columns excluding 'id' and 'text'\n",
    "    merged_label_columns = get_label_columns(merged)\n",
    "    texts = merged['text'].tolist()\n",
    "    labels = merged[merged_label_columns].values.tolist()\n",
    "    texts_train = [texts[i] for i in train_indices]\n",
    "    texts_val = [texts[i] for i in val_indices]\n",
    "    labels_train = [labels[i] for i in train_indices]\n",
    "    labels_val = [labels[i] for i in val_indices]\n",
    "    train_dataset = PolarizationDataset(texts_train, labels_train, tokenizer)\n",
    "    val_dataset = PolarizationDataset(texts_val, labels_val, tokenizer)\n",
    "else:\n",
    "    # Apply the same split to all three datasets\n",
    "    for train in [train_1, train_2, train_3]:\n",
    "        current_label_columns = get_label_columns(train)\n",
    "        texts = train['text'].tolist()\n",
    "        \n",
    "        # Use the same indices for all datasets\n",
    "        texts_train = [texts[i] for i in train_indices]\n",
    "        texts_val = [texts[i] for i in val_indices]\n",
    "        \n",
    "        if current_label_columns:\n",
    "            labels = train[current_label_columns].values.tolist()\n",
    "            labels_train = [labels[i] for i in train_indices]\n",
    "            labels_val = [labels[i] for i in val_indices]\n",
    "        else:\n",
    "            labels_train = [[] for _ in texts_train]\n",
    "            labels_val = [[] for _ in texts_val]\n",
    "        \n",
    "        train_datasets.append(PolarizationDataset(texts_train, labels_train, tokenizer))\n",
    "        val_datasets.append(PolarizationDataset(texts_val, labels_val, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500cca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMTLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_types, num_manifestations):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.num_types = num_types\n",
    "        self.num_manifestations = num_manifestations\n",
    "\n",
    "        self.head1 = nn.Linear(hidden_size, 1)\n",
    "        self.head2 = nn.Linear(hidden_size, num_types)\n",
    "        self.head3 = nn.Linear(hidden_size, num_manifestations)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        H = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits1 = self.head1(H)\n",
    "        gate = torch.sigmoid(logits1)\n",
    "        H_gated = H * gate\n",
    "\n",
    "        logits2 = self.head2(H_gated)\n",
    "        logits3 = self.head3(H_gated)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2, logits3], dim=-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            y1_true = labels[:, :1]\n",
    "            y2_true = labels[:, 1:1 + self.num_types]\n",
    "            y3_true = labels[:, 1 + self.num_types:]\n",
    "\n",
    "            loss1 = loss_fct(logits1, y1_true)\n",
    "            loss2 = loss_fct(logits2, y2_true)\n",
    "            loss3 = loss_fct(logits3, y3_true)\n",
    "\n",
    "            loss = (loss1 + loss2 + loss3) / 3.0\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"polarization_logits\": logits1,\n",
    "            \"types_logits\": logits2,\n",
    "            \"manifestations_logits\": logits3,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902a37ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea83ef1c5914cd5bf77fef92668b417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547df537625944f09e6a07a75b692d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GatedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    y1_true = labels[:, 0]\n",
    "    y1_pred = preds[:, 0]\n",
    "\n",
    "    y2_true = labels[:, 1:1+NUM_TYPES]\n",
    "    y2_pred = preds[:, 1:1+NUM_TYPES]\n",
    "\n",
    "    y3_true = labels[:, 1+NUM_TYPES:]\n",
    "    y3_pred = preds[:, 1+NUM_TYPES:]\n",
    "\n",
    "    return {\n",
    "        \"subtask_1/accuracy\": accuracy_score(y1_true, y1_pred),\n",
    "        \"subtask_1/f1_binary\": f1_score(y1_true, y1_pred, average=\"binary\", zero_division=0),\n",
    "        \"subtask_1/f1_macro\": f1_score(y1_true, y1_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_1/f1_micro\": f1_score(y1_true, y1_pred, average=\"micro\", zero_division=0),\n",
    "\n",
    "        \"subtask_2/f1_macro\": f1_score(y2_true, y2_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_2/f1_micro\": f1_score(y2_true, y2_pred, average=\"micro\", zero_division=0),\n",
    "\n",
    "        \"subtask_3/f1_macro\": f1_score(y3_true, y3_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_3/f1_micro\": f1_score(y3_true, y3_pred, average=\"micro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=f\"./\",\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=len(train_dataset) // 64,\n",
    "        disable_tqdm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b392db14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/410 03:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Subtask 1/accuracy</th>\n",
       "      <th>Subtask 1/f1 Binary</th>\n",
       "      <th>Subtask 1/f1 Macro</th>\n",
       "      <th>Subtask 1/f1 Micro</th>\n",
       "      <th>Subtask 2/f1 Macro</th>\n",
       "      <th>Subtask 2/f1 Micro</th>\n",
       "      <th>Subtask 3/f1 Macro</th>\n",
       "      <th>Subtask 3/f1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.463416</td>\n",
       "      <td>0.779845</td>\n",
       "      <td>0.710204</td>\n",
       "      <td>0.766352</td>\n",
       "      <td>0.779845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.431845</td>\n",
       "      <td>0.787597</td>\n",
       "      <td>0.722110</td>\n",
       "      <td>0.775107</td>\n",
       "      <td>0.787597</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.547486</td>\n",
       "      <td>0.188813</td>\n",
       "      <td>0.369892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.460585</td>\n",
       "      <td>0.747287</td>\n",
       "      <td>0.675944</td>\n",
       "      <td>0.734414</td>\n",
       "      <td>0.747287</td>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.549488</td>\n",
       "      <td>0.241772</td>\n",
       "      <td>0.423529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.481673</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.681363</td>\n",
       "      <td>0.740176</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.549488</td>\n",
       "      <td>0.247989</td>\n",
       "      <td>0.424188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.793798</td>\n",
       "      <td>0.671605</td>\n",
       "      <td>0.760661</td>\n",
       "      <td>0.793798</td>\n",
       "      <td>0.132283</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.349250</td>\n",
       "      <td>0.462701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.427450</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.757219</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.132984</td>\n",
       "      <td>0.509018</td>\n",
       "      <td>0.356120</td>\n",
       "      <td>0.462523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.434216</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.663462</td>\n",
       "      <td>0.751639</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.131633</td>\n",
       "      <td>0.506876</td>\n",
       "      <td>0.346226</td>\n",
       "      <td>0.459165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.466363</td>\n",
       "      <td>0.778295</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.757228</td>\n",
       "      <td>0.778295</td>\n",
       "      <td>0.135198</td>\n",
       "      <td>0.531136</td>\n",
       "      <td>0.372141</td>\n",
       "      <td>0.468156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.470431</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.679912</td>\n",
       "      <td>0.753337</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.134906</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.354561</td>\n",
       "      <td>0.448306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.478152</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.753833</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.140043</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.354242</td>\n",
       "      <td>0.444255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: \n",
      "subtask_1 accuracy: 0.7752 \n",
      "subtask_1 f1_binary: 0.6813 \n",
      "subtask_1 f1_macro: 0.7538 \n",
      "subtask_1 f1_micro: 0.7752 \n",
      "subtask_2 f1_macro: 0.1400 \n",
      "subtask_2 f1_micro: 0.5263 \n",
      "subtask_3 f1_macro: 0.3542 \n",
      "subtask_3 f1_micro: 0.4443\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\n",
    "    \"Validation Results:\",\n",
    "    f\"\\nsubtask_1 accuracy: {eval_results['eval_subtask_1/accuracy']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_binary: {eval_results['eval_subtask_1/f1_binary']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_micro: {eval_results['eval_subtask_1/f1_micro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_micro: {eval_results['eval_subtask_2/f1_micro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_micro: {eval_results['eval_subtask_3/f1_micro']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7c00c",
   "metadata": {},
   "source": [
    "# Log Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4ddef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Experiment results logged to logs.json (trial_id: 000002)\n",
      "  - subtask_1: eng\n",
      "  - subtask_2: eng\n",
      "  - subtask_3: eng\n"
     ]
    }
   ],
   "source": [
    "# Log the experiment results - each subtask separately\n",
    "\n",
    "\n",
    "# Prepare metadata for the experiment\n",
    "experiment_metadata = {\n",
    "    \"approach\": \"MTL_Gated\",\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,\n",
    "    \"num_types\": NUM_TYPES,\n",
    "    \"num_manifestations\": NUM_MANIFESTATIONS,\n",
    "    \"datasets_merge\": datasets_merge,\n",
    "}\n",
    "\n",
    "# Extract metrics for each subtask\n",
    "subtask_1_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_accuracy\": eval_results.get(\"eval_subtask_1/accuracy\"),\n",
    "    \"eval_f1_binary\": eval_results.get(\"eval_subtask_1/f1_binary\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_1/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_1/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "subtask_2_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_2/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_2/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "subtask_3_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_3/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_3/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "# To respect pre-existing metadata, update it INSTEAD of replacing it\n",
    "import json\n",
    "\n",
    "# Attempt to load existing logs and merge metadata for this trial if present\n",
    "existing_metadata = {}\n",
    "try:\n",
    "    with open(\"logs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        logs = json.load(f)\n",
    "        if isinstance(logs, dict):\n",
    "            logs = [logs]\n",
    "        for trial in logs:\n",
    "            if trial.get(\"trial_id\") == trial_id and \"metadata\" in trial:\n",
    "                existing_metadata = trial[\"metadata\"].copy()\n",
    "                break\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    pass\n",
    "\n",
    "# Only add/replace model_{lang}, don't overwrite the whole metadata\n",
    "merged_metadata = dict(existing_metadata)\n",
    "merged_metadata.update({\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"approach\": experiment_metadata[\"approach\"],\n",
    "    \"learning_rate\": experiment_metadata[\"learning_rate\"],\n",
    "    \"num_train_epochs\": experiment_metadata[\"num_train_epochs\"],\n",
    "    \"per_device_train_batch_size\": experiment_metadata[\"per_device_train_batch_size\"],\n",
    "    \"per_device_eval_batch_size\": experiment_metadata[\"per_device_eval_batch_size\"],\n",
    "    \"num_types\": experiment_metadata[\"num_types\"],\n",
    "    \"num_manifestations\": experiment_metadata[\"num_manifestations\"],\n",
    "    \"datasets_merge\": experiment_metadata[\"datasets_merge\"]\n",
    "})\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_1\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_1_results,\n",
    "    metadata=merged_metadata,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "# Log subtask_2 and subtask_3 using the same trial_id and do not pass metadata to avoid overwrite\n",
    "log(\n",
    "    subtask_name=\"subtask_2\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_2_results,\n",
    "    metadata=None,  # Don't overwrite metadata\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_3\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_3_results,\n",
    "    metadata=None,  # Don't overwrite metadata\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Experiment results logged to logs.json (trial_id: {trial_id})\")\n",
    "print(f\"  - subtask_1: {lang}\")\n",
    "print(f\"  - subtask_2: {lang}\")\n",
    "print(f\"  - subtask_3: {lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654cdde",
   "metadata": {},
   "source": [
    "# Predict on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe02703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved for all 3 dev sets\n"
     ]
    }
   ],
   "source": [
    "# Load dev 1 and predict all 3 dev sets\n",
    "dev_1 = pd.read_csv(f\"./dev_phase/subtask1/dev/{lang}.csv\")\n",
    "dev_2 = pd.read_csv(f\"./dev_phase/subtask2/dev/{lang}.csv\")\n",
    "dev_3 = pd.read_csv(f\"./dev_phase/subtask3/dev/{lang}.csv\")\n",
    "\n",
    "# Create dataset from dev 1 texts (all dev sets have same texts)\n",
    "dev_texts = dev_1['text'].tolist()\n",
    "dev_dataset = PolarizationDataset(dev_texts, [[0]*12]*len(dev_texts), tokenizer)\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.predict(dev_dataset)\n",
    "logits = predictions.predictions\n",
    "if isinstance(logits, tuple):\n",
    "    logits = logits[0]\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Extract predictions for each subtask\n",
    "polarization_preds = preds[:, 0]\n",
    "types_preds = preds[:, 1:1+NUM_TYPES]\n",
    "manifestations_preds = preds[:, 1+NUM_TYPES:]\n",
    "\n",
    "# Create output DataFrames\n",
    "output_1 = dev_1[['id', 'text']].copy()\n",
    "output_1['polarization'] = polarization_preds\n",
    "\n",
    "output_2 = dev_2[['id', 'text']].copy()\n",
    "type_cols = [col for col in dev_2.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(type_cols):\n",
    "    output_2[col] = types_preds[:, i]\n",
    "\n",
    "output_3 = dev_3[['id', 'text']].copy()\n",
    "manifest_cols = [col for col in dev_3.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(manifest_cols):\n",
    "    output_3[col] = manifestations_preds[:, i]\n",
    "\n",
    "\n",
    "# Drop the 'text' column before saving\n",
    "output_1 = output_1.drop(columns=['text'])\n",
    "output_2 = output_2.drop(columns=['text'])\n",
    "output_3 = output_3.drop(columns=['text'])\n",
    "\n",
    "# Create dir under results with trial_id\n",
    "import os\n",
    "os.makedirs(f\"./results/{trial_id}\", exist_ok=True)\n",
    "# Create 3 dirs under it: subtask_1, subtask_2, subtask_3\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_1\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_2\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_3\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Save predictions to subtask_ directories, using the lang parameter instead of hard coding\n",
    "output_1.to_csv(f\"./results/{trial_id}/subtask_1/pred_{lang}.csv\", index=False)\n",
    "output_2.to_csv(f\"./results/{trial_id}/subtask_2/pred_{lang}.csv\", index=False)\n",
    "output_3.to_csv(f\"./results/{trial_id}/subtask_3/pred_{lang}.csv\", index=False)\n",
    "\n",
    "print(f\"Predictions saved for all 3 dev sets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
