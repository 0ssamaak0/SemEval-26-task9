{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf088005",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from logging import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2508329d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")\n",
        "\n",
        "# CONFIG\n",
        "NUM_TYPES = 5\n",
        "NUM_MANIFESTATIONS = 6\n",
        "datasets_merge = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6407ec79",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df_1 = pd.read_csv(\"./dev_phase/subtask1/train/arb.csv\")\n",
        "train_df_2 = pd.read_csv(\"./dev_phase/subtask2/train/arb.csv\")\n",
        "train_df_3 = pd.read_csv(\"./dev_phase/subtask3/train/arb.csv\")\n",
        "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/arb.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e844f3e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4288712",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_1 = pd.read_csv('./dev_phase/subtask1/train/eng.csv')\n",
        "train_2 = pd.read_csv('./dev_phase/subtask2/train/eng.csv')\n",
        "train_3 = pd.read_csv('./dev_phase/subtask3/train/eng.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09f26488",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
        "    self.texts=texts\n",
        "    self.labels=labels\n",
        "    self.tokenizer= tokenizer\n",
        "    self.max_length = max_length # Store max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
        "\n",
        "    # Ensure consistent tensor conversion for all items\n",
        "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "    item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8900bea6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Load the tokenizer\n",
        "model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\"]\n",
        "model_name = model_names[0]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "# Prepare label columns separately for each task, fallback to the correct columns per train DataFrame\n",
        "def get_label_columns(df):\n",
        "    return [col for col in df.columns if col not in ['id', 'text']]\n",
        "\n",
        "# Split indices once and reuse for all datasets to ensure same split\n",
        "n_samples = len(train_1)\n",
        "indices = np.arange(n_samples)\n",
        "train_indices, val_indices = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "if datasets_merge:\n",
        "    # Merge all datasets on 'id'\n",
        "    merged = train_1.merge(train_2, on=['id', 'text'], how='outer', suffixes=('_1', '_2'))\n",
        "    # For the third, avoid duplicate columns of 'text', so drop redundant one, or merge only on id\n",
        "    merged = merged.merge(train_3, on=['id', 'text'], how='outer', suffixes=('', '_3'))\n",
        "    # Get label columns: all columns excluding 'id' and 'text'\n",
        "    merged_label_columns = get_label_columns(merged)\n",
        "    texts = merged['text'].tolist()\n",
        "    labels = merged[merged_label_columns].values.tolist()\n",
        "    texts_train = [texts[i] for i in train_indices]\n",
        "    texts_val = [texts[i] for i in val_indices]\n",
        "    labels_train = [labels[i] for i in train_indices]\n",
        "    labels_val = [labels[i] for i in val_indices]\n",
        "    train_dataset = PolarizationDataset(texts_train, labels_train, tokenizer)\n",
        "    val_dataset = PolarizationDataset(texts_val, labels_val, tokenizer)\n",
        "else:\n",
        "    # Apply the same split to all three datasets\n",
        "    for train in [train_1, train_2, train_3]:\n",
        "        current_label_columns = get_label_columns(train)\n",
        "        texts = train['text'].tolist()\n",
        "        \n",
        "        # Use the same indices for all datasets\n",
        "        texts_train = [texts[i] for i in train_indices]\n",
        "        texts_val = [texts[i] for i in val_indices]\n",
        "        \n",
        "        if current_label_columns:\n",
        "            labels = train[current_label_columns].values.tolist()\n",
        "            labels_train = [labels[i] for i in train_indices]\n",
        "            labels_val = [labels[i] for i in val_indices]\n",
        "        else:\n",
        "            labels_train = [[] for _ in texts_train]\n",
        "            labels_val = [[] for _ in texts_val]\n",
        "        \n",
        "        train_datasets.append(PolarizationDataset(texts_train, labels_train, tokenizer))\n",
        "        val_datasets.append(PolarizationDataset(texts_val, labels_val, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500cca39",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GatedMTLModel(nn.Module):\n",
        "    def __init__(self, model_name, num_types, num_manifestations):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        self.num_types = num_types\n",
        "        self.num_manifestations = num_manifestations\n",
        "\n",
        "        self.head1 = nn.Linear(hidden_size, 1)\n",
        "        self.head2 = nn.Linear(hidden_size, num_types)\n",
        "        self.head3 = nn.Linear(hidden_size, num_manifestations)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        H = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        logits1 = self.head1(H)\n",
        "        gate = torch.sigmoid(logits1).detach()\n",
        "        H_gated = H * gate\n",
        "\n",
        "        logits2 = self.head2(H_gated)\n",
        "        logits3 = self.head3(H_gated)\n",
        "\n",
        "        logits = torch.cat([logits1, logits2, logits3], dim=-1)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.float()\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "            y1_true = labels[:, :1]\n",
        "            y2_true = labels[:, 1:1 + self.num_types]\n",
        "            y3_true = labels[:, 1 + self.num_types:]\n",
        "\n",
        "            loss1 = loss_fct(logits1, y1_true)\n",
        "            loss2 = loss_fct(logits2, y2_true)\n",
        "            loss3 = loss_fct(logits3, y3_true)\n",
        "\n",
        "            loss = (loss1 + loss2 + loss3) / 3.0\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "            \"polarization_logits\": logits1,\n",
        "            \"types_logits\": logits2,\n",
        "            \"manifestations_logits\": logits3,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "902a37ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GatedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    y1_true = labels[:, 0]\n",
        "    y1_pred = preds[:, 0]\n",
        "\n",
        "    y2_true = labels[:, 1:1+NUM_TYPES]\n",
        "    y2_pred = preds[:, 1:1+NUM_TYPES]\n",
        "\n",
        "    y3_true = labels[:, 1+NUM_TYPES:]\n",
        "    y3_pred = preds[:, 1+NUM_TYPES:]\n",
        "\n",
        "    return {\n",
        "        \"subtask_1/accuracy\": accuracy_score(y1_true, y1_pred),\n",
        "        \"subtask_1/f1_binary\": f1_score(y1_true, y1_pred, average=\"binary\", zero_division=0),\n",
        "        \"subtask_1/f1_macro\": f1_score(y1_true, y1_pred, average=\"macro\", zero_division=0),\n",
        "        \"subtask_1/f1_micro\": f1_score(y1_true, y1_pred, average=\"micro\", zero_division=0),\n",
        "\n",
        "        \"subtask_2/f1_macro\": f1_score(y2_true, y2_pred, average=\"macro\", zero_division=0),\n",
        "        \"subtask_2/f1_micro\": f1_score(y2_true, y2_pred, average=\"micro\", zero_division=0),\n",
        "\n",
        "        \"subtask_3/f1_macro\": f1_score(y3_true, y3_pred, average=\"macro\", zero_division=0),\n",
        "        \"subtask_3/f1_micro\": f1_score(y3_true, y3_pred, average=\"micro\", zero_division=0),\n",
        "    }\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=f\"./\",\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=64,\n",
        "        per_device_eval_batch_size=8,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=100,\n",
        "        disable_tqdm=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536c287c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 50/123 04:25 < 06:43, 0.18 it/s, Epoch 1.20/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Subtask 1/accuracy</th>\n",
              "      <th>Subtask 1/f1 Binary</th>\n",
              "      <th>Subtask 1/f1 Macro</th>\n",
              "      <th>Subtask 1/f1 Micro</th>\n",
              "      <th>Subtask 2/f1 Macro</th>\n",
              "      <th>Subtask 2/f1 Micro</th>\n",
              "      <th>Subtask 3/f1 Macro</th>\n",
              "      <th>Subtask 3/f1 Micro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.405967</td>\n",
              "      <td>0.364341</td>\n",
              "      <td>0.534091</td>\n",
              "      <td>0.267045</td>\n",
              "      <td>0.364341</td>\n",
              "      <td>0.006639</td>\n",
              "      <td>0.023055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        "    data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Validation Results:\\nAccuracy: {eval_results['eval_accuracy']:.4f}\\nPrecision: {eval_results['eval_precision']:.4f}\\nRecall: {eval_results['eval_recall']:.4f}\\nF1 (binary): {eval_results['eval_f1_binary']:.4f}\\nF1 (macro): {eval_results['eval_f1_macro']:.4f}\\nF1 (micro): {eval_results['eval_f1_micro']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd5f21b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
