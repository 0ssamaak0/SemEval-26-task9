[
    {
        "trial_id": "B000000",
        "metadata": {
            "approach": "bert",
            "model_eng": "bert-base-uncased",
            "model_arb": "bert-base-uncased",
            "learning_rate": 2e-05,
            "num_train_epochs": 3,
            "id_on_site": 447161
        },
        "subtask_1": {
            "score": 0.68,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.756
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.65
                }
            }
        },
        "subtask_2": {
            "score": 0.03,
            "eng": {
                "score": 0.187,
                "eval_results": {
                    "eval_f1_macro": 0.268
                }
            },
            "arb": {
                "score": 0.0324,
                "eval_results": {
                    "eval_f1_macro": 0.045
                }
            }
        },
        "subtask_3": {
            "score": 0.21,
            "eng": {
                "score": 0.3161,
                "eval_results": {
                    "eval_f1_macro": 0.317
                }
            },
            "arb": {
                "score": 0.2119,
                "eval_results": {
                    "eval_f1_macro": 0.287
                }
            }
        }
    },
    {
        "trial_id": "B000001",
        "metadata": {
            "approach": "bert",
            "model_eng": "microsoft/deberta-v3-base",
            "model_arb": "UBC-NLP/MARBERTv2",
            "learning_rate": 2e-05,
            "num_train_epochs": 3,
            "id_on_site": 447912
        },
        "subtask_1": {
            "score": 0.68,
            "eng": {
                "score": 0.74,
                "eval_results": {
                    "eval_f1_macro": 0.756
                }
            },
            "arb": {
                "score": 0.5963,
                "eval_results": {
                    "eval_f1_macro": 0.8
                }
            }
        },
        "subtask_2": {
            "score": 0.03,
            "eng": {
                "score": 0.187,
                "eval_results": {
                    "eval_f1_macro": 0.268
                }
            },
            "arb": {
                "score": 0.0324,
                "eval_results": {
                    "eval_f1_macro": 0.61
                }
            }
        },
        "subtask_3": {
            "score": 0.4,
            "eng": {
                "score": 0.2344,
                "eval_results": {
                    "eval_f1_macro": 0.317
                }
            },
            "arb": {
                "score": 0.4016,
                "eval_results": {
                    "eval_f1_macro": 0.486
                }
            }
        }
    },
    {
        "trial_id": "GT000001",
        "metadata": {
            "approach": "MTL_Gated",
            "model_eng": "bert-base-uncased",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 64,
            "per_device_eval_batch_size": 8,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_arb": "bert-base-uncased",
            "id_on_site": 447014
        },
        "subtask_1": {
            "score": 0.66,
            "eng": {
                "score": 0.7718,
                "eval_results": {
                    "eval_f1_macro": 0.74693544282321
                }
            },
            "arb": {
                "score": 0.6597,
                "eval_results": {
                    "eval_f1_macro": 0.7106716973180833
                }
            }
        },
        "subtask_2": {
            "score": 0.06,
            "eng": {
                "score": 0.1612,
                "eval_results": {
                    "eval_f1_macro": 0.1649313318428617
                }
            },
            "arb": {
                "score": 0.0646,
                "eval_results": {
                    "eval_f1_macro": 0.1143635724331927
                }
            }
        },
        "subtask_3": {
            "score": 0.26,
            "eng": {
                "score": 0.2978,
                "eval_results": {
                    "eval_f1_macro": 0.2938462815302812
                }
            },
            "arb": {
                "score": 0.2553,
                "eval_results": {
                    "eval_f1_macro": 0.3068775328177798
                }
            }
        }
    },
    {
        "trial_id": "GT000002",
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_Gated",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 64,
            "per_device_eval_batch_size": 8,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_eng": "microsoft/deberta-v3-base",
            "id_on_site": 447043
        },
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8090775936998496
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7538329933539514
                }
            }
        },
        "subtask_2": {
            "score": 0.38,
            "arb": {
                "score": 0.3843,
                "eval_results": {
                    "eval_f1_macro": 0.5169923091301157
                }
            },
            "eng": {
                "score": 0.1426,
                "eval_results": {
                    "eval_f1_macro": 0.14004294801669298
                }
            }
        },
        "subtask_3": {
            "score": 0.37,
            "arb": {
                "score": 0.3697,
                "eval_results": {
                    "eval_f1_macro": 0.432488336994267
                }
            },
            "eng": {
                "score": 0.367,
                "eval_results": {
                    "eval_f1_macro": 0.3542418115939625
                }
            }
        }
    },
    {
        "trial_id": "DSP0001S",
        "metadata": {
            "approach": "dspy basic",
            "model": "ollama_chat/gemma3:12b",
            "id_on_site": 447959
        },
        "subtask_1": {
            "score": 0.75,
            "eng": {
                "score": 0.74,
                "eval_results": {
                    "eval_f1_macro": 0.7735584489912719
                }
            },
            "arb": {
                "score": 0.76,
                "eval_results": {
                    "eval_f1_macro": 0.7747937467482595
                }
            }
        }
    },
    {
        "trial_id": "0000NG1",
        "subtask_1": {
            "score": 0.8,
            "arb": {
                "score": 0.803,
                "eval_results": {
                    "eval_f1_macro": 0.8222222222222222
                }
            },
            "eng": {
                "score": 0.782,
                "eval_results": {
                    "eval_f1_macro": 0.7916488358673776
                }
            }
        },
        "metadata": {
            "id_on_site": 447952,
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_eng": "microsoft/deberta-v3-base"
        },
        "subtask_2": {
            "arb": {
                "score": 0.2808,
                "eval_results": {
                    "eval_f1_macro": 0.3943146759286401
                }
            },
            "eng": {
                "score": 0.1441,
                "eval_results": {
                    "eval_f1_macro": 0.1407035175879397
                }
            }
        },
        "subtask_3": {
            "score": 0.33,
            "arb": {
                "score": 0.3269,
                "eval_results": {
                    "eval_f1_macro": 0.38416407256783786
                }
            },
            "eng": {
                "score": 0.18,
                "eval_results": {
                    "eval_f1_macro": 0.1922514619883041
                }
            }
        }
    },
    {
        "trial_id": "MIPRODSP0001S",
        "metadata": {
            "approach": "dspy MIPROv2",
            "student_model": "ollama_chat/gemma3:12b",
            "teacher_model": "openai/gpt-5.1-2025-11-13"
        },
        "subtask_1": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7784664830119375
                }
            }
        }
    },
    {
        "trial_id": "DSP0002S",
        "metadata": {
            "approach": "dspy basic",
            "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-20b",
            "id_on_site": 449736
        },
        "subtask_1": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8091876305061152
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8166420966420966
                }
            }
        },
        "subtask_2": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5274590137479565
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.476800717013483
                }
            }
        },
        "subtask_3": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4037261292133856
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.4947564335231264
                }
            }
        }
    },
    {
        "trial_id": "0000NG2",
        "subtask_1": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7917077175697866
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8340347923681257
                }
            }
        },
        "metadata": {
            "model_eng": "FacebookAI/xlm-roberta-large",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_arb": "UBC-NLP/MARBERTv2",
            "id_on_site": 449058
        },
        "subtask_2": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.35815935837575
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6332211687061601
                }
            }
        },
        "subtask_3": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4747274959278143
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6057450663352554
                }
            }
        }
    },
    {
        "trial_id": "MIPRODSP0002S",
        "metadata": {
            "approach": "dspy MIPROv2",
            "student_model": "fireworks_ai/accounts/fireworks/models/gpt-oss-20b",
            "teacher_model": "openai/gpt-5.1-2025-11-13",
            "id_on_site": 449751
        },
        "subtask_1": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8166420966420966
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8423265768601285
                }
            }
        },
        "subtask_3": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4037261292133856
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.4783349521383498
                }
            }
        },
        "subtask_2": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5331215409620789
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4016325605124426
                }
            }
        }
    },
    {
        "trial_id": "DSP0003S",
        "metadata": {
            "approach": "dspy basic",
            "model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-instruct"
        },
        "subtask_1": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7404206610387752
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.7668639343859736
                }
            }
        }
    },
    {
        "trial_id": "MIPRODSP0003S",
        "metadata": {
            "approach": "dspy MIPROv2",
            "student_model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-instruct",
            "teacher_model": "openai/gpt-5.1-2025-11-13"
        },
        "subtask_1": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7611729833952057
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.7668639343859736
                }
            }
        }
    },
    {
        "trial_id": "DSP0004S",
        "metadata": {
            "approach": "dspy basic",
            "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-120b",
            "id_on_site": 449053
        },
        "subtask_1": {
            "score": 0.77,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8244568590937307
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8189682126107347
                }
            }
        },
        "subtask_2": {
            "score": 0.54,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4141102756892231
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5788326615912822
                }
            }
        },
        "subtask_3": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5438393939508607
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.46022704221867455
                }
            }
        }
    },
    {
        "trial_id": "MIPRODSP0004S",
        "metadata": {
            "approach": "dspy MIPROv2",
            "student_model": "fireworks_ai/accounts/fireworks/models/gpt-oss-120b",
            "teacher_model": "openai/gpt-5.1-2025-11-13",
            "id_on_site": 449228
        },
        "subtask_1": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8331957060280759
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.823905109489051
                }
            }
        },
        "subtask_2": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.33861755146262185
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5650762076982693
                }
            }
        },
        "subtask_3": {
            "score": null,
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5020813107304671
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.46022704221867455
                }
            }
        }
    },
    {
        "trial_id": "0000NG2LEGO",
        "subtask_1": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7821079786253915
                }
            }
        },
        "metadata": {
            "model_eng": "0ssamaak0/roberta-base-LEGO_emotions",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true
        },
        "subtask_2": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.27787095465457273
                }
            }
        },
        "subtask_3": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4845081760460899
                }
            }
        }
    },
    {
        "trial_id": "0000NG3",
        "subtask_1": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.7824338349766476
                }
            }
        },
        "metadata": {
            "model_eng": "FacebookAI/roberta-base",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true
        },
        "subtask_2": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.3555946257837248
                }
            }
        },
        "subtask_3": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4895205057043228
                }
            }
        }
    },
    {
        "trial_id": "DSP0002SPRO",
        "metadata": {
            "approach": "dspy basic",
            "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-20b",
            "id_on_site": 449782
        },
        "subtask_2": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.46280423280423283
                }
            }
        }
    },
    {
        "trial_id": "ensemble",
        "metadata": {
            "approach": "no gate ensemble",
            "model_1": "marbert",
            "model_2": "arbert",
            "safe mixture": false,
            "id_on_site": 452557
        },
        "subtask_1": {
            "score": 0.69
        },
        "subtask_2": {
            "score": 0.48
        },
        "subtask_3": {
            "score": 0.4
        }
    },
    {
        "trial_id": "MIPRODSP0002SPRO",
        "metadata": {
            "approach": "dspy MIPROv2",
            "student_model": "fireworks_ai/accounts/fireworks/models/gpt-oss-20b",
            "teacher_model": "openai/gpt-5.1-2025-11-13"
        },
        "subtask_2": {
            "score": null,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.46280423280423283
                }
            }
        }
    },
    {
        "trial_id": "GT000003",
        "subtask_1": {
            "eng": {
                "eval_results": {
                    "eval_loss": 0.44293421506881714,
                    "eval_accuracy": null,
                    "eval_f1_binary": null,
                    "eval_f1_macro": 0.7958779336480286,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.4268,
                    "eval_samples_per_second": 452.061,
                    "eval_steps_per_second": 28.736,
                    "epoch": 15.0
                }
            },
            "arb": {
                "eval_results": {
                    "eval_loss": 0.46620678901672363,
                    "eval_accuracy": null,
                    "eval_f1_binary": null,
                    "eval_f1_macro": 0.8164073451079339,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.0006,
                    "eval_samples_per_second": 675.606,
                    "eval_steps_per_second": 42.975,
                    "epoch": 15.0
                }
            }
        },
        "metadata": {
            "model_eng": "microsoft/deberta-v3-base",
            "approach": "MTL_Gated",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_arb": "UBC-NLP/MARBERTv2",
            "id_on_site": 450377
        },
        "subtask_2": {
            "eng": {
                "eval_results": {
                    "eval_loss": 0.44293421506881714,
                    "eval_f1_macro": 0.14299065420560747,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.4268,
                    "eval_samples_per_second": 452.061,
                    "eval_steps_per_second": 28.736,
                    "epoch": 15.0
                }
            },
            "arb": {
                "eval_results": {
                    "eval_loss": 0.46620678901672363,
                    "eval_f1_macro": 0.6075830494043218,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.0006,
                    "eval_samples_per_second": 675.606,
                    "eval_steps_per_second": 42.975,
                    "epoch": 15.0
                }
            }
        },
        "subtask_3": {
            "eng": {
                "eval_results": {
                    "eval_loss": 0.44293421506881714,
                    "eval_f1_macro": 0.3834128055180687,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.4268,
                    "eval_samples_per_second": 452.061,
                    "eval_steps_per_second": 28.736,
                    "epoch": 15.0
                }
            },
            "arb": {
                "eval_results": {
                    "eval_loss": 0.46620678901672363,
                    "eval_f1_macro": 0.5298895339718985,
                    "eval_f1_micro": null,
                    "eval_runtime": 1.0006,
                    "eval_samples_per_second": 675.606,
                    "eval_steps_per_second": 42.975,
                    "epoch": 15.0
                }
            }
        }
    },
    {
        "trial_id": "MARBERTAUG_FULL",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.9484126984126984
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8339082431957979
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "id_on_site": 452660,
            "model_eng": "cardiffnlp/twitter-roberta-base"
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5629565217391305
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.43077061850162474
                }
            }
        },
        "subtask_3": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6913576761402848
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5250862554181749
                }
            }
        }
    },
    {
        "trial_id": "ARBERTAUG_FULL",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8792802262432113
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/ARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "id_on_site": 452622
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.7062058892380343
                }
            }
        },
        "subtask_3": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6458400333640993
                }
            }
        }
    },
    {
        "trial_id": "MARBERTAUG_80",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8826049827965012
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 15,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.704304780164591
                }
            }
        },
        "subtask_3": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6501506679125233
                }
            }
        }
    },
    {
        "trial_id": "MTL_10epochs_full",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.840214308225654
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.822230615658555
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "model_eng": "cardiffnlp/twitter-roberta-base-hate-latest",
            "id_on_site": 454370
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5551501728147148
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.503232208713393
                }
            }
        },
        "subtask_3": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.5733462758890745
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.4827627244559791
                }
            }
        }
    },
    {
        "trial_id": "MTL_10epochs_full_1",
        "subtask_1": {
            "eng": {
                "score": 0.73,
                "eval_results": {
                    "eval_f1_macro": 0.8041954042708941
                }
            }
        },
        "metadata": {
            "model_eng": "vinai/bertweet-base",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "id_on_site": 454390
        },
        "subtask_2": {
            "eng": {
                "score": 0.3,
                "eval_results": {
                    "eval_f1_macro": 0.3411265697524055
                }
            }
        },
        "subtask_3": {
            "eng": {
                "score": 0.45,
                "eval_results": {
                    "eval_f1_macro": 0.5001444640225094
                }
            }
        }
    },
    {
        "trial_id": "MTL_10epochs_full_2",
        "subtask_1": {
            "score": 0.8,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.840696564885496
                }
            }
        },
        "metadata": {
            "model_eng": "cardiffnlp/twitter-roberta-base-hate",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": true,
            "id_on_site": 454405
        },
        "subtask_2": {
            "score": 0.41,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.40108720414567484
                }
            }
        },
        "subtask_3": {
            "score": 0.5,
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5165629730973345
                }
            }
        }
    },
    {
        "trial_id": "MTL_10epochs_full_soft_gating",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8865896727426932
                }
            },
            "eng": {
                "eval_results": {
                    "score": 78.2,
                    "eval_f1_macro": 0.8460580912863072
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": null,
            "model_eng": "cardiffnlp/twitter-roberta-base-hate",
            "trial_id": 454419
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.7099370078740157
                }
            },
            "eng": {
                "eval_results": {
                    "score": 41.6,
                    "eval_f1_macro": 0.5060956477732793
                }
            }
        },
        "subtask_3": {
            "arb": {
                "score": 57.0,
                "eval_results": {
                    "eval_f1_macro": 0.6486800511905745
                }
            },
            "eng": {
                "score": 49.84,
                "eval_results": {
                    "eval_f1_macro": 0.5452917144762014
                }
            }
        }
    },
    {
        "trial_id": "MTL_Thresholds",
        "subtask_1": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8865896727426932
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8328909705958887
                }
            }
        },
        "metadata": {
            "model_arb": "UBC-NLP/MARBERTv2",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": null,
            "model_eng": "cardiffnlp/twitter-roberta-base-hate",
            "id_on_site": 454419,
            "thresholds": {
                "subtask_1": [
                    0.20000000000000004
                ],
                "subtask_2": [
                    0.1,
                    0.7000000000000002,
                    0.7500000000000002,
                    0.8000000000000002,
                    0.9000000000000002
                ],
                "subtask_3": [
                    0.8000000000000002,
                    0.8000000000000002,
                    0.7500000000000002,
                    0.20000000000000004,
                    0.7500000000000002,
                    0.6000000000000002
                ]
            }
        },
        "subtask_2": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.7099370078740157
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5174202382245812
                }
            }
        },
        "subtask_3": {
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6486800511905745
                }
            },
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5499493888923438
                }
            }
        }
    },
    {
        "trial_id": "MTL_Thresholds_1",
        "subtask_1": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.8368483132814344
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.8760949195731804
                }
            }
        },
        "metadata": {
            "model_eng": "cardiffnlp/twitter-roberta-base-hate",
            "approach": "MTL_no_gate",
            "learning_rate": 2e-05,
            "num_train_epochs": 10,
            "per_device_train_batch_size": 32,
            "per_device_eval_batch_size": 16,
            "num_types": 5,
            "num_manifestations": 6,
            "datasets_merge": null,
            "id_on_site": 454490,
            "thresholds": {
                "subtask_1": [
                    0.3
                ],
                "subtask_2": [
                    0.65,
                    0.65,
                    0.8,
                    0.55,
                    0.6
                ],
                "subtask_3": [
                    0.35,
                    0.35,
                    0.6,
                    0.5,
                    0.55,
                    0.6
                ]
            },
            "model_arb": "UBC-NLP/MARBERTv2"
        },
        "subtask_2": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5086829260166259
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6997458019162542
                }
            }
        },
        "subtask_3": {
            "eng": {
                "eval_results": {
                    "eval_f1_macro": 0.5410468548504098
                }
            },
            "arb": {
                "eval_results": {
                    "eval_f1_macro": 0.6514737259454034
                }
            }
        }
    }
]