{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
      "metadata": {
        "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697"
      },
      "source": [
        "# Bert baseline for POLAR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
      "metadata": {
        "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this part of the starter notebook, we will take you through the process of all three Subtasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aIt64l96d4TR",
      "metadata": {
        "id": "aIt64l96d4TR"
      },
      "source": [
        "## Subtask 1 - Polarization detection\n",
        "\n",
        "This is a binary classification to determine whether a post contains polarized content (Polarized or Not Polarized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e5w0WSE89hdW",
      "metadata": {
        "id": "e5w0WSE89hdW"
      },
      "outputs": [],
      "source": [
        "# !unzip dev_phase.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
      "metadata": {
        "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
      "metadata": {
        "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "UkC2r47nManC",
      "metadata": {
        "id": "UkC2r47nManC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/nis248p6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x31374bcb0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dac92ad-3fac-4aa1-aae6-1fe3ad14b1c0",
      "metadata": {
        "id": "4dac92ad-3fac-4aa1-aae6-1fe3ad14b1c0"
      },
      "source": [
        "## Data Import\n",
        "\n",
        "The training data consists of a short text and binary labels\n",
        "\n",
        "The data is structured as a CSV file with the following fields:\n",
        "- id: a unique identifier for the sample\n",
        "- text: a sentence or short text\n",
        "- polarization:  1 text is polarized, 0 text is not polarized\n",
        "\n",
        "The data is in all three subtask folders the same but only containing the labels for the specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e4fe8cc2-ba47-4240-bdd2-b1d3ea323134",
      "metadata": {
        "id": "e4fe8cc2-ba47-4240-bdd2-b1d3ea323134"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>polarization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1350</th>\n",
              "      <td>eng_f308ca27dccc22549e50f1042ceb1df8</td>\n",
              "      <td>And where did I say h8 or xenophobia?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2605</th>\n",
              "      <td>eng_b1d3f6b9a86d738b0dbe5a5d891f79ef</td>\n",
              "      <td>WOW bad will the RedWAVE bloodbath be for the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1754</th>\n",
              "      <td>eng_5836d1c4db446e1a501fe30fb1b2615c</td>\n",
              "      <td>Breitbart is racist trash, for revealing Racis...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>eng_789191ae3e9c3a9c6de6565559820379</td>\n",
              "      <td>Israeli Bedouins, lacking government protectio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2700</th>\n",
              "      <td>eng_737e402f83d6f36f2023195aecac92df</td>\n",
              "      <td>As Joe Biden leaves the White House today, I r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        id  \\\n",
              "1350  eng_f308ca27dccc22549e50f1042ceb1df8   \n",
              "2605  eng_b1d3f6b9a86d738b0dbe5a5d891f79ef   \n",
              "1754  eng_5836d1c4db446e1a501fe30fb1b2615c   \n",
              "634   eng_789191ae3e9c3a9c6de6565559820379   \n",
              "2700  eng_737e402f83d6f36f2023195aecac92df   \n",
              "\n",
              "                                                   text  polarization  \n",
              "1350              And where did I say h8 or xenophobia?             0  \n",
              "2605  WOW bad will the RedWAVE bloodbath be for the ...             1  \n",
              "1754  Breitbart is racist trash, for revealing Racis...             1  \n",
              "634   Israeli Bedouins, lacking government protectio...             0  \n",
              "2700  As Joe Biden leaves the White House today, I r...             0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the training and validation data for subtask 1\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('./dev_phase/subtask1/train/eng.csv')\n",
        "\n",
        "# Split into train (80%) and val (20%), stratify if possible\n",
        "train, val = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data[\"polarization\"] if \"polarization\" in data.columns else None\n",
        ")\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eQWDFio83z9g",
      "metadata": {
        "id": "eQWDFio83z9g"
      },
      "source": [
        "# Dataset\n",
        "-  Create a pytorch class for handling data\n",
        "-  Wrapping the raw texts and labels into a format that Huggingfaceâ€™s Trainer can use for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e749d08",
      "metadata": {
        "id": "8e749d08"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
        "    self.texts=texts\n",
        "    self.labels=labels\n",
        "    self.tokenizer= tokenizer\n",
        "    self.max_length = max_length # Store max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
        "\n",
        "    # Ensure consistent tensor conversion for all items\n",
        "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "    return item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfb74a3",
      "metadata": {
        "id": "adfb74a3"
      },
      "source": [
        "Now, we'll tokenize the text data and create the datasets using `bert-base-uncased` as the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2e6215b",
      "metadata": {
        "id": "c2e6215b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train['polarization'].tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val['polarization'].tolist(), tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13caa5c",
      "metadata": {
        "id": "b13caa5c"
      },
      "source": [
        "Next, we'll load the pre-trained `bert-base-uncased` model for sequence classification. Since this is a binary classification task (Polarized/Not Polarized), we set `num_labels=2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7cd0411f",
      "metadata": {
        "id": "7cd0411f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b250d61e",
      "metadata": {
        "id": "b250d61e"
      },
      "source": [
        "Now, we'll define the training arguments and the evaluation metric. We'll use macro F1 score for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21c97269",
      "metadata": {
        "id": "21c97269"
      },
      "outputs": [],
      "source": [
        "# Define metrics function\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=f\"./\",\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=64,\n",
        "        per_device_eval_batch_size=8,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=100,\n",
        "        disable_tqdm=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20167a2b",
      "metadata": {
        "id": "20167a2b"
      },
      "source": [
        "Finally, we'll initialize the `Trainer` and start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ecd9f444",
      "metadata": {
        "id": "ecd9f444"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 21/123 02:10 < 11:41, 0.15 it/s, Epoch 0.49/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInsufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\n",
            "\t<AGXG15XFamilyCommandBuffer: 0x36cae3590>\n",
            "    label = <none> \n",
            "    device = <AGXG15SDevice: 0x366818600>\n",
            "        name = Apple M3 Pro \n",
            "    commandQueue = <AGXG15XFamilyCommandQueue: 0x309c8fa00>\n",
            "        label = <none> \n",
            "        device = <AGXG15SDevice: 0x366818600>\n",
            "            name = Apple M3 Pro \n",
            "    retainedReferences = 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "MPS backend out of memory (MPS allocated: 15.46 GiB, other allocations: 7.14 GiB, max allowed: 22.64 GiB). Tried to allocate 183.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     model=model,                         \u001b[38;5;66;03m# the instantiated ðŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[32m      4\u001b[39m     args=training_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     data_collator=DataCollatorWithPadding(tokenizer) \u001b[38;5;66;03m# Data collator for dynamic padding\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[32m     15\u001b[39m eval_results = trainer.evaluate()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1077\u001b[39m, in \u001b[36mDebertaV2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1069\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1075\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1089\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:784\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    774\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    776\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    777\u001b[39m     input_ids=input_ids,\n\u001b[32m    778\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:657\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    655\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    667\u001b[39m         all_attentions = all_attentions + (attn_weights,)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:436\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    429\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    435\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    445\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:369\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    362\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    368\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    378\u001b[39m         query_states = hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:250\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    249\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     attention_scores = attention_scores + rel_att\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.13/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:321\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_attention_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mc2p\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_att_type:\n\u001b[32m    320\u001b[39m     scale = scaled_size_sqrt(pos_key_layer, scale_factor)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     c2p_att = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_key_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     c2p_pos = torch.clamp(relative_pos + att_span, \u001b[32m0\u001b[39m, att_span * \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m)\n\u001b[32m    323\u001b[39m     c2p_att = torch.gather(\n\u001b[32m    324\u001b[39m         c2p_att,\n\u001b[32m    325\u001b[39m         dim=-\u001b[32m1\u001b[39m,\n\u001b[32m    326\u001b[39m         index=c2p_pos.squeeze(\u001b[32m0\u001b[39m).expand([query_layer.size(\u001b[32m0\u001b[39m), query_layer.size(\u001b[32m1\u001b[39m), relative_pos.size(-\u001b[32m1\u001b[39m)]),\n\u001b[32m    327\u001b[39m     )\n",
            "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 15.46 GiB, other allocations: 7.14 GiB, max allowed: 22.64 GiB). Tried to allocate 183.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
          ]
        }
      ],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        "    data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea47953",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20eb4a7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:02<00:00, 84.40it/s] \n"
          ]
        }
      ],
      "source": [
        "test_dataset = pd.read_csv('./dev_phase/subtask1/dev/eng.csv')\n",
        "labels = []\n",
        "probs_list = []\n",
        "labels = []\n",
        "for text in tqdm(test_dataset['text']):\n",
        "    # Run the model\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_label = logits.argmax(dim=1).cpu().numpy()[0]\n",
        "        labels.append(pred_label)\n",
        "        probs_list.append(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb65325",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arb_67be47e5216d7bee41e17484e619f4e6,1\n",
            "arb_272322e5b265e177613d685e5619e402,0\n",
            "arb_d1ec38dd0ec5d7a4fe28ef8317fc96c1,0\n",
            "arb_fad75310b17c124d98ebc514189ec033,1\n",
            "arb_95caf70cec5bf00c94c35cf7af2a0ab5,1\n",
            "arb_ac108c1ecf5071892c61abd253847b15,1\n",
            "arb_adaaa6d482119e65ce337ee224674e70,1\n",
            "arb_2794b08cac6cc9394a68c51cfc436243,1\n",
            "arb_19dd96c989323c9e950a2c3ab9c285be,1\n",
            "arb_f2bd638d9d9fc7a617130ff2b198b562,1\n",
            "arb_f992bf7776b854d4f7f8475aebf80f49,1\n",
            "arb_0b5ac70e86926f5e84cad94028864a37,1\n",
            "arb_8ababf95f952e2425c2df1033192dac0,0\n",
            "arb_06cd19aac6cc52e394a22d7d1dd58efc,0\n",
            "arb_12eeeb8d2fa2d04be2ed9830d5f36ce9,0\n",
            "arb_5bc23bacf9a161cd0f99719c70681a81,1\n",
            "arb_9ee7c931ab1ecd655533042d8301f6bb,0\n",
            "arb_bb7c40559f3a7ca1ecdd7dd7c136198f,0\n",
            "arb_5d394c0cce56675e2fc36a0590b47ed7,0\n",
            "arb_0704305e8313650e672563a2d073384f,0\n",
            "arb_e56b759d14fd70506e01cf971315453d,0\n",
            "arb_d286a2aac63432acef285a4799041f55,0\n",
            "arb_5a9f322a530e85cd640f21af5c6bae42,1\n",
            "arb_133c4737c4e04a8991fc7c219106b4e4,1\n",
            "arb_f03fd416ab63cbb8f9f92a020fdd46be,1\n",
            "arb_b8fbf2253ba4b8a83829b964012d1a9b,0\n",
            "arb_4188caa3678846451bb812d7a7902459,1\n",
            "arb_c0224b694b38805f15fae4137121302e,1\n",
            "arb_3991e487126513829ec24ab5c1f1e325,1\n",
            "arb_9861662780205d38780ea04feed598dd,0\n",
            "arb_2b14abe9527a935117ae771612afca21,1\n",
            "arb_925eb03f72f22102e242a475f1f54530,0\n",
            "arb_c531bdc9a777137d8aa23a3bf529f87e,1\n",
            "arb_51a6eb82ab53edd30fdd18b6940dc275,0\n",
            "arb_6aa01210c44d11393ea419d6781b89a9,0\n",
            "arb_0c8c4d1e368fa6081c8840c3924b3711,1\n",
            "arb_f15fdbdb119f76f430f7cf7c773c3590,1\n",
            "arb_9b66d2e70ea93d6d37d0b656c1b1e880,1\n",
            "arb_773241d9837d36ed8962eacefa5143ef,1\n",
            "arb_b8f51143c06fbaed500ee2e7e7f5ac14,1\n",
            "arb_99dc675d6e1f9acdc7714fa066b4242b,1\n",
            "arb_3bcca38054a14240945f770aebe23355,1\n",
            "arb_730fba0ced42edd151f49218959abcdc,1\n",
            "arb_1a960ef19b8ebbeba5d8a48f2da3baf7,0\n",
            "arb_60a8455969e0277c96a31b2ab8f57bb6,0\n",
            "arb_8e46849f4110205740552b9b3d8afcc8,0\n",
            "arb_6535199985cd1f867d14c01e1780ed45,0\n",
            "arb_90448d504fbf1cdf02fe55dd474078fe,0\n",
            "arb_650c8d322f6c2ea8be78385ac29ad57d,0\n",
            "arb_8e08d6c50f1bc16d8aa16c4fcc461702,0\n",
            "arb_aafb2df7e4108d4d09ffefa74811ea2e,0\n",
            "arb_3a9070f93fa0aed9b4c7d7974d6134fe,0\n",
            "arb_a25e888067bd682f8712cc254a94450d,0\n",
            "arb_69b3fac31a9776e98df1f2ed3a37f743,0\n",
            "arb_70d64fdc7707ba7b23590bc3987a22bc,0\n",
            "arb_3561147626932826873155b2cb70e11e,1\n",
            "arb_757e07f4b35078992f4e9656cb4ea948,0\n",
            "arb_954a7a38de550d4b5252483bb9b748ea,1\n",
            "arb_a9847cdfa1abb179e11c85955a5f49bf,0\n",
            "arb_46b4284e3ace41c2806a041162aa1a39,0\n",
            "arb_8e5e356a5c81496169940989d545c6ac,0\n",
            "arb_c30e1b90992cee20688833ca1e382de0,0\n",
            "arb_0340513e0b537d1db0934fc235651218,0\n",
            "arb_726332fa6d54c653d55f5c7b42deb977,0\n",
            "arb_88c88e64e71c1e1b9222a49458d448ee,0\n",
            "arb_031d376aa7923c4268ac73dc2b9334a9,0\n",
            "arb_1cbab69c6f39bfd5484bd5832ece0ac0,0\n",
            "arb_ce785dd8c5b8140363e2e057dae241ce,0\n",
            "arb_60d2f094428bbcbde294086a7f75d5d9,0\n",
            "arb_28552d3893a580bf74bb1e4711c30e29,0\n",
            "arb_7e021fb631c018eb7a1dc96bb3b7d3f5,0\n",
            "arb_1429c399c30de8d7188cdd239c91aaa6,0\n",
            "arb_1a1c75b0f7b1b571c157dd83243f81c6,0\n",
            "arb_143932e3ea47fd0d8d45ff79c8e9a529,0\n",
            "arb_c0a60d5d5cb40fbd604c9c3e0b3098bf,0\n",
            "arb_13636b23e853783423f760db617520a9,0\n",
            "arb_6c8048c166f7ee2e8a94ca3650335345,1\n",
            "arb_8530294a4c91bae6bcbc9d7888b8d183,0\n",
            "arb_c4c60186c0b797433e86f3f08cd77e7b,1\n",
            "arb_2de8255707465f99180bf1c58204816b,0\n",
            "arb_52f0037e1b1196f8462aba17ce30b0bd,0\n",
            "arb_05a11d7f973c5b33f51a53a92a08c9f7,0\n",
            "arb_cfdb8f2486dbb02404f69c104509e956,0\n",
            "arb_119473be95ebd1ff087a086a8cdf67f0,0\n",
            "arb_06aa7b6d64858e1ea54c8d5c093a1385,0\n",
            "arb_58683c41f6320b304993fbc9ce8e559c,0\n",
            "arb_ae60c3911c8290e556ffdfa0e73ea408,0\n",
            "arb_c6624ecf4359e15fe5298b8878a53630,0\n",
            "arb_2228fe1e68bf10ac10d74c8f24ed672f,0\n",
            "arb_0d027dd29b767b3cb54141a67da833a8,0\n",
            "arb_f5345aeade0452ce211afa3b91a2a98a,0\n",
            "arb_fb4217f3ac08ddbbe93b87607d9e7eea,0\n",
            "arb_6eaa4e896b8b2056308fba14b04d1323,0\n",
            "arb_0c49ebfdc0e82c39fb952a55a2eec2a6,1\n",
            "arb_d380557a1e7c162fbf0bf8bb8cb7c374,0\n",
            "arb_445262028526e3ac19d4a7617b89312f,0\n",
            "arb_5078361bd8c9205ae1abd0f554b413a8,0\n",
            "arb_d8fc83089c56da4eaa4ea5dc1c8385cd,1\n",
            "arb_e203ea25f0d5899540684c600c27e313,0\n",
            "arb_8af35c96dd0ab3abdd243488210d41fb,1\n",
            "arb_80e0c1d020bb3e68c57092a5da055789,1\n",
            "arb_e9afce419a9fd57fd8fde9130084d86f,1\n",
            "arb_4208815a310227400e1d9b718aa3201b,1\n",
            "arb_f78e2ee76eaf751e219c0a6c24f17477,1\n",
            "arb_9444af3fa26415a2e1a1215b618b77fc,1\n",
            "arb_8ca54749e8a4cb192d605e8d0fed5077,1\n",
            "arb_985e458b817a04e5b4b856bdd66baad1,1\n",
            "arb_2ae6d2826d3f741c81074893b7368e8f,0\n",
            "arb_752cbd7c5fa526b02871ee2174b75260,1\n",
            "arb_d4e513d177a18cd0361907f0abcee264,0\n",
            "arb_3fd0b4dcbfd27d815c7c1600ed523acc,0\n",
            "arb_a5714ea7faf17fb64bce39550b8f66a0,1\n",
            "arb_dbb306bb98f778a984aa4e89a2a0c69c,1\n",
            "arb_8fe3d20ecd2bffca8608ad581e1d450d,1\n",
            "arb_4d3ed82d7641e459b2143327303141ca,1\n",
            "arb_2fe0dd13d24583051d76f1efe2a6b221,1\n",
            "arb_328246a090fa79fae208734fb506356a,1\n",
            "arb_fa9bfd0d2544a6a0da7e9fdf67dcb61a,1\n",
            "arb_eca8c2a903579f5bc09bbae9a914192b,1\n",
            "arb_ae665012d4d85ec2b7f67036e8e7ef91,0\n",
            "arb_7b2074b411c5991657d956f2e868d0d6,0\n",
            "arb_a2d10460956f12da3bfad87d75c17b85,0\n",
            "arb_68d1cbebcef21c8b7e157d3deb68c96a,0\n",
            "arb_2320e4a90c9a3e16ad43013187628bf0,0\n",
            "arb_03c025ab5aac30fb1b817698060f88e9,0\n",
            "arb_28a4a6fa8a0d9478a678ead7d87141aa,0\n",
            "arb_26bf97302d3bbf29dc55628a7cdce3b2,0\n",
            "arb_6c9c12fd45b04bafdda96d36b4abf654,0\n",
            "arb_717eb9eb752dde1f2ba0b1f75431ecb9,0\n",
            "arb_22ad0f62586145ee322627cc04737aed,1\n",
            "arb_9965f78f084c4b52eba5ad3eaf00c383,0\n",
            "arb_230ff60adcb94e26bb447d4d2d2d4d87,1\n",
            "arb_de5e97574cb90756816e4acad906cbcb,1\n",
            "arb_b930858672f01ab4e30913d1a42b2fdd,1\n",
            "arb_9e735136b6578cfa2e617c9686b060c6,1\n",
            "arb_2f940f44c8882b16eedd16ceb9a22f3a,0\n",
            "arb_63bba78965d711655b703211386a1d66,0\n",
            "arb_796f4f5016b6b057af6e4d9a931be707,1\n",
            "arb_35b4ec4ee73ff5865897b530f86a0df2,1\n",
            "arb_54d246af610d9df0efc222e9f5f568ee,1\n",
            "arb_c03c23fb168af6d9974173940301a49e,1\n",
            "arb_04c10b142c039bea31abc347a05e901e,1\n",
            "arb_fc3a3dada9c28b31f4501b1aff617009,1\n",
            "arb_db683247365c64679c4cc519af44f581,1\n",
            "arb_aecc5b9082fe643e2b8d6f0ac0dcdfce,1\n",
            "arb_f17c38194db34a072fb9ffa06a3c1a36,1\n",
            "arb_00ba726a4ccda40d3aa9970b605643d0,1\n",
            "arb_466691d07d329912a743f07951be4b63,1\n",
            "arb_32df687e0801943c203047322024a082,1\n",
            "arb_5956bff1d7e2d783e032bbb840bc0b0c,0\n",
            "arb_8d9f1d3fdd08dccf0fe640b32ee3c75d,0\n",
            "arb_2f9508e688439711aadf9ffe4c7dc43c,0\n",
            "arb_ea84b8ff4b701131207666ad68d9b4e6,0\n",
            "arb_c3baa08c705605f300bacbfd7b32cdb5,0\n",
            "arb_968554a6b5723334ac89acd80d35f5f3,0\n",
            "arb_63849e6cf2a4b209a3a7c88cf689c09f,0\n",
            "arb_7c14ba01ef8cdc91ee907afe06c6d57b,1\n",
            "arb_d90e03b791ee0a44e1ff732b3502cff2,1\n",
            "arb_a614e47120a6cc5ab2c22d062f799d1d,1\n",
            "arb_d0bec4c3e835773e91bade24082405dc,0\n",
            "arb_54d3d10c58ab60fe3bc8135cec8a7745,1\n",
            "arb_3a6d9f8b7468345e5fe80353682ea2b6,1\n",
            "arb_a74cb5b1ac1b5e1e3297ec29040104f7,0\n",
            "arb_902c5ae816b9323758ed8e33424c5427,1\n",
            "arb_7d86f321f6d638c75df7ed4645bdabbc,0\n",
            "arb_452c08a3d812c6786509a33f13bdca7a,1\n",
            "arb_fee827860874f8bf04cfdaca1a0bff7a,0\n",
            "arb_d072529aace0ca637b85d8bb968577c0,0\n",
            "arb_e564370d0a844c67763dafdd2ede0411,1\n"
          ]
        }
      ],
      "source": [
        "# print the results row by row in csv format\n",
        "for i in range(len(labels)):\n",
        "    print(f\"{test_dataset['id'][i]},{labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Qi53eheGIbu",
      "metadata": {
        "id": "5Qi53eheGIbu"
      },
      "source": [
        "# Subtask 2: Polarization Type Classification\n",
        "Multi-label classification to identify the target of polarization as one of the following categories: Gender/Sexual, Political, Religious, Racial/Ethnic, or Other.\n",
        "For this task we will load the data for subtask 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaWIvnYv0rV2",
      "metadata": {
        "id": "YaWIvnYv0rV2"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('./dev_phase/subtask2/train/eng.csv')\n",
        "\n",
        "# Split into train (80%) and val (20%), stratify if possible\n",
        "train, val = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I6S2S6QBDKzw",
      "metadata": {
        "id": "I6S2S6QBDKzw"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1_KYsG68nxI",
      "metadata": {
        "id": "u1_KYsG68nxI"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "dev_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdiYJyr08bw2",
      "metadata": {
        "id": "cdiYJyr08bw2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=5, problem_type=\"multi_label_classification\") # 5 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArVWKwze2mtS",
      "metadata": {
        "id": "ArVWKwze2mtS"
      },
      "outputs": [],
      "source": [
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qd3QPyfc2RKE",
      "metadata": {
        "id": "Qd3QPyfc2RKE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1014' max='1014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1014/1014 03:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.320600</td>\n",
              "      <td>0.294996</td>\n",
              "      <td>0.327454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.242300</td>\n",
              "      <td>0.261739</td>\n",
              "      <td>0.586681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.201000</td>\n",
              "      <td>0.257001</td>\n",
              "      <td>0.610804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 score on validation set for Subtask 2: 0.6108039208443157\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 2: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8536deba",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:02<00:00, 81.45it/s] \n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "test_dataset = pd.read_csv('./dev_phase/subtask2/dev/eng.csv')\n",
        "labels = []\n",
        "probs_list = []\n",
        "labels = []\n",
        "for text in tqdm(test_dataset['text']):\n",
        "    # Run the model\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "        pred_label = (probabilities > 0.5).astype(int)\n",
        "        labels.append(pred_label)\n",
        "        probs_list.append(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c951d74e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arb_67be47e5216d7bee41e17484e619f4e6,0,0,0,0,1\n",
            "arb_272322e5b265e177613d685e5619e402,0,0,0,0,0\n",
            "arb_d1ec38dd0ec5d7a4fe28ef8317fc96c1,0,0,0,0,0\n",
            "arb_fad75310b17c124d98ebc514189ec033,1,0,0,1,1\n",
            "arb_95caf70cec5bf00c94c35cf7af2a0ab5,0,1,0,1,0\n",
            "arb_ac108c1ecf5071892c61abd253847b15,1,0,0,0,1\n",
            "arb_adaaa6d482119e65ce337ee224674e70,0,1,0,1,0\n",
            "arb_2794b08cac6cc9394a68c51cfc436243,1,0,0,0,0\n",
            "arb_19dd96c989323c9e950a2c3ab9c285be,0,0,0,0,0\n",
            "arb_f2bd638d9d9fc7a617130ff2b198b562,0,1,0,1,0\n",
            "arb_f992bf7776b854d4f7f8475aebf80f49,0,1,1,0,0\n",
            "arb_0b5ac70e86926f5e84cad94028864a37,0,0,0,0,1\n",
            "arb_8ababf95f952e2425c2df1033192dac0,0,0,0,0,0\n",
            "arb_06cd19aac6cc52e394a22d7d1dd58efc,0,0,0,0,0\n",
            "arb_12eeeb8d2fa2d04be2ed9830d5f36ce9,0,0,0,0,0\n",
            "arb_5bc23bacf9a161cd0f99719c70681a81,0,0,0,0,1\n",
            "arb_9ee7c931ab1ecd655533042d8301f6bb,0,0,0,0,0\n",
            "arb_bb7c40559f3a7ca1ecdd7dd7c136198f,0,0,0,0,0\n",
            "arb_5d394c0cce56675e2fc36a0590b47ed7,0,0,0,0,0\n",
            "arb_0704305e8313650e672563a2d073384f,0,0,0,0,0\n",
            "arb_e56b759d14fd70506e01cf971315453d,0,0,0,0,0\n",
            "arb_d286a2aac63432acef285a4799041f55,0,0,0,0,0\n",
            "arb_5a9f322a530e85cd640f21af5c6bae42,0,0,0,0,1\n",
            "arb_133c4737c4e04a8991fc7c219106b4e4,0,0,0,0,1\n",
            "arb_f03fd416ab63cbb8f9f92a020fdd46be,0,0,0,0,0\n",
            "arb_b8fbf2253ba4b8a83829b964012d1a9b,0,0,0,0,0\n",
            "arb_4188caa3678846451bb812d7a7902459,0,0,0,0,1\n",
            "arb_c0224b694b38805f15fae4137121302e,0,0,0,0,0\n",
            "arb_3991e487126513829ec24ab5c1f1e325,0,1,0,0,0\n",
            "arb_9861662780205d38780ea04feed598dd,0,0,0,0,0\n",
            "arb_2b14abe9527a935117ae771612afca21,0,1,1,0,0\n",
            "arb_925eb03f72f22102e242a475f1f54530,0,0,0,0,0\n",
            "arb_c531bdc9a777137d8aa23a3bf529f87e,0,1,0,1,0\n",
            "arb_51a6eb82ab53edd30fdd18b6940dc275,0,0,0,0,0\n",
            "arb_6aa01210c44d11393ea419d6781b89a9,0,0,0,0,0\n",
            "arb_0c8c4d1e368fa6081c8840c3924b3711,0,0,0,0,0\n",
            "arb_f15fdbdb119f76f430f7cf7c773c3590,0,0,0,0,0\n",
            "arb_9b66d2e70ea93d6d37d0b656c1b1e880,1,0,0,0,1\n",
            "arb_773241d9837d36ed8962eacefa5143ef,0,0,0,0,1\n",
            "arb_b8f51143c06fbaed500ee2e7e7f5ac14,0,1,0,0,0\n",
            "arb_99dc675d6e1f9acdc7714fa066b4242b,0,0,0,0,1\n",
            "arb_3bcca38054a14240945f770aebe23355,0,1,0,0,0\n",
            "arb_730fba0ced42edd151f49218959abcdc,0,1,0,0,0\n",
            "arb_1a960ef19b8ebbeba5d8a48f2da3baf7,0,0,0,0,0\n",
            "arb_60a8455969e0277c96a31b2ab8f57bb6,0,0,0,0,0\n",
            "arb_8e46849f4110205740552b9b3d8afcc8,0,0,0,0,0\n",
            "arb_6535199985cd1f867d14c01e1780ed45,0,0,0,0,0\n",
            "arb_90448d504fbf1cdf02fe55dd474078fe,0,0,0,0,0\n",
            "arb_650c8d322f6c2ea8be78385ac29ad57d,0,0,0,0,0\n",
            "arb_8e08d6c50f1bc16d8aa16c4fcc461702,0,0,0,0,0\n",
            "arb_aafb2df7e4108d4d09ffefa74811ea2e,0,0,0,0,0\n",
            "arb_3a9070f93fa0aed9b4c7d7974d6134fe,0,0,0,0,0\n",
            "arb_a25e888067bd682f8712cc254a94450d,0,0,0,0,0\n",
            "arb_69b3fac31a9776e98df1f2ed3a37f743,0,0,0,0,0\n",
            "arb_70d64fdc7707ba7b23590bc3987a22bc,0,0,0,0,0\n",
            "arb_3561147626932826873155b2cb70e11e,1,0,0,0,0\n",
            "arb_757e07f4b35078992f4e9656cb4ea948,0,0,0,0,0\n",
            "arb_954a7a38de550d4b5252483bb9b748ea,0,0,0,0,0\n",
            "arb_a9847cdfa1abb179e11c85955a5f49bf,0,0,0,0,0\n",
            "arb_46b4284e3ace41c2806a041162aa1a39,0,0,0,0,0\n",
            "arb_8e5e356a5c81496169940989d545c6ac,0,0,0,0,0\n",
            "arb_c30e1b90992cee20688833ca1e382de0,0,0,0,0,0\n",
            "arb_0340513e0b537d1db0934fc235651218,0,0,0,0,0\n",
            "arb_726332fa6d54c653d55f5c7b42deb977,0,0,0,0,0\n",
            "arb_88c88e64e71c1e1b9222a49458d448ee,0,0,0,0,0\n",
            "arb_031d376aa7923c4268ac73dc2b9334a9,0,0,0,0,0\n",
            "arb_1cbab69c6f39bfd5484bd5832ece0ac0,0,0,0,0,0\n",
            "arb_ce785dd8c5b8140363e2e057dae241ce,0,0,0,0,0\n",
            "arb_60d2f094428bbcbde294086a7f75d5d9,0,0,0,0,0\n",
            "arb_28552d3893a580bf74bb1e4711c30e29,0,0,0,0,0\n",
            "arb_7e021fb631c018eb7a1dc96bb3b7d3f5,0,0,0,0,0\n",
            "arb_1429c399c30de8d7188cdd239c91aaa6,0,0,0,0,0\n",
            "arb_1a1c75b0f7b1b571c157dd83243f81c6,0,0,0,0,0\n",
            "arb_143932e3ea47fd0d8d45ff79c8e9a529,0,0,0,0,0\n",
            "arb_c0a60d5d5cb40fbd604c9c3e0b3098bf,0,0,0,0,0\n",
            "arb_13636b23e853783423f760db617520a9,0,0,0,0,0\n",
            "arb_6c8048c166f7ee2e8a94ca3650335345,0,0,0,0,1\n",
            "arb_8530294a4c91bae6bcbc9d7888b8d183,0,0,0,0,0\n",
            "arb_c4c60186c0b797433e86f3f08cd77e7b,0,1,0,0,0\n",
            "arb_2de8255707465f99180bf1c58204816b,0,0,0,0,0\n",
            "arb_52f0037e1b1196f8462aba17ce30b0bd,0,0,0,0,0\n",
            "arb_05a11d7f973c5b33f51a53a92a08c9f7,0,0,0,0,0\n",
            "arb_cfdb8f2486dbb02404f69c104509e956,0,0,0,0,0\n",
            "arb_119473be95ebd1ff087a086a8cdf67f0,0,0,0,0,0\n",
            "arb_06aa7b6d64858e1ea54c8d5c093a1385,0,0,0,0,0\n",
            "arb_58683c41f6320b304993fbc9ce8e559c,0,0,0,0,0\n",
            "arb_ae60c3911c8290e556ffdfa0e73ea408,0,0,0,0,0\n",
            "arb_c6624ecf4359e15fe5298b8878a53630,0,0,0,0,0\n",
            "arb_2228fe1e68bf10ac10d74c8f24ed672f,0,0,0,0,0\n",
            "arb_0d027dd29b767b3cb54141a67da833a8,0,0,0,0,0\n",
            "arb_f5345aeade0452ce211afa3b91a2a98a,0,0,0,0,0\n",
            "arb_fb4217f3ac08ddbbe93b87607d9e7eea,0,0,0,0,0\n",
            "arb_6eaa4e896b8b2056308fba14b04d1323,0,0,0,0,0\n",
            "arb_0c49ebfdc0e82c39fb952a55a2eec2a6,0,1,0,1,0\n",
            "arb_d380557a1e7c162fbf0bf8bb8cb7c374,0,0,0,0,0\n",
            "arb_445262028526e3ac19d4a7617b89312f,0,0,0,0,0\n",
            "arb_5078361bd8c9205ae1abd0f554b413a8,0,0,0,0,0\n",
            "arb_d8fc83089c56da4eaa4ea5dc1c8385cd,0,1,0,1,0\n",
            "arb_e203ea25f0d5899540684c600c27e313,0,1,0,1,0\n",
            "arb_8af35c96dd0ab3abdd243488210d41fb,0,1,0,1,0\n",
            "arb_80e0c1d020bb3e68c57092a5da055789,0,1,0,1,0\n",
            "arb_e9afce419a9fd57fd8fde9130084d86f,0,1,0,1,0\n",
            "arb_4208815a310227400e1d9b718aa3201b,0,1,0,1,0\n",
            "arb_f78e2ee76eaf751e219c0a6c24f17477,0,1,0,1,0\n",
            "arb_9444af3fa26415a2e1a1215b618b77fc,0,1,1,1,0\n",
            "arb_8ca54749e8a4cb192d605e8d0fed5077,0,0,0,0,1\n",
            "arb_985e458b817a04e5b4b856bdd66baad1,0,1,0,0,0\n",
            "arb_2ae6d2826d3f741c81074893b7368e8f,0,0,0,0,0\n",
            "arb_752cbd7c5fa526b02871ee2174b75260,0,1,0,1,0\n",
            "arb_d4e513d177a18cd0361907f0abcee264,0,0,0,0,0\n",
            "arb_3fd0b4dcbfd27d815c7c1600ed523acc,0,0,0,0,0\n",
            "arb_a5714ea7faf17fb64bce39550b8f66a0,0,0,1,0,0\n",
            "arb_dbb306bb98f778a984aa4e89a2a0c69c,0,0,0,0,0\n",
            "arb_8fe3d20ecd2bffca8608ad581e1d450d,0,0,0,0,0\n",
            "arb_4d3ed82d7641e459b2143327303141ca,0,1,1,1,0\n",
            "arb_2fe0dd13d24583051d76f1efe2a6b221,0,1,0,0,0\n",
            "arb_328246a090fa79fae208734fb506356a,1,0,0,0,0\n",
            "arb_fa9bfd0d2544a6a0da7e9fdf67dcb61a,0,0,0,0,1\n",
            "arb_eca8c2a903579f5bc09bbae9a914192b,0,1,0,0,0\n",
            "arb_ae665012d4d85ec2b7f67036e8e7ef91,0,0,0,0,0\n",
            "arb_7b2074b411c5991657d956f2e868d0d6,0,0,0,0,0\n",
            "arb_a2d10460956f12da3bfad87d75c17b85,0,0,0,0,0\n",
            "arb_68d1cbebcef21c8b7e157d3deb68c96a,0,0,0,0,0\n",
            "arb_2320e4a90c9a3e16ad43013187628bf0,0,0,0,0,0\n",
            "arb_03c025ab5aac30fb1b817698060f88e9,0,0,0,0,0\n",
            "arb_28a4a6fa8a0d9478a678ead7d87141aa,0,0,0,0,0\n",
            "arb_26bf97302d3bbf29dc55628a7cdce3b2,0,0,0,0,0\n",
            "arb_6c9c12fd45b04bafdda96d36b4abf654,0,0,0,0,0\n",
            "arb_717eb9eb752dde1f2ba0b1f75431ecb9,0,0,0,0,0\n",
            "arb_22ad0f62586145ee322627cc04737aed,0,1,0,1,0\n",
            "arb_9965f78f084c4b52eba5ad3eaf00c383,0,0,0,0,0\n",
            "arb_230ff60adcb94e26bb447d4d2d2d4d87,0,0,0,0,0\n",
            "arb_de5e97574cb90756816e4acad906cbcb,0,0,0,0,0\n",
            "arb_b930858672f01ab4e30913d1a42b2fdd,0,0,0,0,1\n",
            "arb_9e735136b6578cfa2e617c9686b060c6,0,0,1,0,0\n",
            "arb_2f940f44c8882b16eedd16ceb9a22f3a,0,0,0,0,0\n",
            "arb_63bba78965d711655b703211386a1d66,0,0,0,0,1\n",
            "arb_796f4f5016b6b057af6e4d9a931be707,0,1,0,0,0\n",
            "arb_35b4ec4ee73ff5865897b530f86a0df2,0,1,0,0,0\n",
            "arb_54d246af610d9df0efc222e9f5f568ee,0,0,0,0,0\n",
            "arb_c03c23fb168af6d9974173940301a49e,0,0,0,0,1\n",
            "arb_04c10b142c039bea31abc347a05e901e,0,1,0,1,0\n",
            "arb_fc3a3dada9c28b31f4501b1aff617009,0,1,0,0,0\n",
            "arb_db683247365c64679c4cc519af44f581,0,0,0,0,0\n",
            "arb_aecc5b9082fe643e2b8d6f0ac0dcdfce,0,0,0,0,0\n",
            "arb_f17c38194db34a072fb9ffa06a3c1a36,0,0,1,0,0\n",
            "arb_00ba726a4ccda40d3aa9970b605643d0,0,0,1,0,0\n",
            "arb_466691d07d329912a743f07951be4b63,0,0,0,0,0\n",
            "arb_32df687e0801943c203047322024a082,1,0,1,0,0\n",
            "arb_5956bff1d7e2d783e032bbb840bc0b0c,0,0,0,0,0\n",
            "arb_8d9f1d3fdd08dccf0fe640b32ee3c75d,0,0,0,0,0\n",
            "arb_2f9508e688439711aadf9ffe4c7dc43c,0,0,0,0,0\n",
            "arb_ea84b8ff4b701131207666ad68d9b4e6,0,0,0,0,0\n",
            "arb_c3baa08c705605f300bacbfd7b32cdb5,0,0,0,0,0\n",
            "arb_968554a6b5723334ac89acd80d35f5f3,0,0,0,0,0\n",
            "arb_63849e6cf2a4b209a3a7c88cf689c09f,0,0,0,0,0\n",
            "arb_7c14ba01ef8cdc91ee907afe06c6d57b,0,0,0,0,1\n",
            "arb_d90e03b791ee0a44e1ff732b3502cff2,0,0,0,0,0\n",
            "arb_a614e47120a6cc5ab2c22d062f799d1d,0,0,0,1,0\n",
            "arb_d0bec4c3e835773e91bade24082405dc,0,0,0,0,0\n",
            "arb_54d3d10c58ab60fe3bc8135cec8a7745,0,0,0,0,0\n",
            "arb_3a6d9f8b7468345e5fe80353682ea2b6,0,1,0,1,0\n",
            "arb_a74cb5b1ac1b5e1e3297ec29040104f7,0,0,0,0,0\n",
            "arb_902c5ae816b9323758ed8e33424c5427,0,0,0,0,0\n",
            "arb_7d86f321f6d638c75df7ed4645bdabbc,0,0,0,0,0\n",
            "arb_452c08a3d812c6786509a33f13bdca7a,0,0,0,0,0\n",
            "arb_fee827860874f8bf04cfdaca1a0bff7a,0,0,0,0,0\n",
            "arb_d072529aace0ca637b85d8bb968577c0,0,0,0,0,0\n",
            "arb_e564370d0a844c67763dafdd2ede0411,0,0,0,0,0\n"
          ]
        }
      ],
      "source": [
        "# print the results row by row in csv format\n",
        "for i in range(len(labels)):\n",
        "    print(f\"{test_dataset['id'][i]},\" + \",\".join(str(x) for x in labels[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UL1uE8llIgTQ",
      "metadata": {
        "id": "UL1uE8llIgTQ"
      },
      "source": [
        "# Subtask 3: Manifestation Identification\n",
        "Multi-label classification to classify how polarization is expressed, with multiple possible labels including Vilification, Extreme Language, Stereotype, Invalidation, Lack of Empathy, and Dehumanization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCz20cgl-K3t",
      "metadata": {
        "id": "nCz20cgl-K3t"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('./dev_phase/subtask3/train/eng.csv')\n",
        "\n",
        "# Split into train (80%) and val (20%), stratify if possible\n",
        "train, val = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qs-UjVYsInpD",
      "metadata": {
        "id": "Qs-UjVYsInpD"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-yxSaDCA9IMi",
      "metadata": {
        "id": "-yxSaDCA9IMi"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VXqqqIH9A3M",
      "metadata": {
        "id": "0VXqqqIH9A3M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=6, problem_type=\"multi_label_classification\") # use 6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QLubGtx988hm",
      "metadata": {
        "id": "QLubGtx988hm"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qEhm1TEv82mP",
      "metadata": {
        "id": "qEhm1TEv82mP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1014' max='1014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1014/1014 02:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.369500</td>\n",
              "      <td>0.375278</td>\n",
              "      <td>0.483522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.301000</td>\n",
              "      <td>0.357675</td>\n",
              "      <td>0.511258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.254900</td>\n",
              "      <td>0.364251</td>\n",
              "      <td>0.486960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/0ssamaak0/miniconda3/envs/nlp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 score on validation set for Subtask 3: 0.48695961263769266\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 3: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023e38ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3881bc1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:01<00:00, 125.50it/s]\n"
          ]
        }
      ],
      "source": [
        "test_dataset = pd.read_csv('./dev_phase/subtask3/dev/eng.csv')\n",
        "labels = []\n",
        "probs_list = []\n",
        "labels = []\n",
        "for text in tqdm(test_dataset['text']):\n",
        "    # Run the model\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "        pred_label = (probabilities > 0.5).astype(int)\n",
        "        labels.append(pred_label)\n",
        "        probs_list.append(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd5235f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arb_67be47e5216d7bee41e17484e619f4e6,0,0,0,0,0,0\n",
            "arb_272322e5b265e177613d685e5619e402,0,0,0,0,0,0\n",
            "arb_d1ec38dd0ec5d7a4fe28ef8317fc96c1,0,0,0,0,0,0\n",
            "arb_fad75310b17c124d98ebc514189ec033,1,1,1,0,1,0\n",
            "arb_95caf70cec5bf00c94c35cf7af2a0ab5,1,1,1,0,0,0\n",
            "arb_ac108c1ecf5071892c61abd253847b15,1,0,0,0,0,0\n",
            "arb_adaaa6d482119e65ce337ee224674e70,1,1,1,0,0,0\n",
            "arb_2794b08cac6cc9394a68c51cfc436243,1,1,1,0,0,0\n",
            "arb_19dd96c989323c9e950a2c3ab9c285be,1,1,1,0,0,0\n",
            "arb_f2bd638d9d9fc7a617130ff2b198b562,1,1,1,0,0,0\n",
            "arb_f992bf7776b854d4f7f8475aebf80f49,1,1,1,0,1,0\n",
            "arb_0b5ac70e86926f5e84cad94028864a37,1,0,0,0,0,0\n",
            "arb_8ababf95f952e2425c2df1033192dac0,0,0,0,0,0,0\n",
            "arb_06cd19aac6cc52e394a22d7d1dd58efc,0,0,0,0,0,0\n",
            "arb_12eeeb8d2fa2d04be2ed9830d5f36ce9,0,0,0,0,0,0\n",
            "arb_5bc23bacf9a161cd0f99719c70681a81,1,0,0,0,0,0\n",
            "arb_9ee7c931ab1ecd655533042d8301f6bb,0,0,0,0,0,0\n",
            "arb_bb7c40559f3a7ca1ecdd7dd7c136198f,0,0,0,0,0,0\n",
            "arb_5d394c0cce56675e2fc36a0590b47ed7,0,0,0,0,0,0\n",
            "arb_0704305e8313650e672563a2d073384f,0,0,0,0,0,0\n",
            "arb_e56b759d14fd70506e01cf971315453d,0,0,0,0,0,0\n",
            "arb_d286a2aac63432acef285a4799041f55,0,0,0,0,0,0\n",
            "arb_5a9f322a530e85cd640f21af5c6bae42,0,0,0,0,0,0\n",
            "arb_133c4737c4e04a8991fc7c219106b4e4,1,1,1,0,0,0\n",
            "arb_f03fd416ab63cbb8f9f92a020fdd46be,1,0,0,0,0,0\n",
            "arb_b8fbf2253ba4b8a83829b964012d1a9b,0,0,0,0,0,0\n",
            "arb_4188caa3678846451bb812d7a7902459,0,0,0,0,0,0\n",
            "arb_c0224b694b38805f15fae4137121302e,0,0,0,0,0,0\n",
            "arb_3991e487126513829ec24ab5c1f1e325,0,0,0,0,0,0\n",
            "arb_9861662780205d38780ea04feed598dd,0,0,0,0,0,0\n",
            "arb_2b14abe9527a935117ae771612afca21,1,1,1,0,0,0\n",
            "arb_925eb03f72f22102e242a475f1f54530,0,0,0,0,0,0\n",
            "arb_c531bdc9a777137d8aa23a3bf529f87e,1,1,1,0,0,0\n",
            "arb_51a6eb82ab53edd30fdd18b6940dc275,0,0,0,0,0,0\n",
            "arb_6aa01210c44d11393ea419d6781b89a9,0,0,0,0,0,0\n",
            "arb_0c8c4d1e368fa6081c8840c3924b3711,0,0,0,0,0,0\n",
            "arb_f15fdbdb119f76f430f7cf7c773c3590,0,0,0,0,0,0\n",
            "arb_9b66d2e70ea93d6d37d0b656c1b1e880,1,1,1,0,1,1\n",
            "arb_773241d9837d36ed8962eacefa5143ef,0,0,0,0,0,0\n",
            "arb_b8f51143c06fbaed500ee2e7e7f5ac14,1,1,1,0,0,0\n",
            "arb_99dc675d6e1f9acdc7714fa066b4242b,1,1,1,0,0,0\n",
            "arb_3bcca38054a14240945f770aebe23355,0,0,0,0,0,0\n",
            "arb_730fba0ced42edd151f49218959abcdc,1,1,1,0,0,0\n",
            "arb_1a960ef19b8ebbeba5d8a48f2da3baf7,0,0,0,0,0,0\n",
            "arb_60a8455969e0277c96a31b2ab8f57bb6,0,0,0,0,0,0\n",
            "arb_8e46849f4110205740552b9b3d8afcc8,0,0,0,0,0,0\n",
            "arb_6535199985cd1f867d14c01e1780ed45,0,0,0,0,0,0\n",
            "arb_90448d504fbf1cdf02fe55dd474078fe,0,0,0,0,0,0\n",
            "arb_650c8d322f6c2ea8be78385ac29ad57d,0,0,0,0,0,0\n",
            "arb_8e08d6c50f1bc16d8aa16c4fcc461702,0,0,0,0,0,0\n",
            "arb_aafb2df7e4108d4d09ffefa74811ea2e,0,0,0,0,0,0\n",
            "arb_3a9070f93fa0aed9b4c7d7974d6134fe,0,0,0,0,0,0\n",
            "arb_a25e888067bd682f8712cc254a94450d,0,0,0,0,0,0\n",
            "arb_69b3fac31a9776e98df1f2ed3a37f743,0,0,0,0,0,0\n",
            "arb_70d64fdc7707ba7b23590bc3987a22bc,0,0,0,0,0,0\n",
            "arb_3561147626932826873155b2cb70e11e,1,1,1,0,0,0\n",
            "arb_757e07f4b35078992f4e9656cb4ea948,0,0,0,0,0,0\n",
            "arb_954a7a38de550d4b5252483bb9b748ea,0,0,0,0,0,0\n",
            "arb_a9847cdfa1abb179e11c85955a5f49bf,0,0,0,0,0,0\n",
            "arb_46b4284e3ace41c2806a041162aa1a39,0,0,0,0,0,0\n",
            "arb_8e5e356a5c81496169940989d545c6ac,0,0,0,0,0,0\n",
            "arb_c30e1b90992cee20688833ca1e382de0,0,0,0,0,0,0\n",
            "arb_0340513e0b537d1db0934fc235651218,0,0,0,0,0,0\n",
            "arb_726332fa6d54c653d55f5c7b42deb977,0,0,0,0,0,0\n",
            "arb_88c88e64e71c1e1b9222a49458d448ee,0,0,0,0,0,0\n",
            "arb_031d376aa7923c4268ac73dc2b9334a9,0,0,0,0,0,0\n",
            "arb_1cbab69c6f39bfd5484bd5832ece0ac0,0,0,0,0,0,0\n",
            "arb_ce785dd8c5b8140363e2e057dae241ce,0,0,0,0,0,0\n",
            "arb_60d2f094428bbcbde294086a7f75d5d9,0,0,0,0,0,0\n",
            "arb_28552d3893a580bf74bb1e4711c30e29,0,0,0,0,0,0\n",
            "arb_7e021fb631c018eb7a1dc96bb3b7d3f5,0,0,0,0,0,0\n",
            "arb_1429c399c30de8d7188cdd239c91aaa6,0,0,0,0,0,0\n",
            "arb_1a1c75b0f7b1b571c157dd83243f81c6,0,0,0,0,0,0\n",
            "arb_143932e3ea47fd0d8d45ff79c8e9a529,0,0,0,0,0,0\n",
            "arb_c0a60d5d5cb40fbd604c9c3e0b3098bf,0,0,0,0,0,0\n",
            "arb_13636b23e853783423f760db617520a9,0,0,0,0,0,0\n",
            "arb_6c8048c166f7ee2e8a94ca3650335345,1,0,1,0,0,0\n",
            "arb_8530294a4c91bae6bcbc9d7888b8d183,0,0,0,0,0,0\n",
            "arb_c4c60186c0b797433e86f3f08cd77e7b,0,0,0,0,0,0\n",
            "arb_2de8255707465f99180bf1c58204816b,0,0,0,0,0,0\n",
            "arb_52f0037e1b1196f8462aba17ce30b0bd,0,0,0,0,0,0\n",
            "arb_05a11d7f973c5b33f51a53a92a08c9f7,0,0,0,0,0,0\n",
            "arb_cfdb8f2486dbb02404f69c104509e956,0,0,0,0,0,0\n",
            "arb_119473be95ebd1ff087a086a8cdf67f0,0,0,0,0,0,0\n",
            "arb_06aa7b6d64858e1ea54c8d5c093a1385,0,0,0,0,0,0\n",
            "arb_58683c41f6320b304993fbc9ce8e559c,0,0,0,0,0,0\n",
            "arb_ae60c3911c8290e556ffdfa0e73ea408,0,0,0,0,0,0\n",
            "arb_c6624ecf4359e15fe5298b8878a53630,0,0,0,0,0,0\n",
            "arb_2228fe1e68bf10ac10d74c8f24ed672f,0,0,0,0,0,0\n",
            "arb_0d027dd29b767b3cb54141a67da833a8,0,0,0,0,0,0\n",
            "arb_f5345aeade0452ce211afa3b91a2a98a,0,0,0,0,0,0\n",
            "arb_fb4217f3ac08ddbbe93b87607d9e7eea,0,0,0,0,0,0\n",
            "arb_6eaa4e896b8b2056308fba14b04d1323,0,0,0,0,0,0\n",
            "arb_0c49ebfdc0e82c39fb952a55a2eec2a6,1,1,1,0,0,0\n",
            "arb_d380557a1e7c162fbf0bf8bb8cb7c374,0,0,0,0,0,0\n",
            "arb_445262028526e3ac19d4a7617b89312f,0,0,0,0,0,0\n",
            "arb_5078361bd8c9205ae1abd0f554b413a8,0,0,0,0,0,0\n",
            "arb_d8fc83089c56da4eaa4ea5dc1c8385cd,1,1,1,1,1,1\n",
            "arb_e203ea25f0d5899540684c600c27e313,1,1,1,0,0,0\n",
            "arb_8af35c96dd0ab3abdd243488210d41fb,1,1,1,0,1,0\n",
            "arb_80e0c1d020bb3e68c57092a5da055789,1,1,1,0,0,0\n",
            "arb_e9afce419a9fd57fd8fde9130084d86f,1,1,1,0,0,0\n",
            "arb_4208815a310227400e1d9b718aa3201b,1,1,1,0,0,0\n",
            "arb_f78e2ee76eaf751e219c0a6c24f17477,1,1,1,0,1,0\n",
            "arb_9444af3fa26415a2e1a1215b618b77fc,1,1,1,0,1,1\n",
            "arb_8ca54749e8a4cb192d605e8d0fed5077,0,0,0,0,0,0\n",
            "arb_985e458b817a04e5b4b856bdd66baad1,1,1,1,0,1,0\n",
            "arb_2ae6d2826d3f741c81074893b7368e8f,0,0,0,0,0,0\n",
            "arb_752cbd7c5fa526b02871ee2174b75260,1,1,1,0,1,0\n",
            "arb_d4e513d177a18cd0361907f0abcee264,0,0,0,0,0,0\n",
            "arb_3fd0b4dcbfd27d815c7c1600ed523acc,0,0,0,0,0,0\n",
            "arb_a5714ea7faf17fb64bce39550b8f66a0,1,1,1,0,0,0\n",
            "arb_dbb306bb98f778a984aa4e89a2a0c69c,0,0,0,0,0,0\n",
            "arb_8fe3d20ecd2bffca8608ad581e1d450d,0,0,0,0,0,0\n",
            "arb_4d3ed82d7641e459b2143327303141ca,1,1,1,0,1,0\n",
            "arb_2fe0dd13d24583051d76f1efe2a6b221,1,0,1,0,0,0\n",
            "arb_328246a090fa79fae208734fb506356a,0,0,0,0,0,0\n",
            "arb_fa9bfd0d2544a6a0da7e9fdf67dcb61a,0,0,0,0,0,0\n",
            "arb_eca8c2a903579f5bc09bbae9a914192b,1,1,1,0,1,0\n",
            "arb_ae665012d4d85ec2b7f67036e8e7ef91,0,0,0,0,0,0\n",
            "arb_7b2074b411c5991657d956f2e868d0d6,0,0,0,0,0,0\n",
            "arb_a2d10460956f12da3bfad87d75c17b85,0,0,0,0,0,0\n",
            "arb_68d1cbebcef21c8b7e157d3deb68c96a,0,0,0,0,0,0\n",
            "arb_2320e4a90c9a3e16ad43013187628bf0,0,0,0,0,0,0\n",
            "arb_03c025ab5aac30fb1b817698060f88e9,0,0,0,0,0,0\n",
            "arb_28a4a6fa8a0d9478a678ead7d87141aa,0,0,0,0,0,0\n",
            "arb_26bf97302d3bbf29dc55628a7cdce3b2,0,0,0,0,0,0\n",
            "arb_6c9c12fd45b04bafdda96d36b4abf654,0,0,0,0,0,0\n",
            "arb_717eb9eb752dde1f2ba0b1f75431ecb9,0,0,0,0,0,0\n",
            "arb_22ad0f62586145ee322627cc04737aed,1,1,1,0,0,0\n",
            "arb_9965f78f084c4b52eba5ad3eaf00c383,0,0,0,0,0,0\n",
            "arb_230ff60adcb94e26bb447d4d2d2d4d87,1,1,1,0,0,0\n",
            "arb_de5e97574cb90756816e4acad906cbcb,1,0,1,0,0,0\n",
            "arb_b930858672f01ab4e30913d1a42b2fdd,1,1,1,0,0,0\n",
            "arb_9e735136b6578cfa2e617c9686b060c6,1,1,1,0,0,0\n",
            "arb_2f940f44c8882b16eedd16ceb9a22f3a,0,0,0,0,0,0\n",
            "arb_63bba78965d711655b703211386a1d66,0,0,0,0,0,0\n",
            "arb_796f4f5016b6b057af6e4d9a931be707,0,0,0,0,0,0\n",
            "arb_35b4ec4ee73ff5865897b530f86a0df2,1,0,1,0,0,0\n",
            "arb_54d246af610d9df0efc222e9f5f568ee,1,1,1,0,0,0\n",
            "arb_c03c23fb168af6d9974173940301a49e,0,0,0,0,0,0\n",
            "arb_04c10b142c039bea31abc347a05e901e,1,1,1,0,0,0\n",
            "arb_fc3a3dada9c28b31f4501b1aff617009,1,1,1,0,0,0\n",
            "arb_db683247365c64679c4cc519af44f581,0,0,0,0,0,0\n",
            "arb_aecc5b9082fe643e2b8d6f0ac0dcdfce,0,0,0,0,0,0\n",
            "arb_f17c38194db34a072fb9ffa06a3c1a36,1,0,1,0,0,0\n",
            "arb_00ba726a4ccda40d3aa9970b605643d0,1,1,1,0,0,0\n",
            "arb_466691d07d329912a743f07951be4b63,1,1,1,0,0,0\n",
            "arb_32df687e0801943c203047322024a082,1,1,1,0,1,1\n",
            "arb_5956bff1d7e2d783e032bbb840bc0b0c,0,0,0,0,0,0\n",
            "arb_8d9f1d3fdd08dccf0fe640b32ee3c75d,0,0,0,0,0,0\n",
            "arb_2f9508e688439711aadf9ffe4c7dc43c,0,0,0,0,0,0\n",
            "arb_ea84b8ff4b701131207666ad68d9b4e6,0,0,0,0,0,0\n",
            "arb_c3baa08c705605f300bacbfd7b32cdb5,0,0,0,0,0,0\n",
            "arb_968554a6b5723334ac89acd80d35f5f3,0,0,0,0,0,0\n",
            "arb_63849e6cf2a4b209a3a7c88cf689c09f,0,0,0,0,0,0\n",
            "arb_7c14ba01ef8cdc91ee907afe06c6d57b,0,0,0,0,0,0\n",
            "arb_d90e03b791ee0a44e1ff732b3502cff2,1,0,1,0,0,0\n",
            "arb_a614e47120a6cc5ab2c22d062f799d1d,0,0,0,0,0,0\n",
            "arb_d0bec4c3e835773e91bade24082405dc,0,0,0,0,0,0\n",
            "arb_54d3d10c58ab60fe3bc8135cec8a7745,0,0,0,0,0,0\n",
            "arb_3a6d9f8b7468345e5fe80353682ea2b6,1,1,1,0,0,0\n",
            "arb_a74cb5b1ac1b5e1e3297ec29040104f7,0,0,0,0,0,0\n",
            "arb_902c5ae816b9323758ed8e33424c5427,0,0,0,0,0,0\n",
            "arb_7d86f321f6d638c75df7ed4645bdabbc,0,0,0,0,0,0\n",
            "arb_452c08a3d812c6786509a33f13bdca7a,0,0,0,0,0,0\n",
            "arb_fee827860874f8bf04cfdaca1a0bff7a,0,0,0,0,0,0\n",
            "arb_d072529aace0ca637b85d8bb968577c0,0,0,0,0,0,0\n",
            "arb_e564370d0a844c67763dafdd2ede0411,0,0,0,0,0,0\n"
          ]
        }
      ],
      "source": [
        "# print the results row by row in csv format\n",
        "for i in range(len(labels)):\n",
        "    print(f\"{test_dataset['id'][i]},\" + \",\".join(str(x) for x in labels[i]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
