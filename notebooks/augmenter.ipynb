{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e79d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from logs import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7bdb9c66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")\n",
        "\n",
        "# CONFIG\n",
        "NUM_TYPES = 5\n",
        "NUM_MANIFESTATIONS = 6\n",
        "datasets_merge = True\n",
        "lang = \"arb\"\n",
        "# trial_id = \"0000NG3\"\n",
        "# model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\", \"FacebookAI/xlm-roberta-large\", \"0ssamaak0/roberta-base-LEGO_emotions\", \"FacebookAI/roberta-base\"]\n",
        "# model_name = model_names[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a6e2fe95",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_1 = pd.read_csv(\"./dev_phase/subtask1/train/\" + lang + \".csv\")\n",
        "train_2 = pd.read_csv(\"./dev_phase/subtask2/train/\" + lang + \".csv\")\n",
        "train_3 = pd.read_csv(\"./dev_phase/subtask3/train/\" + lang + \".csv\")\n",
        "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/\" + lang + \".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a67443db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training examples: 3380\n"
          ]
        }
      ],
      "source": [
        "# Merge all training data to get unique texts with all labels\n",
        "train_df = train_1.merge(train_2, on=[\"id\", \"text\"], how=\"outer\").merge(train_3, on=[\"id\", \"text\"], how=\"outer\")\n",
        "train_df = train_df.fillna(0).astype({col: int for col in train_df.columns if col not in [\"id\", \"text\"]})\n",
        "print(f\"Total training examples: {len(train_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7358f074",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def paraphrase_text(text: str, model: str = \"gpt-5.1-2025-11-13\") -> str:\n",
        "    \"\"\"Paraphrase text using OpenAI API while preserving meaning and tone.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a paraphrasing assistant. Rephrase the given text while preserving its exact meaning, tone, and sentiment. Keep the same level of formality and emotional intensity. Output ONLY the paraphrased text, nothing else.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": text\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b723d466",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples to augment: 507\n"
          ]
        }
      ],
      "source": [
        "# Sample 15% of the dataset for augmentation\n",
        "AUGMENT_RATIO = 0.15\n",
        "sample_df = train_df.sample(frac=AUGMENT_RATIO, random_state=42)\n",
        "print(f\"Samples to augment: {len(sample_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4e5f5510",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56a7a3807a444c25b3888c5a67a83968",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Paraphrasing:   0%|          | 0/507 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully augmented: 507 examples\n"
          ]
        }
      ],
      "source": [
        "# Augment the sampled data\n",
        "augmented_rows = []\n",
        "\n",
        "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Paraphrasing\"):\n",
        "    try:\n",
        "        paraphrased = paraphrase_text(row[\"text\"])\n",
        "        new_row = row.copy()\n",
        "        new_row[\"text\"] = paraphrased\n",
        "        new_row[\"id\"] = row[\"id\"] + \"_aug\"  # Mark as augmented\n",
        "        augmented_rows.append(new_row)\n",
        "    except Exception as e:\n",
        "        print(f\"Error paraphrasing row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "augmented_df = pd.DataFrame(augmented_rows)\n",
        "print(f\"Successfully augmented: {len(augmented_df)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b822907f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset size: 3887 (original: 3380, augmented: 507)\n",
            "Saved augmented dataset!\n"
          ]
        }
      ],
      "source": [
        "# Combine original and augmented data\n",
        "final_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
        "print(f\"Final dataset size: {len(final_df)} (original: {len(train_df)}, augmented: {len(augmented_df)})\")\n",
        "\n",
        "# Save the augmented dataset\n",
        "final_df.to_csv(f\"./dev_phase/subtask1/train/{lang}_augmented.csv\", index=False)\n",
        "print(\"Saved augmented dataset!\")\n",
        "\n",
        "# # or load if already saved\n",
        "# final_df = pd.read_csv(f\"./dev_phase/subtask1/train/{lang}_augmented.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9fd02733",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved split augmented datasets for subtask1/2/3.\n"
          ]
        }
      ],
      "source": [
        "# Split the augmented dataset back into the three subtasks\n",
        "train_1_aug = final_df[train_1.columns].copy()\n",
        "train_2_aug = final_df[train_2.columns].copy()\n",
        "train_3_aug = final_df[train_3.columns].copy()\n",
        "\n",
        "# Save per-subtask augmented datasets\n",
        "train_1_aug.to_csv(f\"./dev_phase/subtask1/train/{lang}_augmented.csv\", index=False)\n",
        "train_2_aug.to_csv(f\"./dev_phase/subtask2/train/{lang}_augmented.csv\", index=False)\n",
        "train_3_aug.to_csv(f\"./dev_phase/subtask3/train/{lang}_augmented.csv\", index=False)\n",
        "\n",
        "print(\"Saved split augmented datasets for subtask1/2/3.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
