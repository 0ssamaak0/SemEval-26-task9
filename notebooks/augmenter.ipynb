{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67e79d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Add parent directory to path to import logs module\n",
        "# This allows importing from the root directory when running from notebooks/\n",
        "current_dir = Path(os.getcwd())\n",
        "if current_dir.name == 'notebooks':\n",
        "    # If we're in notebooks/, add parent directory\n",
        "    sys.path.insert(0, str(current_dir.parent))\n",
        "else:\n",
        "    # If running from root, add current directory\n",
        "    sys.path.insert(0, str(current_dir))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from logs import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bdb9c66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")\n",
        "\n",
        "# CONFIG\n",
        "NUM_TYPES = 5\n",
        "NUM_MANIFESTATIONS = 6\n",
        "datasets_merge = True\n",
        "lang = \"eng\"\n",
        "# trial_id = \"0000NG3\"\n",
        "# model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\", \"FacebookAI/xlm-roberta-large\", \"0ssamaak0/roberta-base-LEGO_emotions\", \"FacebookAI/roberta-base\"]\n",
        "# model_name = model_names[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a6e2fe95",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_1 = pd.read_csv(\"../dev_phase/subtask1/train/\" + lang + \".csv\")\n",
        "train_2 = pd.read_csv(\"../dev_phase/subtask2/train/\" + lang + \".csv\")\n",
        "train_3 = pd.read_csv(\"../dev_phase/subtask3/train/\" + lang + \".csv\")\n",
        "dev_df = pd.read_csv(\"../dev_phase/subtask1/dev/\" + lang + \".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a67443db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training examples: 3380\n"
          ]
        }
      ],
      "source": [
        "# Merge all training data to get unique texts with all labels\n",
        "train_df = train_1.merge(train_2, on=[\"id\", \"text\"], how=\"outer\").merge(train_3, on=[\"id\", \"text\"], how=\"outer\")\n",
        "train_df = train_df.fillna(0).astype({col: int for col in train_df.columns if col not in [\"id\", \"text\"]})\n",
        "print(f\"Total training examples: {len(train_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7358f074",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def paraphrase_text(text: str, model: str = \"gpt-5.2-2025-12-11\") -> str:\n",
        "    \"\"\"Paraphrase text using OpenAI API while preserving meaning and tone.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a paraphrasing assistant. Rephrase the given text while preserving its exact meaning, tone, and sentiment. You can change the wording tone, formality, if it's in standard Arabic you can use dialect or vice versa. The goal is to augment this data to make the model more robust on polarization and hate speech detection. Do everything but keep the same meaning. Output ONLY the paraphrased text, nothing else.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": text\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b723d466",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples to augment: 845\n"
          ]
        }
      ],
      "source": [
        "# Sample 15% of the dataset for augmentation\n",
        "AUGMENT_RATIO = 0.25\n",
        "sample_df = train_df.sample(frac=AUGMENT_RATIO, random_state=42)\n",
        "print(f\"Samples to augment: {len(sample_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e5f5510",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89659231af6342da9174235e96822aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Paraphrasing:   0%|          | 0/845 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully augmented: 845 examples\n"
          ]
        }
      ],
      "source": [
        "# Augment the sampled data\n",
        "augmented_rows = []\n",
        "\n",
        "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Paraphrasing\"):\n",
        "    try:\n",
        "        paraphrased = paraphrase_text(row[\"text\"])\n",
        "        new_row = row.copy()\n",
        "        new_row[\"text\"] = paraphrased\n",
        "        new_row[\"id\"] = row[\"id\"] + \"_aug\"  # Mark as augmented\n",
        "        augmented_rows.append(new_row)\n",
        "    except Exception as e:\n",
        "        print(f\"Error paraphrasing row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "augmented_df = pd.DataFrame(augmented_rows)\n",
        "print(f\"Successfully augmented: {len(augmented_df)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b822907f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset size: 4225 (original: 3380, augmented: 845)\n",
            "Saved augmented dataset!\n"
          ]
        }
      ],
      "source": [
        "# Combine original and augmented data\n",
        "final_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
        "print(f\"Final dataset size: {len(final_df)} (original: {len(train_df)}, augmented: {len(augmented_df)})\")\n",
        "\n",
        "# Save the augmented dataset\n",
        "final_df.to_csv(f\"../dev_phase/subtask1/train/{lang}_augmented_1.csv\", index=False)\n",
        "print(\"Saved augmented dataset!\")\n",
        "\n",
        "# # or load if already saved\n",
        "# final_df = pd.read_csv(f\"./dev_phase/subtask1/train/{lang}_augmented.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9fd02733",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved split augmented datasets for subtask1/2/3.\n"
          ]
        }
      ],
      "source": [
        "# Split the augmented dataset back into the three subtasks\n",
        "train_1_aug = final_df[train_1.columns].copy()\n",
        "train_2_aug = final_df[train_2.columns].copy()\n",
        "train_3_aug = final_df[train_3.columns].copy()\n",
        "\n",
        "# Save per-subtask augmented datasets\n",
        "train_1_aug.to_csv(f\"../dev_phase/subtask1/train/{lang}_augmented1.csv\", index=False)\n",
        "train_2_aug.to_csv(f\"../dev_phase/subtask2/train/{lang}_augmented1.csv\", index=False)\n",
        "train_3_aug.to_csv(f\"../dev_phase/subtask3/train/{lang}_augmented1.csv\", index=False)\n",
        "\n",
        "print(\"Saved split augmented datasets for subtask1/2/3.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
