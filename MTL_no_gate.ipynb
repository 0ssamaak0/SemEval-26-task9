{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbaa52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install gdown if needed\n",
    "# # Replace the ID below with your actual file ID from the Drive link\n",
    "# # (The ID is the long string of random characters in the URL)\n",
    "# file_id = '1xQvIBwYFTqJRfU77LSqIrRJlsnpdMQGv'\n",
    "# url = f'https://drive.google.com/uc?id={file_id}'\n",
    "# output = 'dev_phase.zip'\n",
    "\n",
    "# !gdown {url} -O {output}\n",
    "\n",
    "# !unzip {output}\n",
    "\n",
    "# # Delete __MACOSX directory (if exists) and the dev_phase.zip file (cleanup)\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# if os.path.exists(\"__MACOSX\"):\n",
    "#     shutil.rmtree(\"__MACOSX\")\n",
    "\n",
    "# if os.path.exists(\"dev_phase.zip\"):\n",
    "#     os.remove(\"dev_phase.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf088005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from logs import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# CONFIG\n",
    "NUM_TYPES = 5\n",
    "NUM_MANIFESTATIONS = 6\n",
    "lang = \"eng\"\n",
    "trial_id = \"MTL_10epochs_full_soft_gating\"\n",
    "model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\", \"FacebookAI/xlm-roberta-large\", \"0ssamaak0/roberta-base-LEGO_emotions\", \"FacebookAI/roberta-base\", \"cardiffnlp/twitter-roberta-base\", \"cardiffnlp/roberta-base-emotion\", \"UBC-NLP/ARBERTv2\", \"cardiffnlp/twitter-roberta-base-offensive\", \"cardiffnlp/twitter-xlm-roberta-base-sentiment\", \"cardiffnlp/twitter-roberta-base-hate\"]\n",
    "model_name = model_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6407ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"./dev_phase/subtask1/train/\" + lang + \"_augmented.csv\")\n",
    "train_2 = pd.read_csv(\"./dev_phase/subtask2/train/\" + lang + \"_augmented.csv\")\n",
    "train_3 = pd.read_csv(\"./dev_phase/subtask3/train/\" + lang + \"_augmented.csv\")\n",
    "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/\" + lang + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e844f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
    "    self.texts=texts\n",
    "    self.labels=labels\n",
    "    self.tokenizer= tokenizer\n",
    "    self.max_length = max_length # Store max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    text=self.texts[idx]\n",
    "    label=self.labels[idx]\n",
    "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
    "\n",
    "    # Ensure consistent tensor conversion for all items\n",
    "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "    item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8900bea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f174dafb48d4ff4ac6b462ed201cbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3d953d65fa4221a81dba23b4fba2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b03a7c736945a0a59fa142fcb07e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68fe5ee18334d1ea3e9a0579a17dec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "# Prepare label columns separately for each task, fallback to the correct columns per train DataFrame\n",
    "def get_label_columns(df):\n",
    "    return [col for col in df.columns if col not in ['id', 'text']]\n",
    "\n",
    "# Split indices once and reuse for all datasets to ensure same split\n",
    "n_samples = len(train_1)\n",
    "indices = np.arange(n_samples)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.10,\n",
    "    random_state=42\n",
    ")\n",
    "# Merge all datasets on 'id'\n",
    "merged = train_1.merge(train_2, on=['id', 'text'], how='outer', suffixes=('_1', '_2'))\n",
    "# For the third, avoid duplicate columns of 'text', so drop redundant one, or merge only on id\n",
    "merged = merged.merge(train_3, on=['id', 'text'], how='outer', suffixes=('', '_3'))\n",
    "# Get label columns: all columns excluding 'id' and 'text'\n",
    "merged_label_columns = get_label_columns(merged)\n",
    "texts = merged['text'].tolist()\n",
    "labels = merged[merged_label_columns].values.tolist()\n",
    "texts_train = [texts[i] for i in train_indices]\n",
    "texts_val = [texts[i] for i in val_indices]\n",
    "labels_train = [labels[i] for i in train_indices]\n",
    "labels_val = [labels[i] for i in val_indices]\n",
    "train_dataset = PolarizationDataset(texts_train, labels_train, tokenizer)\n",
    "val_dataset = PolarizationDataset(texts_val, labels_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c696cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(labels_matrix):\n",
    "    # labels_matrix is a list of lists or numpy array\n",
    "    labels_np = np.array(labels_matrix)\n",
    "    num_pos = labels_np.sum(axis=0)\n",
    "    num_neg = len(labels_np) - num_pos\n",
    "    \n",
    "    # Simple ratio: if 10 pos and 90 neg, weight is 9.0\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    weights = num_neg / (num_pos + 1e-5)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "pos_weight_2 = torch.ones([NUM_TYPES]) * 5.0  # Penalize missing a type 5x more\n",
    "pos_weight_3 = torch.ones([NUM_MANIFESTATIONS]) * 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "500cca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMTLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_types, num_manifestations, pos_weight_2=None, pos_weight_3=None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.num_types = num_types\n",
    "        self.num_manifestations = num_manifestations\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.head1 = nn.Linear(hidden_size, 1)\n",
    "        self.head2 = nn.Linear(hidden_size, num_types)\n",
    "        self.head3 = nn.Linear(hidden_size, num_manifestations)\n",
    "\n",
    "        self.register_buffer(\"pos_weight_2\", pos_weight_2 if pos_weight_2 is not None else torch.tensor([]))\n",
    "        self.register_buffer(\"pos_weight_3\", pos_weight_3 if pos_weight_3 is not None else torch.tensor([]))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        H = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        logits1 = self.head1(H)\n",
    "        logits2 = self.head2(H)\n",
    "        logits3 = self.head3(H)\n",
    "        logits = torch.cat([logits1, logits2, logits3], dim=-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            y1_true = labels[:, :1]\n",
    "            y2_true = labels[:, 1:1 + self.num_types]\n",
    "            y3_true = labels[:, 1 + self.num_types:]\n",
    "\n",
    "            device = logits1.device\n",
    "\n",
    "            loss1 = nn.BCEWithLogitsLoss()(logits1, y1_true)\n",
    "\n",
    "            if self.pos_weight_2.numel() > 0:\n",
    "                loss2 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_2.to(device))(logits2, y2_true)\n",
    "            else:\n",
    "                loss2 = nn.BCEWithLogitsLoss()(logits2, y2_true)\n",
    "\n",
    "            if self.pos_weight_3.numel() > 0:\n",
    "                loss3 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_3.to(device))(logits3, y3_true)\n",
    "            else:\n",
    "                loss3 = nn.BCEWithLogitsLoss()(logits3, y3_true)\n",
    "\n",
    "            loss = (loss1 + loss2 + loss3) / 3.0\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb1b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-hate and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def compute_pos_weights(df, label_cols):\n",
    "    labels = df[label_cols].values\n",
    "    pos = labels.sum(axis=0)\n",
    "    neg = (labels == 0).sum(axis=0)\n",
    "    weights = torch.tensor(neg / (pos + 1e-5), dtype=torch.float)\n",
    "    return weights\n",
    "\n",
    "pos_weight_2 = compute_pos_weights(train_2, train_2.columns[2:])\n",
    "pos_weight_3 = compute_pos_weights(train_3, train_3.columns[2:])\n",
    "\n",
    "model = SharedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS, pos_weight_2, pos_weight_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    # Extract Binary Classification (Subtask 1)\n",
    "    y1_true = labels[:, 0]\n",
    "    y1_pred = preds[:, 0]\n",
    "\n",
    "    # --- LOGICAL GATING START ---\n",
    "    # Create a mask from Subtask 1 predictions\n",
    "    # Shape becomes (Batch_Size, 1) to broadcast over the other subtasks\n",
    "    mask = y1_pred[:, None] \n",
    "    \n",
    "    # Extract raw predictions for Subtask 2 & 3\n",
    "    y2_pred_raw = preds[:, 1:1+NUM_TYPES]\n",
    "    y3_pred_raw = preds[:, 1+NUM_TYPES:]\n",
    "    \n",
    "    # Apply the mask: If y1_pred is 0, force y2 and y3 to be 0\n",
    "    y2_pred = y2_pred_raw * mask\n",
    "    y3_pred = y3_pred_raw * mask\n",
    "    # --- LOGICAL GATING END ---\n",
    "\n",
    "    # Extract True Labels for Subtask 2 & 3\n",
    "    y2_true = labels[:, 1:1+NUM_TYPES]\n",
    "    y3_true = labels[:, 1+NUM_TYPES:]\n",
    "\n",
    "    return {\n",
    "        \"subtask_1/f1_macro\": f1_score(y1_true, y1_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_2/f1_macro\": f1_score(y2_true, y2_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_3/f1_macro\": f1_score(y3_true, y3_pred, average=\"macro\", zero_division=0),    }\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{trial_id}\",\n",
    "    num_train_epochs=10,              # Increase max epochs, let EarlyStopping handle the cut\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,   # 64 might be too stable? 32 adds some noise (good for regularization)\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,                # Add Weight Decay!\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # Must save to load best\n",
    "    load_best_model_at_end=True,      # Automatically load the best checkpoint\n",
    "    metric_for_best_model=\"eval_subtask_1/f1_macro\", # Optimize for the hardest metric or the main task\n",
    "    save_total_limit=2,               # Don't fill disk\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca0e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Subtask 1/f1 Macro</th>\n",
       "      <th>Subtask 2/f1 Macro</th>\n",
       "      <th>Subtask 3/f1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.645815</td>\n",
       "      <td>0.816450</td>\n",
       "      <td>0.319520</td>\n",
       "      <td>0.486326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.601729</td>\n",
       "      <td>0.801258</td>\n",
       "      <td>0.408271</td>\n",
       "      <td>0.500152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.507200</td>\n",
       "      <td>0.592193</td>\n",
       "      <td>0.825024</td>\n",
       "      <td>0.445694</td>\n",
       "      <td>0.522047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.622306</td>\n",
       "      <td>0.830610</td>\n",
       "      <td>0.430411</td>\n",
       "      <td>0.520266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.638689</td>\n",
       "      <td>0.824947</td>\n",
       "      <td>0.466941</td>\n",
       "      <td>0.508951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.655902</td>\n",
       "      <td>0.840137</td>\n",
       "      <td>0.488425</td>\n",
       "      <td>0.527493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.696550</td>\n",
       "      <td>0.846058</td>\n",
       "      <td>0.506096</td>\n",
       "      <td>0.545292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.715739</td>\n",
       "      <td>0.834324</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>0.532121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.724865</td>\n",
       "      <td>0.843916</td>\n",
       "      <td>0.512096</td>\n",
       "      <td>0.544302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.721451</td>\n",
       "      <td>0.838574</td>\n",
       "      <td>0.517667</td>\n",
       "      <td>0.540448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: \n",
      "subtask_1 f1_macro: 0.8461 \n",
      "subtask_2 f1_macro: 0.5061 \n",
      "subtask_3 f1_macro: 0.5453\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\n",
    "    \"Validation Results:\",\n",
    "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7c00c",
   "metadata": {},
   "source": [
    "# Log Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b4ddef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Experiment results logged to logs.json (trial_id: MTL_10epochs_full_soft_gating)\n",
      "  - subtask_1: eng\n",
      "  - subtask_2: eng\n",
      "  - subtask_3: eng\n"
     ]
    }
   ],
   "source": [
    "# Log the experiment results - each subtask separately\n",
    "\n",
    "# Prepare metadata for the experiment\n",
    "experiment_metadata = {\n",
    "    \"approach\": \"MTL_no_gate\",\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,\n",
    "    \"num_types\": NUM_TYPES,\n",
    "    \"num_manifestations\": NUM_MANIFESTATIONS,\n",
    "    \"datasets_merge\": datasets_merge,\n",
    "    \"posweight\": \"True\"\n",
    "}\n",
    "\n",
    "# Only log eval_f1_macro for each subtask\n",
    "subtask_1_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_1/f1_macro\"),\n",
    "}\n",
    "subtask_2_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_2/f1_macro\"),\n",
    "}\n",
    "subtask_3_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_3/f1_macro\"),\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "# Attempt to load existing logs and merge metadata for this trial if present\n",
    "existing_metadata = {}\n",
    "try:\n",
    "    with open(\"logs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        logs = json.load(f)\n",
    "        if isinstance(logs, dict):\n",
    "            logs = [logs]\n",
    "        for trial in logs:\n",
    "            if trial.get(\"trial_id\") == trial_id and \"metadata\" in trial:\n",
    "                existing_metadata = trial[\"metadata\"].copy()\n",
    "                break\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    pass\n",
    "\n",
    "# Only add/replace model_{lang}, don't overwrite the whole metadata\n",
    "merged_metadata = dict(existing_metadata)\n",
    "merged_metadata.update({\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"approach\": experiment_metadata[\"approach\"],\n",
    "    \"learning_rate\": experiment_metadata[\"learning_rate\"],\n",
    "    \"num_train_epochs\": experiment_metadata[\"num_train_epochs\"],\n",
    "    \"per_device_train_batch_size\": experiment_metadata[\"per_device_train_batch_size\"],\n",
    "    \"per_device_eval_batch_size\": experiment_metadata[\"per_device_eval_batch_size\"],\n",
    "    \"num_types\": experiment_metadata[\"num_types\"],\n",
    "    \"num_manifestations\": experiment_metadata[\"num_manifestations\"],\n",
    "    \"datasets_merge\": experiment_metadata[\"datasets_merge\"]\n",
    "})\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_1\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_1_results,\n",
    "    metadata=merged_metadata,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "# Log subtask_2 and subtask_3 using the same trial_id and do not pass metadata to avoid overwrite\n",
    "log(\n",
    "    subtask_name=\"subtask_2\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_2_results,\n",
    "    metadata=None,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_3\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_3_results,\n",
    "    metadata=None,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Experiment results logged to logs.json (trial_id: {trial_id})\")\n",
    "print(f\"  - subtask_1: {lang}\")\n",
    "print(f\"  - subtask_2: {lang}\")\n",
    "print(f\"  - subtask_3: {lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654cdde",
   "metadata": {},
   "source": [
    "# Predict on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe02703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved for all 3 dev sets with Logical Gating applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load dev 1 and predict all 3 dev sets\n",
    "dev_1 = pd.read_csv(f\"./dev_phase/subtask1/dev/{lang}.csv\")\n",
    "dev_2 = pd.read_csv(f\"./dev_phase/subtask2/dev/{lang}.csv\")\n",
    "dev_3 = pd.read_csv(f\"./dev_phase/subtask3/dev/{lang}.csv\")\n",
    "\n",
    "# Create dataset from dev 1 texts (all dev sets have same texts)\n",
    "dev_texts = dev_1['text'].tolist()\n",
    "# Dummy labels for prediction\n",
    "dev_dataset = PolarizationDataset(dev_texts, [[0]*12]*len(dev_texts), tokenizer)\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.predict(dev_dataset)\n",
    "logits = predictions.predictions\n",
    "if isinstance(logits, tuple):\n",
    "    logits = logits[0]\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Extract predictions for Subtask 1\n",
    "polarization_preds = preds[:, 0]\n",
    "\n",
    "# --- LOGICAL GATING START ---\n",
    "# Create mask based on Subtask 1 (N, 1)\n",
    "mask = polarization_preds[:, None]\n",
    "\n",
    "# Apply mask to Subtasks 2 and 3\n",
    "# If polarization is 0, these predictions become 0 regardless of model output\n",
    "types_preds = preds[:, 1:1+NUM_TYPES] * mask\n",
    "manifestations_preds = preds[:, 1+NUM_TYPES:] * mask\n",
    "\n",
    "# Create output DataFrames\n",
    "output_1 = dev_1[['id', 'text']].copy()\n",
    "output_1['polarization'] = polarization_preds\n",
    "\n",
    "output_2 = dev_2[['id', 'text']].copy()\n",
    "type_cols = [col for col in dev_2.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(type_cols):\n",
    "    output_2[col] = types_preds[:, i]\n",
    "\n",
    "output_3 = dev_3[['id', 'text']].copy()\n",
    "manifest_cols = [col for col in dev_3.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(manifest_cols):\n",
    "    output_3[col] = manifestations_preds[:, i]\n",
    "\n",
    "# Drop the 'text' column before saving\n",
    "output_1 = output_1.drop(columns=['text'])\n",
    "output_2 = output_2.drop(columns=['text'])\n",
    "output_3 = output_3.drop(columns=['text'])\n",
    "\n",
    "# Create dir under results with trial_id\n",
    "os.makedirs(f\"./results/{trial_id}\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_1\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_2\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_3\", exist_ok=True)\n",
    "\n",
    "# Save predictions to subtask_ directories\n",
    "output_1.to_csv(f\"./results/{trial_id}/subtask_1/pred_{lang}.csv\", index=False)\n",
    "output_2.to_csv(f\"./results/{trial_id}/subtask_2/pred_{lang}.csv\", index=False)\n",
    "output_3.to_csv(f\"./results/{trial_id}/subtask_3/pred_{lang}.csv\", index=False)\n",
    "\n",
    "print(f\"Predictions saved for all 3 dev sets with Logical Gating applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6afe8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_f66ca14d60851371f9720aaf4ccd9b58,0,0,0,0,0,0\n",
      "eng_3a489aa7fed9726aa8d3d4fe74c57efb,0,0,0,0,0,0\n",
      "eng_95770ff547ea5e48b0be00f385986483,0,0,0,0,0,0\n",
      "eng_2048ae6f9aa261c48e6d777bcc5b38bf,0,1,1,1,1,1\n",
      "eng_07781aa88e61e7c0a996abd1e5ea3a20,0,0,0,0,0,0\n",
      "eng_153d96f9dc27f0602c927223404d94b5,0,0,0,0,0,0\n",
      "eng_4ab5a4cc5c87d0af9cf4b80c301647bf,0,0,0,0,0,0\n",
      "eng_e75a95ba52930d6d72d503ab9469eb29,0,0,0,0,0,0\n",
      "eng_eb8fab668668f9959cafdecbfc0f081a,0,0,0,0,0,0\n",
      "eng_702724dc168d600e788d775c8e651f36,0,0,0,0,0,0\n",
      "eng_0efa1a3567443075db38c7ce2dcca571,0,0,0,0,0,0\n",
      "eng_d08d4243fd2786795df39c1a65dacac7,0,0,0,0,0,0\n",
      "eng_79fa99ba6989fb61ec127c6c99fc2343,0,0,0,0,0,1\n",
      "eng_30981038b71c210e97731d90e86038c5,0,0,0,0,0,0\n",
      "eng_b75e663a6fdc280171b6385b99306a3c,0,0,0,0,0,0\n",
      "eng_de2baffcfc59b672905e6f2694f672f6,0,0,0,0,0,0\n",
      "eng_d887794cce49564890a2552cdfb745d2,0,0,0,0,0,0\n",
      "eng_097b78cf6e209e6778e1fbda10d28b8d,0,0,0,0,0,0\n",
      "eng_6d29b7c72a091789d06d92157688e07f,0,0,0,0,0,0\n",
      "eng_00c797e70f1a2f50f1c59655f08581e1,1,1,1,1,1,1\n",
      "eng_f1e66eab0b9b2c4a83103eb65a67046e,0,0,0,0,0,0\n",
      "eng_4bf47d33b804477375ade2a151cb2614,0,0,0,0,0,0\n",
      "eng_6b4616355cbed93c122ca3e2369b3a0d,0,1,0,1,0,1\n",
      "eng_f011b534fb34efc8c574d2e16d3a95f5,0,0,0,0,0,0\n",
      "eng_8376c26da2f537abeada29ed927d3e01,0,0,0,0,0,0\n",
      "eng_af8597fa9be96fdfabb05c593476913c,0,0,0,0,0,0\n",
      "eng_e0f6e8dce6d6a6c13632b80100e5a6b1,0,0,0,0,0,0\n",
      "eng_48b992b30d2029785845b563a5ed5908,0,0,0,0,0,0\n",
      "eng_265277158900e90636bb6614c8f265b7,0,0,0,0,0,0\n",
      "eng_3c4dc44df877cfb232d222462dab6543,0,0,0,0,0,0\n",
      "eng_08a2c6cca0e7d33249339bbbef85c6c1,0,0,0,0,0,0\n",
      "eng_68280914d983fd99c5630ddd76e4bd95,0,0,0,0,0,0\n",
      "eng_cd13f9dc863b830f0f1a123db729eaff,0,0,0,0,0,0\n",
      "eng_6f76d66a4bd34d5ce4a94ec943385197,0,0,0,0,0,0\n",
      "eng_12e097065c75824d4121c0ec670647c1,0,0,0,0,0,0\n",
      "eng_bbd31cdfc7077e4903c3204ef1b9175f,0,0,0,0,0,0\n",
      "eng_4b4f2e1f0e1255dccef1959d2b30165b,0,0,0,0,0,0\n",
      "eng_cae0921076de11e46ad10498cc7d5441,0,0,0,0,0,0\n",
      "eng_298c63907209feee5c7cfe0bba8cb7be,0,0,0,0,0,0\n",
      "eng_47c81418327ba69c71e62f93a060ed39,0,0,0,0,0,0\n",
      "eng_1d443006327009ce2aff905147b39692,0,0,0,0,0,0\n",
      "eng_71021b3579c72f916147f751bce10e0c,0,0,0,0,0,0\n",
      "eng_de9a2ae60667aaddf4c646338764b526,0,0,0,0,0,0\n",
      "eng_c07a5a7dabfdb3965ec98128f318175f,0,0,0,0,0,0\n",
      "eng_fbceb1ae34db8daf3cf672a3d4256ddf,0,0,0,0,0,0\n",
      "eng_3fee89034fbdfa73372d1c88eb0aea59,0,0,0,0,0,0\n",
      "eng_c532b3613bde52aa43a9475f1ea0b32d,0,0,0,0,0,0\n",
      "eng_29ea84be308aaa614a463ddba1dfb36d,0,0,0,0,0,0\n",
      "eng_7b509629f338753f35fef4914bda9da6,0,0,0,0,0,0\n",
      "eng_3152400ae6bd362a4ae4405ea86070ec,0,0,0,0,0,0\n",
      "eng_c3a320d25b76f154caab9d9361b00907,0,0,0,0,0,0\n",
      "eng_3235c4f9ae1a74593f5cc6cff9277a85,0,0,0,0,0,0\n",
      "eng_0077188f1a05dd1d68f8352a9b84ca79,0,0,0,0,0,0\n",
      "eng_a390951455e707af9520c71ffc9db2d9,0,0,0,0,0,0\n",
      "eng_2b75f86d237a8699a27582ad90e42919,0,0,0,0,0,0\n",
      "eng_d21174080515b0ad8dc1d00ee4368d4f,0,0,0,0,0,0\n",
      "eng_ef00c86f2c475df7cee4fd5dd25e2bc7,0,0,0,0,0,0\n",
      "eng_74a62e8e997bc736b2894d9bc4da7661,0,0,0,0,0,0\n",
      "eng_9aad387704111e1f051fce5968ef2232,0,0,0,0,0,0\n",
      "eng_db9761cf21a3abb48b2669423ff5788a,0,0,0,0,0,0\n",
      "eng_7811dd491f8437afd9689a75ccb1ce04,0,0,0,0,0,0\n",
      "eng_437f5f40d878166878c2d32b8e16af31,0,0,0,0,0,0\n",
      "eng_a8b55f8a70d1c34f93fbdefa0350e507,0,0,0,0,0,0\n",
      "eng_01057285508602421377273897c3c0e7,0,0,0,0,0,0\n",
      "eng_175c490fa4665823f1dc2fc44e4d98fb,0,0,0,0,0,0\n",
      "eng_1feb16d41ce65927a8153742d14b5b6b,0,0,0,0,0,0\n",
      "eng_8182654f9b6144da76e1cfcc985a4b3a,0,0,0,0,0,0\n",
      "eng_03a2aea654b7697fcd72acdbf7ecf944,0,0,0,0,0,0\n",
      "eng_a4a13cea5e6750286b14ae7a8d81bd28,1,1,1,1,0,1\n",
      "eng_927ef1e092e0b61d9161dccb20b7d212,0,0,0,0,0,0\n",
      "eng_bab15926554f28dc36582b6e275b8805,0,0,0,0,0,0\n",
      "eng_938602684a67196e4bd95b313ad3919f,0,0,0,0,0,0\n",
      "eng_d6d5259e0d04bf6fc543ba5baaf273f1,0,0,0,0,0,0\n",
      "eng_bf5e5633ac2fdca9164c02022e0c99fd,0,0,0,0,0,0\n",
      "eng_31268830d496fa2970b2ff9918b3b7ba,1,1,0,1,0,1\n",
      "eng_95d7415206b7898a7486b0c69197baa2,0,0,0,0,0,0\n",
      "eng_b91a56f542d0513d636a0a019b7726eb,0,0,0,0,0,0\n",
      "eng_eeaa3e10a86dec32fbc1b9f1d156d796,0,0,0,0,0,0\n",
      "eng_e2622eb5bd4d433e6138c58d943b4043,0,0,0,0,0,0\n",
      "eng_533142c1bdb17ab3ffdf43e6e25f1406,0,0,0,0,0,0\n",
      "eng_325cd4d0e08485ec5693fcf15b14ff1f,0,0,0,0,0,0\n",
      "eng_063b19c19c369b678bf3446054569fdf,0,0,0,0,0,0\n",
      "eng_ebe1ae54e0854b322ad3c9667a2a4802,0,0,0,0,0,0\n",
      "eng_f1a522278c7fff07d7d7f3d033b5fa36,0,0,0,0,0,0\n",
      "eng_50c78c3a83be7868fb1ce0d636911f63,1,1,1,1,1,1\n",
      "eng_0f6d980d360c0a6c8f1edfdf6a791fb8,1,1,1,1,1,1\n",
      "eng_5a19fecc98ef147b13b59bd0d8caf8d5,0,0,0,0,0,0\n",
      "eng_3ad13d30d6003ac059e6c923a7a7eafb,0,0,0,0,0,0\n",
      "eng_74e531f9dcb74c5a7bfd9668012a8c74,1,1,1,1,0,1\n",
      "eng_b666fec715d97437ecb9c0c6a54378c4,0,0,0,0,0,0\n",
      "eng_5b51f6101221e28d6da47f2cb7214ee6,1,1,1,1,0,1\n",
      "eng_c12c886536597ecaddd1e94e922e8969,0,0,0,0,0,0\n",
      "eng_882dae250fd3467770c289b835a2aa83,1,1,1,1,1,1\n",
      "eng_b9cfff2304e5349d315bc474ce2899eb,0,0,0,0,0,0\n",
      "eng_3857e2c131a5b5fef5bbf2e9794bcd5e,0,0,0,0,0,0\n",
      "eng_2806bece99f0002047cc86a78a30a1d7,1,1,1,1,0,1\n",
      "eng_003fc3a198de8381e08ab3d85c7e40be,1,1,1,1,1,1\n",
      "eng_d74c7582012f631348296b2dbf10f4f6,0,0,0,0,0,0\n",
      "eng_0e62ae808071606db649622cb73dc2cb,1,1,1,1,1,1\n",
      "eng_8cef4402ae1b38e7c4593cc163bfe10c,1,1,1,1,1,1\n",
      "eng_310cb1f24f3545a53f33e005798ece95,1,1,1,1,1,1\n",
      "eng_5606f46a55984e9bf7db7034d451a1f6,0,0,0,0,0,1\n",
      "eng_ab5a1479644f5672c6337214087ecb4d,1,1,1,1,1,1\n",
      "eng_5e89647c49019ef6f4e3549bc5b41970,1,1,1,1,1,1\n",
      "eng_fd3ae133fa1ae5dac30d88c601d3171e,0,0,0,0,0,0\n",
      "eng_995661141a6abbbdef6770db6e1bd8f5,0,0,0,0,0,0\n",
      "eng_88e3a4bddcf38fe36666e6875399f6c3,1,1,1,1,1,1\n",
      "eng_5fbc07e0e76d061ad2496d55c93f4970,1,1,1,1,1,1\n",
      "eng_ef3116c56645f94bbe998051aaa49e40,0,0,0,0,0,0\n",
      "eng_4cd9e7e11035bcacb2814a1817238e75,0,0,0,0,0,1\n",
      "eng_ebd69a723309a37d9b5f384a3cd5c87c,0,0,0,0,0,0\n",
      "eng_d0b1c523b1cb02e41ca250d4d705efe6,1,1,1,1,1,1\n",
      "eng_5f408abc83c76617c0525e042a3ceb48,0,0,0,0,0,0\n",
      "eng_947ae20dbd1e2a3ba07bcdbd4b6a4eaa,1,1,1,1,1,1\n",
      "eng_3496f210f022027af26571fa2a2b6e30,0,0,0,0,0,0\n",
      "eng_4fca762d791268e973669733aa684e37,0,0,0,0,0,0\n",
      "eng_93a2492e7860be0d7ca445a97e245abe,1,1,1,1,1,1\n",
      "eng_b2f95890baff716a2d871097a1c210fe,1,1,1,1,1,1\n",
      "eng_e5cfe9e96de0f4873a4a6dbfca573be2,0,1,0,1,0,1\n",
      "eng_897afb355e98b187b9f75478bbbf2c51,1,1,1,1,1,1\n",
      "eng_f0ae86891c2da536ebf24c0f3964dca8,1,1,1,1,1,1\n",
      "eng_e231e20dbb7d3d433637dbb3d3c946a0,1,1,1,1,1,1\n",
      "eng_8ee2b2e8b53c55407f30fbf1219b1588,0,1,1,1,1,1\n",
      "eng_0eb6247db86e3b42c8dc125317957580,0,0,0,0,0,0\n",
      "eng_185e4dabf037ec90771e1bd18e621e2a,1,1,1,1,1,1\n",
      "eng_dbadd716608d71fb92621f7f8259f4e4,0,0,0,0,0,0\n",
      "eng_ac3c36cc719bdce4cab6c919b9b2429e,1,1,1,1,1,1\n",
      "eng_5d8aa4863fe2f8c26f541937ff5a0368,1,1,1,1,1,1\n",
      "eng_2fd4484b6bab80971a96b2100e20966a,0,0,0,0,0,0\n",
      "eng_0e11358da0c0e0cd8e0fdedf05a0cbba,1,1,1,1,1,1\n",
      "eng_0fb944d51bb376102a3ea6b65bafab6a,1,0,0,1,0,0\n",
      "eng_d9253eaeb206934208a57786b688c316,0,1,1,1,1,1\n",
      "eng_d841e099c41eb0fdd803e7c0a6109933,0,0,0,0,0,0\n",
      "eng_f6fc63a70cfafa2144d88c503cd4687b,1,1,1,1,1,1\n",
      "eng_2712f15ca491022233712e630d64d73d,0,0,0,0,0,0\n",
      "eng_d62d432eec0c12d2dc3b023edf67eac0,0,0,0,0,0,0\n",
      "eng_5ee585bf8f98b28a7e65035db0204410,0,0,0,0,0,0\n",
      "eng_07745499010cafb2d59ff898c49cf615,0,0,0,0,0,0\n",
      "eng_ec325c11de1d6163d8feb749d6457c20,0,0,0,0,0,0\n",
      "eng_4d616a82cd313bdd0dfdcf47b5f26795,0,0,0,0,0,0\n",
      "eng_268c8ff24dbb29bfa42a252ad198fe29,0,0,0,0,0,0\n",
      "eng_d9a5c1cc8ce24ec15a6b432204a34a9d,0,1,0,0,0,1\n",
      "eng_e71b8aa8338a1deaae7328a73a122cca,1,1,1,1,1,1\n",
      "eng_8c2492d1274d3693cb98ee3e700baba7,0,0,0,0,0,0\n",
      "eng_d7edf2c962f46f62f1140787e871c4a2,0,0,0,0,0,0\n",
      "eng_4c2694e3632178ffb50db62f48e1d75e,0,0,0,0,0,0\n",
      "eng_e343d9522d1ea4ab6d50998b81ad36b3,1,1,1,1,1,1\n",
      "eng_c15194e3c8b4bba93b3d652340460ffd,1,1,1,1,1,1\n",
      "eng_04eae1abe60bbb1373b9bb0f77a12e6b,1,1,1,1,1,1\n",
      "eng_12b06c46b376b4425fe6ffbcee59276c,1,1,1,1,1,1\n",
      "eng_13f40590af8d17dc49cbd80dd34e6be8,0,0,0,0,0,0\n",
      "eng_66f612addfe5a96a4a1a84342d86bdc4,1,1,1,1,0,1\n",
      "eng_ac17e23b4789ee87ade59dca086adf5c,0,0,0,0,0,0\n",
      "eng_a0a03d5390346e771751b3d0890224c1,0,0,0,0,0,0\n",
      "eng_b01fd24fbfb6630edba40768157dbed3,1,0,0,0,0,0\n",
      "eng_9f170d76134d91a9aae08ec49a0eb1b1,0,0,0,0,0,0\n",
      "eng_3caf621f17c15e73ad88d1d58ace76e1,1,1,1,1,0,1\n",
      "eng_d735bda395a80ff9861c0ab78c5fb9a3,1,1,1,1,1,1\n",
      "eng_576cca5b3a10908cc024bc4c4c6e61b3,0,0,0,0,0,0\n",
      "eng_e729b733cd4c356edea50e29301a8162,1,1,1,1,1,1\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "output = output_3\n",
    "for i in range(len(output)):\n",
    "    print(\",\".join([str(output.iloc[i][col]) for col in output.columns]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
