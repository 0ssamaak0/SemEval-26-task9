{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbaa52b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xQvIBwYFTqJRfU77LSqIrRJlsnpdMQGv\n",
      "To: /content/dev_phase.zip\n",
      "100% 12.0M/12.0M [00:00<00:00, 44.2MB/s]\n",
      "Archive:  dev_phase.zip\n",
      "   creating: dev_phase/\n",
      "   creating: dev_phase/subtask2/\n",
      "   creating: dev_phase/subtask3/\n",
      "   creating: dev_phase/subtask1/\n",
      "   creating: dev_phase/subtask2/train/\n",
      "   creating: dev_phase/subtask2/dev/\n",
      "   creating: dev_phase/subtask3/train/\n",
      "   creating: dev_phase/subtask3/dev/\n",
      "   creating: dev_phase/subtask1/train/\n",
      "   creating: dev_phase/subtask1/dev/\n",
      "  inflating: dev_phase/subtask2/train/arb.csv  \n",
      "  inflating: dev_phase/subtask2/train/fas.csv  \n",
      "  inflating: dev_phase/subtask2/train/eng_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask2/train/._eng_augmented.csv  \n",
      "  inflating: dev_phase/subtask2/train/zho.csv  \n",
      "  inflating: dev_phase/subtask2/train/nep.csv  \n",
      "  inflating: dev_phase/subtask2/train/spa.csv  \n",
      "  inflating: dev_phase/subtask2/train/ita.csv  \n",
      "  inflating: dev_phase/subtask2/train/urd.csv  \n",
      "  inflating: dev_phase/subtask2/train/amh.csv  \n",
      "  inflating: dev_phase/subtask2/train/arb_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask2/train/._arb_augmented.csv  \n",
      "  inflating: dev_phase/subtask2/train/tur.csv  \n",
      "  inflating: dev_phase/subtask2/train/deu.csv  \n",
      "  inflating: dev_phase/subtask2/train/eng.csv  \n",
      "  inflating: dev_phase/subtask2/train/hin.csv  \n",
      "  inflating: dev_phase/subtask2/train/hau.csv  \n",
      "  inflating: dev_phase/subtask2/dev/arb.csv  \n",
      "  inflating: dev_phase/subtask2/dev/fas.csv  \n",
      "  inflating: dev_phase/subtask2/dev/zho.csv  \n",
      "  inflating: dev_phase/subtask2/dev/nep.csv  \n",
      "  inflating: dev_phase/subtask2/dev/spa.csv  \n",
      "  inflating: dev_phase/subtask2/dev/ita.csv  \n",
      "  inflating: dev_phase/subtask2/dev/urd.csv  \n",
      "  inflating: dev_phase/subtask2/dev/amh.csv  \n",
      "  inflating: dev_phase/subtask2/dev/tur.csv  \n",
      "  inflating: dev_phase/subtask2/dev/deu.csv  \n",
      "  inflating: dev_phase/subtask2/dev/eng.csv  \n",
      "  inflating: dev_phase/subtask2/dev/hin.csv  \n",
      "  inflating: dev_phase/subtask2/dev/hau.csv  \n",
      "  inflating: dev_phase/subtask3/train/arb.csv  \n",
      "  inflating: dev_phase/subtask3/train/fas.csv  \n",
      "  inflating: dev_phase/subtask3/train/eng_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask3/train/._eng_augmented.csv  \n",
      "  inflating: dev_phase/subtask3/train/zho.csv  \n",
      "  inflating: dev_phase/subtask3/train/nep.csv  \n",
      "  inflating: dev_phase/subtask3/train/spa.csv  \n",
      "  inflating: dev_phase/subtask3/train/urd.csv  \n",
      "  inflating: dev_phase/subtask3/train/amh.csv  \n",
      "  inflating: dev_phase/subtask3/train/arb_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask3/train/._arb_augmented.csv  \n",
      "  inflating: dev_phase/subtask3/train/tur.csv  \n",
      "  inflating: dev_phase/subtask3/train/deu.csv  \n",
      "  inflating: dev_phase/subtask3/train/eng.csv  \n",
      "  inflating: dev_phase/subtask3/train/hin.csv  \n",
      "  inflating: dev_phase/subtask3/train/hau.csv  \n",
      "  inflating: dev_phase/subtask3/dev/arb.csv  \n",
      "  inflating: dev_phase/subtask3/dev/fas.csv  \n",
      "  inflating: dev_phase/subtask3/dev/zho.csv  \n",
      "  inflating: dev_phase/subtask3/dev/nep.csv  \n",
      "  inflating: dev_phase/subtask3/dev/spa.csv  \n",
      "  inflating: dev_phase/subtask3/dev/urd.csv  \n",
      "  inflating: dev_phase/subtask3/dev/amh.csv  \n",
      "  inflating: dev_phase/subtask3/dev/tur.csv  \n",
      "  inflating: dev_phase/subtask3/dev/deu.csv  \n",
      "  inflating: dev_phase/subtask3/dev/eng.csv  \n",
      "  inflating: dev_phase/subtask3/dev/hin.csv  \n",
      "  inflating: dev_phase/subtask3/dev/hau.csv  \n",
      "  inflating: dev_phase/subtask1/train/arb.csv  \n",
      "  inflating: dev_phase/subtask1/train/fas.csv  \n",
      "  inflating: dev_phase/subtask1/train/eng_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask1/train/._eng_augmented.csv  \n",
      "  inflating: dev_phase/subtask1/train/zho.csv  \n",
      "  inflating: dev_phase/subtask1/train/nep.csv  \n",
      "  inflating: dev_phase/subtask1/train/spa.csv  \n",
      "  inflating: dev_phase/subtask1/train/ita.csv  \n",
      "  inflating: dev_phase/subtask1/train/urd.csv  \n",
      "  inflating: dev_phase/subtask1/train/amh.csv  \n",
      "  inflating: dev_phase/subtask1/train/arb_augmented.csv  \n",
      "  inflating: __MACOSX/dev_phase/subtask1/train/._arb_augmented.csv  \n",
      "  inflating: dev_phase/subtask1/train/tur.csv  \n",
      "  inflating: dev_phase/subtask1/train/deu.csv  \n",
      "  inflating: dev_phase/subtask1/train/eng.csv  \n",
      "  inflating: dev_phase/subtask1/train/hin.csv  \n",
      "  inflating: dev_phase/subtask1/train/hau.csv  \n",
      "  inflating: dev_phase/subtask1/dev/arb.csv  \n",
      "  inflating: dev_phase/subtask1/dev/fas.csv  \n",
      "  inflating: dev_phase/subtask1/dev/zho.csv  \n",
      "  inflating: dev_phase/subtask1/dev/nep.csv  \n",
      "  inflating: dev_phase/subtask1/dev/spa.csv  \n",
      "  inflating: dev_phase/subtask1/dev/ita.csv  \n",
      "  inflating: dev_phase/subtask1/dev/urd.csv  \n",
      "  inflating: dev_phase/subtask1/dev/amh.csv  \n",
      "  inflating: dev_phase/subtask1/dev/tur.csv  \n",
      "  inflating: dev_phase/subtask1/dev/deu.csv  \n",
      "  inflating: dev_phase/subtask1/dev/eng.csv  \n",
      "  inflating: dev_phase/subtask1/dev/hin.csv  \n",
      "  inflating: dev_phase/subtask1/dev/hau.csv  \n"
     ]
    }
   ],
   "source": [
    "# Install gdown if needed\n",
    "# Replace the ID below with your actual file ID from the Drive link\n",
    "# (The ID is the long string of random characters in the URL)\n",
    "file_id = '1xQvIBwYFTqJRfU77LSqIrRJlsnpdMQGv'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = 'dev_phase.zip'\n",
    "\n",
    "!gdown {url} -O {output}\n",
    "\n",
    "!unzip {output}\n",
    "\n",
    "# Delete __MACOSX directory (if exists) and the dev_phase.zip file (cleanup)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"__MACOSX\"):\n",
    "    shutil.rmtree(\"__MACOSX\")\n",
    "\n",
    "if os.path.exists(\"dev_phase.zip\"):\n",
    "    os.remove(\"dev_phase.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf088005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "# from logs import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2508329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# CONFIG\n",
    "NUM_TYPES = 5\n",
    "NUM_MANIFESTATIONS = 6\n",
    "datasets_merge = True\n",
    "lang = \"arb\"\n",
    "trial_id = \"0000NG3\"\n",
    "model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\", \"FacebookAI/xlm-roberta-large\", \"0ssamaak0/roberta-base-LEGO_emotions\", \"FacebookAI/roberta-base\", \"cardiffnlp/twitter-roberta-base\", \"cardiffnlp/roberta-base-emotion\", \"UBC-NLP/ARBERTv2\"]\n",
    "model_name = model_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6407ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"./dev_phase/subtask1/train/\" + lang + \"_augmented.csv\")\n",
    "train_2 = pd.read_csv(\"./dev_phase/subtask2/train/\" + lang + \"_augmented.csv\")\n",
    "train_3 = pd.read_csv(\"./dev_phase/subtask3/train/\" + lang + \"_augmented.csv\")\n",
    "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/\" + lang + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e844f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
    "    self.texts=texts\n",
    "    self.labels=labels\n",
    "    self.tokenizer= tokenizer\n",
    "    self.max_length = max_length # Store max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    text=self.texts[idx]\n",
    "    label=self.labels[idx]\n",
    "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
    "\n",
    "    # Ensure consistent tensor conversion for all items\n",
    "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "    item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8900bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e48d6339f943e39dba03029411accf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74739154f1fd486781d1c2a3e8ed1497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8271b31fb7514fe1a7ac550657cba2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7bca7d170f44d09339b628eba4e3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e73192a8154212bd9666e0ca644cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "# Prepare label columns separately for each task, fallback to the correct columns per train DataFrame\n",
    "def get_label_columns(df):\n",
    "    return [col for col in df.columns if col not in ['id', 'text']]\n",
    "\n",
    "# Split indices once and reuse for all datasets to ensure same split\n",
    "n_samples = len(train_1)\n",
    "indices = np.arange(n_samples)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "if datasets_merge:\n",
    "    # Merge all datasets on 'id'\n",
    "    merged = train_1.merge(train_2, on=['id', 'text'], how='outer', suffixes=('_1', '_2'))\n",
    "    # For the third, avoid duplicate columns of 'text', so drop redundant one, or merge only on id\n",
    "    merged = merged.merge(train_3, on=['id', 'text'], how='outer', suffixes=('', '_3'))\n",
    "    # Get label columns: all columns excluding 'id' and 'text'\n",
    "    merged_label_columns = get_label_columns(merged)\n",
    "    texts = merged['text'].tolist()\n",
    "    labels = merged[merged_label_columns].values.tolist()\n",
    "    texts_train = [texts[i] for i in train_indices]\n",
    "    texts_val = [texts[i] for i in val_indices]\n",
    "    labels_train = [labels[i] for i in train_indices]\n",
    "    labels_val = [labels[i] for i in val_indices]\n",
    "    train_dataset = PolarizationDataset(texts_train, labels_train, tokenizer)\n",
    "    val_dataset = PolarizationDataset(texts_val, labels_val, tokenizer)\n",
    "else:\n",
    "    # Apply the same split to all three datasets\n",
    "    for train in [train_1, train_2, train_3]:\n",
    "        current_label_columns = get_label_columns(train)\n",
    "        texts = train['text'].tolist()\n",
    "        \n",
    "        # Use the same indices for all datasets\n",
    "        texts_train = [texts[i] for i in train_indices]\n",
    "        texts_val = [texts[i] for i in val_indices]\n",
    "        \n",
    "        if current_label_columns:\n",
    "            labels = train[current_label_columns].values.tolist()\n",
    "            labels_train = [labels[i] for i in train_indices]\n",
    "            labels_val = [labels[i] for i in val_indices]\n",
    "        else:\n",
    "            labels_train = [[] for _ in texts_train]\n",
    "            labels_val = [[] for _ in texts_val]\n",
    "        \n",
    "        train_datasets.append(PolarizationDataset(texts_train, labels_train, tokenizer))\n",
    "        val_datasets.append(PolarizationDataset(texts_val, labels_val, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c696cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(labels_matrix):\n",
    "    # labels_matrix is a list of lists or numpy array\n",
    "    labels_np = np.array(labels_matrix)\n",
    "    num_pos = labels_np.sum(axis=0)\n",
    "    num_neg = len(labels_np) - num_pos\n",
    "    \n",
    "    # Simple ratio: if 10 pos and 90 neg, weight is 9.0\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    weights = num_neg / (num_pos + 1e-5)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "pos_weight_2 = torch.ones([NUM_TYPES]) * 5.0  # Penalize missing a type 5x more\n",
    "pos_weight_3 = torch.ones([NUM_MANIFESTATIONS]) * 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500cca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMTLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_types, num_manifestations, pos_weight_2=None, pos_weight_3=None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.num_types = num_types\n",
    "        self.num_manifestations = num_manifestations\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.head1 = nn.Linear(hidden_size, 1)\n",
    "        self.head2 = nn.Linear(hidden_size, num_types)\n",
    "        self.head3 = nn.Linear(hidden_size, num_manifestations)\n",
    "\n",
    "        self.register_buffer(\"pos_weight_2\", pos_weight_2 if pos_weight_2 is not None else torch.tensor([]))\n",
    "        self.register_buffer(\"pos_weight_3\", pos_weight_3 if pos_weight_3 is not None else torch.tensor([]))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        H = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        logits1 = self.head1(H)\n",
    "        logits2 = self.head2(H)\n",
    "        logits3 = self.head3(H)\n",
    "        logits = torch.cat([logits1, logits2, logits3], dim=-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            y1_true = labels[:, :1]\n",
    "            y2_true = labels[:, 1:1 + self.num_types]\n",
    "            y3_true = labels[:, 1 + self.num_types:]\n",
    "\n",
    "            device = logits1.device\n",
    "\n",
    "            loss1 = nn.BCEWithLogitsLoss()(logits1, y1_true)\n",
    "\n",
    "            if self.pos_weight_2.numel() > 0:\n",
    "                loss2 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_2.to(device))(logits2, y2_true)\n",
    "            else:\n",
    "                loss2 = nn.BCEWithLogitsLoss()(logits2, y2_true)\n",
    "\n",
    "            if self.pos_weight_3.numel() > 0:\n",
    "                loss3 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_3.to(device))(logits3, y3_true)\n",
    "            else:\n",
    "                loss3 = nn.BCEWithLogitsLoss()(logits3, y3_true)\n",
    "\n",
    "            loss = (loss1 + loss2 + loss3) / 3.0\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb1b04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1611e355404370853f342f3d47775e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/753 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb16754e379f4ba0a7af7ab6e901fdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_pos_weights(df, label_cols):\n",
    "    labels = df[label_cols].values\n",
    "    pos = labels.sum(axis=0)\n",
    "    neg = (labels == 0).sum(axis=0)\n",
    "    weights = torch.tensor(neg / (pos + 1e-5), dtype=torch.float)\n",
    "    return weights\n",
    "\n",
    "pos_weight_2 = compute_pos_weights(train_2, train_2.columns[2:])\n",
    "pos_weight_3 = compute_pos_weights(train_3, train_3.columns[2:])\n",
    "\n",
    "model = SharedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS, pos_weight_2, pos_weight_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902a37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    # Extract Binary Classification (Subtask 1)\n",
    "    y1_true = labels[:, 0]\n",
    "    y1_pred = preds[:, 0]\n",
    "\n",
    "    # --- LOGICAL GATING START ---\n",
    "    # Create a mask from Subtask 1 predictions\n",
    "    # Shape becomes (Batch_Size, 1) to broadcast over the other subtasks\n",
    "    mask = y1_pred[:, None] \n",
    "    \n",
    "    # Extract raw predictions for Subtask 2 & 3\n",
    "    y2_pred_raw = preds[:, 1:1+NUM_TYPES]\n",
    "    y3_pred_raw = preds[:, 1+NUM_TYPES:]\n",
    "    \n",
    "    # Apply the mask: If y1_pred is 0, force y2 and y3 to be 0\n",
    "    y2_pred = y2_pred_raw * mask\n",
    "    y3_pred = y3_pred_raw * mask\n",
    "    # --- LOGICAL GATING END ---\n",
    "\n",
    "    # Extract True Labels for Subtask 2 & 3\n",
    "    y2_true = labels[:, 1:1+NUM_TYPES]\n",
    "    y3_true = labels[:, 1+NUM_TYPES:]\n",
    "\n",
    "    return {\n",
    "        \"subtask_1/f1_macro\": f1_score(y1_true, y1_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_2/f1_macro\": f1_score(y2_true, y2_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_3/f1_macro\": f1_score(y3_true, y3_pred, average=\"macro\", zero_division=0),    }\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{trial_id}\",\n",
    "    num_train_epochs=15,              # Increase max epochs, let EarlyStopping handle the cut\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,   # 64 might be too stable? 32 adds some noise (good for regularization)\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,                # Add Weight Decay!\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # Must save to load best\n",
    "    load_best_model_at_end=True,      # Automatically load the best checkpoint\n",
    "    metric_for_best_model=\"eval_subtask_1/f1_macro\", # Optimize for the hardest metric or the main task\n",
    "    save_total_limit=2,               # Don't fill disk\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca0e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1274' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1274/1470 10:11 < 01:34, 2.08 it/s, Epoch 13/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Subtask 1/f1 Macro</th>\n",
       "      <th>Subtask 2/f1 Macro</th>\n",
       "      <th>Subtask 3/f1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.801500</td>\n",
       "      <td>0.593499</td>\n",
       "      <td>0.816061</td>\n",
       "      <td>0.581012</td>\n",
       "      <td>0.585893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.556582</td>\n",
       "      <td>0.842954</td>\n",
       "      <td>0.610778</td>\n",
       "      <td>0.614589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.657979</td>\n",
       "      <td>0.836043</td>\n",
       "      <td>0.628343</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.677746</td>\n",
       "      <td>0.848204</td>\n",
       "      <td>0.645960</td>\n",
       "      <td>0.631834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.717257</td>\n",
       "      <td>0.844380</td>\n",
       "      <td>0.682468</td>\n",
       "      <td>0.646173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.745745</td>\n",
       "      <td>0.851850</td>\n",
       "      <td>0.659754</td>\n",
       "      <td>0.644457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.784631</td>\n",
       "      <td>0.849525</td>\n",
       "      <td>0.672327</td>\n",
       "      <td>0.667840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.771890</td>\n",
       "      <td>0.852097</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.665268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.827781</td>\n",
       "      <td>0.858341</td>\n",
       "      <td>0.691743</td>\n",
       "      <td>0.672751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.829893</td>\n",
       "      <td>0.859869</td>\n",
       "      <td>0.695349</td>\n",
       "      <td>0.673462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.883869</td>\n",
       "      <td>0.851885</td>\n",
       "      <td>0.680183</td>\n",
       "      <td>0.674160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.853384</td>\n",
       "      <td>0.857307</td>\n",
       "      <td>0.690767</td>\n",
       "      <td>0.670481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.894741</td>\n",
       "      <td>0.852097</td>\n",
       "      <td>0.691514</td>\n",
       "      <td>0.670910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: \n",
      "subtask_1 f1_macro: 0.8599 \n",
      "subtask_2 f1_macro: 0.6953 \n",
      "subtask_3 f1_macro: 0.6735\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\n",
    "    \"Validation Results:\",\n",
    "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b392db14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  21/1470 00:05 < 07:27, 3.24 it/s, Epoch 0.20/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-184575189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Evaluate the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2677\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m                     ):\n\u001b[1;32m   2681\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MARBERTv2 RESULTS\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\n",
    "    \"Validation Results:\",\n",
    "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7c00c",
   "metadata": {},
   "source": [
    "# Log Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ddef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Experiment results logged to logs.json (trial_id: 0000NG3)\n",
      "  - subtask_1: eng\n",
      "  - subtask_2: eng\n",
      "  - subtask_3: eng\n"
     ]
    }
   ],
   "source": [
    "# Log the experiment results - each subtask separately\n",
    "\n",
    "# Prepare metadata for the experiment\n",
    "experiment_metadata = {\n",
    "    \"approach\": \"MTL_no_gate\",\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,\n",
    "    \"num_types\": NUM_TYPES,\n",
    "    \"num_manifestations\": NUM_MANIFESTATIONS,\n",
    "    \"datasets_merge\": datasets_merge,\n",
    "    \"posweight\": \"True\"\n",
    "}\n",
    "\n",
    "# Only log eval_f1_macro for each subtask\n",
    "subtask_1_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_1/f1_macro\"),\n",
    "}\n",
    "subtask_2_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_2/f1_macro\"),\n",
    "}\n",
    "subtask_3_results = {\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_3/f1_macro\"),\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "# Attempt to load existing logs and merge metadata for this trial if present\n",
    "existing_metadata = {}\n",
    "try:\n",
    "    with open(\"logs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        logs = json.load(f)\n",
    "        if isinstance(logs, dict):\n",
    "            logs = [logs]\n",
    "        for trial in logs:\n",
    "            if trial.get(\"trial_id\") == trial_id and \"metadata\" in trial:\n",
    "                existing_metadata = trial[\"metadata\"].copy()\n",
    "                break\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    pass\n",
    "\n",
    "# Only add/replace model_{lang}, don't overwrite the whole metadata\n",
    "merged_metadata = dict(existing_metadata)\n",
    "merged_metadata.update({\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"approach\": experiment_metadata[\"approach\"],\n",
    "    \"learning_rate\": experiment_metadata[\"learning_rate\"],\n",
    "    \"num_train_epochs\": experiment_metadata[\"num_train_epochs\"],\n",
    "    \"per_device_train_batch_size\": experiment_metadata[\"per_device_train_batch_size\"],\n",
    "    \"per_device_eval_batch_size\": experiment_metadata[\"per_device_eval_batch_size\"],\n",
    "    \"num_types\": experiment_metadata[\"num_types\"],\n",
    "    \"num_manifestations\": experiment_metadata[\"num_manifestations\"],\n",
    "    \"datasets_merge\": experiment_metadata[\"datasets_merge\"]\n",
    "})\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_1\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_1_results,\n",
    "    metadata=merged_metadata,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "# Log subtask_2 and subtask_3 using the same trial_id and do not pass metadata to avoid overwrite\n",
    "log(\n",
    "    subtask_name=\"subtask_2\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_2_results,\n",
    "    metadata=None,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_3\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_3_results,\n",
    "    metadata=None,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Experiment results logged to logs.json (trial_id: {trial_id})\")\n",
    "print(f\"  - subtask_1: {lang}\")\n",
    "print(f\"  - subtask_2: {lang}\")\n",
    "print(f\"  - subtask_3: {lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654cdde",
   "metadata": {},
   "source": [
    "# Predict on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe02703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved for all 3 dev sets with Logical Gating applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load dev 1 and predict all 3 dev sets\n",
    "dev_1 = pd.read_csv(f\"./dev_phase/subtask1/dev/{lang}.csv\")\n",
    "dev_2 = pd.read_csv(f\"./dev_phase/subtask2/dev/{lang}.csv\")\n",
    "dev_3 = pd.read_csv(f\"./dev_phase/subtask3/dev/{lang}.csv\")\n",
    "\n",
    "# Create dataset from dev 1 texts (all dev sets have same texts)\n",
    "dev_texts = dev_1['text'].tolist()\n",
    "# Dummy labels for prediction\n",
    "dev_dataset = PolarizationDataset(dev_texts, [[0]*12]*len(dev_texts), tokenizer)\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.predict(dev_dataset)\n",
    "logits = predictions.predictions\n",
    "if isinstance(logits, tuple):\n",
    "    logits = logits[0]\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Extract predictions for Subtask 1\n",
    "polarization_preds = preds[:, 0]\n",
    "\n",
    "# --- LOGICAL GATING START ---\n",
    "# Create mask based on Subtask 1 (N, 1)\n",
    "mask = polarization_preds[:, None]\n",
    "\n",
    "# Apply mask to Subtasks 2 and 3\n",
    "# If polarization is 0, these predictions become 0 regardless of model output\n",
    "types_preds = preds[:, 1:1+NUM_TYPES] * mask\n",
    "manifestations_preds = preds[:, 1+NUM_TYPES:] * mask\n",
    "# --- LOGICAL GATING END ---\n",
    "\n",
    "# Create output DataFrames\n",
    "output_1 = dev_1[['id', 'text']].copy()\n",
    "output_1['polarization'] = polarization_preds\n",
    "\n",
    "output_2 = dev_2[['id', 'text']].copy()\n",
    "type_cols = [col for col in dev_2.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(type_cols):\n",
    "    output_2[col] = types_preds[:, i]\n",
    "\n",
    "output_3 = dev_3[['id', 'text']].copy()\n",
    "manifest_cols = [col for col in dev_3.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(manifest_cols):\n",
    "    output_3[col] = manifestations_preds[:, i]\n",
    "\n",
    "# Drop the 'text' column before saving\n",
    "output_1 = output_1.drop(columns=['text'])\n",
    "output_2 = output_2.drop(columns=['text'])\n",
    "output_3 = output_3.drop(columns=['text'])\n",
    "\n",
    "# Create dir under results with trial_id\n",
    "os.makedirs(f\"./results/{trial_id}\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_1\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_2\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_3\", exist_ok=True)\n",
    "\n",
    "# Save predictions to subtask_ directories\n",
    "output_1.to_csv(f\"./results/{trial_id}/subtask_1/pred_{lang}.csv\", index=False)\n",
    "output_2.to_csv(f\"./results/{trial_id}/subtask_2/pred_{lang}.csv\", index=False)\n",
    "output_3.to_csv(f\"./results/{trial_id}/subtask_3/pred_{lang}.csv\", index=False)\n",
    "\n",
    "print(f\"Predictions saved for all 3 dev sets with Logical Gating applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6afe8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arb_67be47e5216d7bee41e17484e619f4e6,1,1,0,0,0,0\n",
      "arb_272322e5b265e177613d685e5619e402,0,1,1,0,0,0\n",
      "arb_d1ec38dd0ec5d7a4fe28ef8317fc96c1,0,1,0,1,0,0\n",
      "arb_fad75310b17c124d98ebc514189ec033,1,1,1,1,1,0\n",
      "arb_95caf70cec5bf00c94c35cf7af2a0ab5,1,1,0,1,0,0\n",
      "arb_ac108c1ecf5071892c61abd253847b15,0,1,0,1,0,0\n",
      "arb_adaaa6d482119e65ce337ee224674e70,1,1,0,1,1,0\n",
      "arb_2794b08cac6cc9394a68c51cfc436243,0,1,0,0,0,0\n",
      "arb_19dd96c989323c9e950a2c3ab9c285be,0,1,0,0,0,0\n",
      "arb_f2bd638d9d9fc7a617130ff2b198b562,1,1,0,0,0,1\n",
      "arb_f992bf7776b854d4f7f8475aebf80f49,1,1,0,1,1,0\n",
      "arb_0b5ac70e86926f5e84cad94028864a37,0,1,0,0,0,0\n",
      "arb_8ababf95f952e2425c2df1033192dac0,0,0,0,0,0,0\n",
      "arb_06cd19aac6cc52e394a22d7d1dd58efc,0,0,0,0,0,0\n",
      "arb_12eeeb8d2fa2d04be2ed9830d5f36ce9,0,0,0,0,0,0\n",
      "arb_5bc23bacf9a161cd0f99719c70681a81,1,1,0,1,1,0\n",
      "arb_9ee7c931ab1ecd655533042d8301f6bb,0,0,0,0,0,0\n",
      "arb_bb7c40559f3a7ca1ecdd7dd7c136198f,1,1,0,1,0,0\n",
      "arb_5d394c0cce56675e2fc36a0590b47ed7,0,0,0,0,0,0\n",
      "arb_0704305e8313650e672563a2d073384f,0,0,0,0,0,0\n",
      "arb_e56b759d14fd70506e01cf971315453d,0,0,0,0,0,0\n",
      "arb_d286a2aac63432acef285a4799041f55,0,0,0,0,0,0\n",
      "arb_5a9f322a530e85cd640f21af5c6bae42,0,0,0,0,0,0\n",
      "arb_133c4737c4e04a8991fc7c219106b4e4,0,1,1,1,0,0\n",
      "arb_f03fd416ab63cbb8f9f92a020fdd46be,0,0,0,0,0,0\n",
      "arb_b8fbf2253ba4b8a83829b964012d1a9b,1,1,0,1,1,0\n",
      "arb_4188caa3678846451bb812d7a7902459,0,1,0,0,0,0\n",
      "arb_c0224b694b38805f15fae4137121302e,1,0,0,1,0,0\n",
      "arb_3991e487126513829ec24ab5c1f1e325,1,1,0,0,0,0\n",
      "arb_9861662780205d38780ea04feed598dd,0,1,0,0,0,0\n",
      "arb_2b14abe9527a935117ae771612afca21,1,0,0,1,1,0\n",
      "arb_925eb03f72f22102e242a475f1f54530,1,1,0,1,0,0\n",
      "arb_c531bdc9a777137d8aa23a3bf529f87e,1,1,0,1,1,1\n",
      "arb_51a6eb82ab53edd30fdd18b6940dc275,0,0,0,0,0,0\n",
      "arb_6aa01210c44d11393ea419d6781b89a9,0,0,0,1,0,0\n",
      "arb_0c8c4d1e368fa6081c8840c3924b3711,0,0,0,0,0,0\n",
      "arb_f15fdbdb119f76f430f7cf7c773c3590,0,1,0,0,1,0\n",
      "arb_9b66d2e70ea93d6d37d0b656c1b1e880,0,1,1,1,0,1\n",
      "arb_773241d9837d36ed8962eacefa5143ef,1,1,1,1,0,0\n",
      "arb_b8f51143c06fbaed500ee2e7e7f5ac14,1,1,0,1,1,0\n",
      "arb_99dc675d6e1f9acdc7714fa066b4242b,1,1,0,1,1,0\n",
      "arb_3bcca38054a14240945f770aebe23355,1,0,0,1,1,0\n",
      "arb_730fba0ced42edd151f49218959abcdc,1,1,1,1,1,1\n",
      "arb_1a960ef19b8ebbeba5d8a48f2da3baf7,0,0,0,0,0,0\n",
      "arb_60a8455969e0277c96a31b2ab8f57bb6,0,0,0,0,0,0\n",
      "arb_8e46849f4110205740552b9b3d8afcc8,0,0,0,0,0,0\n",
      "arb_6535199985cd1f867d14c01e1780ed45,0,0,0,0,0,0\n",
      "arb_90448d504fbf1cdf02fe55dd474078fe,0,0,0,0,0,0\n",
      "arb_650c8d322f6c2ea8be78385ac29ad57d,0,0,0,0,0,0\n",
      "arb_8e08d6c50f1bc16d8aa16c4fcc461702,0,0,0,0,0,0\n",
      "arb_aafb2df7e4108d4d09ffefa74811ea2e,0,0,0,0,0,0\n",
      "arb_3a9070f93fa0aed9b4c7d7974d6134fe,0,0,0,0,0,0\n",
      "arb_a25e888067bd682f8712cc254a94450d,0,0,0,0,0,0\n",
      "arb_69b3fac31a9776e98df1f2ed3a37f743,0,0,0,0,0,0\n",
      "arb_70d64fdc7707ba7b23590bc3987a22bc,0,0,0,0,0,0\n",
      "arb_3561147626932826873155b2cb70e11e,0,0,0,0,1,0\n",
      "arb_757e07f4b35078992f4e9656cb4ea948,0,0,0,0,0,0\n",
      "arb_954a7a38de550d4b5252483bb9b748ea,0,0,0,0,0,0\n",
      "arb_a9847cdfa1abb179e11c85955a5f49bf,0,0,0,0,0,0\n",
      "arb_46b4284e3ace41c2806a041162aa1a39,0,0,0,0,0,0\n",
      "arb_8e5e356a5c81496169940989d545c6ac,0,0,0,0,0,0\n",
      "arb_c30e1b90992cee20688833ca1e382de0,0,0,0,0,0,0\n",
      "arb_0340513e0b537d1db0934fc235651218,0,0,0,0,0,0\n",
      "arb_726332fa6d54c653d55f5c7b42deb977,0,0,0,0,0,0\n",
      "arb_88c88e64e71c1e1b9222a49458d448ee,0,0,0,0,0,0\n",
      "arb_031d376aa7923c4268ac73dc2b9334a9,0,0,0,0,0,0\n",
      "arb_1cbab69c6f39bfd5484bd5832ece0ac0,0,0,0,0,0,0\n",
      "arb_ce785dd8c5b8140363e2e057dae241ce,0,0,0,0,0,0\n",
      "arb_60d2f094428bbcbde294086a7f75d5d9,0,0,0,0,0,0\n",
      "arb_28552d3893a580bf74bb1e4711c30e29,0,0,0,0,0,0\n",
      "arb_7e021fb631c018eb7a1dc96bb3b7d3f5,0,0,0,0,0,0\n",
      "arb_1429c399c30de8d7188cdd239c91aaa6,0,0,0,0,0,0\n",
      "arb_1a1c75b0f7b1b571c157dd83243f81c6,0,0,0,0,0,0\n",
      "arb_143932e3ea47fd0d8d45ff79c8e9a529,0,0,0,0,0,0\n",
      "arb_c0a60d5d5cb40fbd604c9c3e0b3098bf,0,0,0,0,0,0\n",
      "arb_13636b23e853783423f760db617520a9,0,0,0,0,0,0\n",
      "arb_6c8048c166f7ee2e8a94ca3650335345,0,1,0,1,1,0\n",
      "arb_8530294a4c91bae6bcbc9d7888b8d183,0,1,0,0,0,0\n",
      "arb_c4c60186c0b797433e86f3f08cd77e7b,1,1,0,1,1,0\n",
      "arb_2de8255707465f99180bf1c58204816b,0,0,0,0,0,0\n",
      "arb_52f0037e1b1196f8462aba17ce30b0bd,0,0,0,0,0,0\n",
      "arb_05a11d7f973c5b33f51a53a92a08c9f7,0,0,0,0,0,0\n",
      "arb_cfdb8f2486dbb02404f69c104509e956,0,0,0,0,0,0\n",
      "arb_119473be95ebd1ff087a086a8cdf67f0,0,0,0,0,0,0\n",
      "arb_06aa7b6d64858e1ea54c8d5c093a1385,0,0,0,0,0,0\n",
      "arb_58683c41f6320b304993fbc9ce8e559c,0,0,0,0,0,0\n",
      "arb_ae60c3911c8290e556ffdfa0e73ea408,0,0,0,0,0,0\n",
      "arb_c6624ecf4359e15fe5298b8878a53630,0,0,0,0,0,0\n",
      "arb_2228fe1e68bf10ac10d74c8f24ed672f,0,0,0,0,0,0\n",
      "arb_0d027dd29b767b3cb54141a67da833a8,0,0,0,0,0,0\n",
      "arb_f5345aeade0452ce211afa3b91a2a98a,0,0,0,0,0,0\n",
      "arb_fb4217f3ac08ddbbe93b87607d9e7eea,0,0,0,0,0,0\n",
      "arb_6eaa4e896b8b2056308fba14b04d1323,0,0,0,0,0,0\n",
      "arb_0c49ebfdc0e82c39fb952a55a2eec2a6,0,0,0,0,0,0\n",
      "arb_d380557a1e7c162fbf0bf8bb8cb7c374,0,0,0,0,0,0\n",
      "arb_445262028526e3ac19d4a7617b89312f,0,0,0,0,0,0\n",
      "arb_5078361bd8c9205ae1abd0f554b413a8,1,0,0,0,1,0\n",
      "arb_d8fc83089c56da4eaa4ea5dc1c8385cd,1,1,1,1,1,0\n",
      "arb_e203ea25f0d5899540684c600c27e313,1,1,0,1,1,1\n",
      "arb_8af35c96dd0ab3abdd243488210d41fb,1,1,1,1,1,1\n",
      "arb_80e0c1d020bb3e68c57092a5da055789,1,1,0,1,1,0\n",
      "arb_e9afce419a9fd57fd8fde9130084d86f,1,1,0,1,1,0\n",
      "arb_4208815a310227400e1d9b718aa3201b,1,1,1,1,1,0\n",
      "arb_f78e2ee76eaf751e219c0a6c24f17477,1,1,1,1,1,1\n",
      "arb_9444af3fa26415a2e1a1215b618b77fc,1,1,0,1,1,0\n",
      "arb_8ca54749e8a4cb192d605e8d0fed5077,1,1,0,1,0,0\n",
      "arb_985e458b817a04e5b4b856bdd66baad1,1,1,0,1,1,0\n",
      "arb_2ae6d2826d3f741c81074893b7368e8f,0,0,0,0,0,0\n",
      "arb_752cbd7c5fa526b02871ee2174b75260,1,1,1,1,0,0\n",
      "arb_d4e513d177a18cd0361907f0abcee264,0,0,0,0,0,0\n",
      "arb_3fd0b4dcbfd27d815c7c1600ed523acc,1,1,0,0,0,0\n",
      "arb_a5714ea7faf17fb64bce39550b8f66a0,0,1,0,1,1,0\n",
      "arb_dbb306bb98f778a984aa4e89a2a0c69c,1,1,0,0,1,0\n",
      "arb_8fe3d20ecd2bffca8608ad581e1d450d,1,1,0,1,1,1\n",
      "arb_4d3ed82d7641e459b2143327303141ca,1,1,1,1,1,1\n",
      "arb_2fe0dd13d24583051d76f1efe2a6b221,0,0,0,0,0,0\n",
      "arb_328246a090fa79fae208734fb506356a,0,0,0,0,0,0\n",
      "arb_fa9bfd0d2544a6a0da7e9fdf67dcb61a,0,1,0,0,0,0\n",
      "arb_eca8c2a903579f5bc09bbae9a914192b,1,1,0,1,1,1\n",
      "arb_ae665012d4d85ec2b7f67036e8e7ef91,0,0,0,0,0,0\n",
      "arb_7b2074b411c5991657d956f2e868d0d6,0,0,0,0,0,0\n",
      "arb_a2d10460956f12da3bfad87d75c17b85,0,0,0,0,0,0\n",
      "arb_68d1cbebcef21c8b7e157d3deb68c96a,0,0,0,0,0,0\n",
      "arb_2320e4a90c9a3e16ad43013187628bf0,0,0,0,0,0,0\n",
      "arb_03c025ab5aac30fb1b817698060f88e9,0,0,0,0,0,0\n",
      "arb_28a4a6fa8a0d9478a678ead7d87141aa,0,0,0,0,0,0\n",
      "arb_26bf97302d3bbf29dc55628a7cdce3b2,0,0,0,0,0,0\n",
      "arb_6c9c12fd45b04bafdda96d36b4abf654,0,0,0,0,0,0\n",
      "arb_717eb9eb752dde1f2ba0b1f75431ecb9,1,0,0,0,0,0\n",
      "arb_22ad0f62586145ee322627cc04737aed,1,1,1,1,1,0\n",
      "arb_9965f78f084c4b52eba5ad3eaf00c383,0,1,0,0,0,0\n",
      "arb_230ff60adcb94e26bb447d4d2d2d4d87,1,1,0,1,1,0\n",
      "arb_de5e97574cb90756816e4acad906cbcb,0,0,0,0,0,0\n",
      "arb_b930858672f01ab4e30913d1a42b2fdd,0,1,0,0,0,0\n",
      "arb_9e735136b6578cfa2e617c9686b060c6,1,1,0,1,1,0\n",
      "arb_2f940f44c8882b16eedd16ceb9a22f3a,0,0,0,0,0,0\n",
      "arb_63bba78965d711655b703211386a1d66,1,1,0,1,0,0\n",
      "arb_796f4f5016b6b057af6e4d9a931be707,1,1,0,1,0,0\n",
      "arb_35b4ec4ee73ff5865897b530f86a0df2,1,1,0,1,0,0\n",
      "arb_54d246af610d9df0efc222e9f5f568ee,1,1,0,1,0,0\n",
      "arb_c03c23fb168af6d9974173940301a49e,0,1,1,0,0,1\n",
      "arb_04c10b142c039bea31abc347a05e901e,1,1,0,1,1,1\n",
      "arb_fc3a3dada9c28b31f4501b1aff617009,1,1,1,1,1,0\n",
      "arb_db683247365c64679c4cc519af44f581,0,0,0,0,0,0\n",
      "arb_aecc5b9082fe643e2b8d6f0ac0dcdfce,0,0,0,0,0,0\n",
      "arb_f17c38194db34a072fb9ffa06a3c1a36,1,1,0,1,1,0\n",
      "arb_00ba726a4ccda40d3aa9970b605643d0,0,0,0,1,1,0\n",
      "arb_466691d07d329912a743f07951be4b63,1,1,0,1,1,0\n",
      "arb_32df687e0801943c203047322024a082,0,1,1,0,0,0\n",
      "arb_5956bff1d7e2d783e032bbb840bc0b0c,0,0,0,0,0,0\n",
      "arb_8d9f1d3fdd08dccf0fe640b32ee3c75d,0,0,0,0,0,0\n",
      "arb_2f9508e688439711aadf9ffe4c7dc43c,0,0,0,0,0,0\n",
      "arb_ea84b8ff4b701131207666ad68d9b4e6,0,0,0,0,0,0\n",
      "arb_c3baa08c705605f300bacbfd7b32cdb5,0,0,0,0,0,0\n",
      "arb_968554a6b5723334ac89acd80d35f5f3,0,0,0,0,0,0\n",
      "arb_63849e6cf2a4b209a3a7c88cf689c09f,1,1,0,1,1,0\n",
      "arb_7c14ba01ef8cdc91ee907afe06c6d57b,0,1,0,0,0,0\n",
      "arb_d90e03b791ee0a44e1ff732b3502cff2,0,1,0,0,0,0\n",
      "arb_a614e47120a6cc5ab2c22d062f799d1d,1,1,0,0,1,0\n",
      "arb_d0bec4c3e835773e91bade24082405dc,0,0,0,0,0,0\n",
      "arb_54d3d10c58ab60fe3bc8135cec8a7745,1,1,0,0,0,0\n",
      "arb_3a6d9f8b7468345e5fe80353682ea2b6,1,1,0,1,1,0\n",
      "arb_a74cb5b1ac1b5e1e3297ec29040104f7,0,0,0,0,1,0\n",
      "arb_902c5ae816b9323758ed8e33424c5427,0,0,0,0,0,0\n",
      "arb_7d86f321f6d638c75df7ed4645bdabbc,0,0,0,0,0,0\n",
      "arb_452c08a3d812c6786509a33f13bdca7a,1,1,0,1,1,0\n",
      "arb_fee827860874f8bf04cfdaca1a0bff7a,0,0,0,0,0,0\n",
      "arb_d072529aace0ca637b85d8bb968577c0,0,0,0,0,0,0\n",
      "arb_e564370d0a844c67763dafdd2ede0411,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "output = output_3\n",
    "for i in range(len(output)):\n",
    "    print(\",\".join([str(output.iloc[i][col]) for col in output.columns]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
