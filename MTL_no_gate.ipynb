{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf088005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from logs import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2508329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# CONFIG\n",
    "NUM_TYPES = 5\n",
    "NUM_MANIFESTATIONS = 6\n",
    "datasets_merge = True\n",
    "lang = \"eng\"\n",
    "trial_id = \"0000NG1\"\n",
    "model_names = ['bert-base-uncased', \"UBC-NLP/MARBERTv2\", \"microsoft/deberta-v3-base\"]\n",
    "model_name = model_names[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6407ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv(\"./dev_phase/subtask1/train/\" + lang + \".csv\")\n",
    "train_2 = pd.read_csv(\"./dev_phase/subtask2/train/\" + lang + \".csv\")\n",
    "train_3 = pd.read_csv(\"./dev_phase/subtask3/train/\" + lang + \".csv\")\n",
    "dev_df = pd.read_csv(\"./dev_phase/subtask1/dev/\" + lang + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e844f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
    "    self.texts=texts\n",
    "    self.labels=labels\n",
    "    self.tokenizer= tokenizer\n",
    "    self.max_length = max_length # Store max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    text=self.texts[idx]\n",
    "    label=self.labels[idx]\n",
    "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
    "\n",
    "    # Ensure consistent tensor conversion for all items\n",
    "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
    "    item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8900bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "# Prepare label columns separately for each task, fallback to the correct columns per train DataFrame\n",
    "def get_label_columns(df):\n",
    "    return [col for col in df.columns if col not in ['id', 'text']]\n",
    "\n",
    "# Split indices once and reuse for all datasets to ensure same split\n",
    "n_samples = len(train_1)\n",
    "indices = np.arange(n_samples)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "if datasets_merge:\n",
    "    # Merge all datasets on 'id'\n",
    "    merged = train_1.merge(train_2, on=['id', 'text'], how='outer', suffixes=('_1', '_2'))\n",
    "    # For the third, avoid duplicate columns of 'text', so drop redundant one, or merge only on id\n",
    "    merged = merged.merge(train_3, on=['id', 'text'], how='outer', suffixes=('', '_3'))\n",
    "    # Get label columns: all columns excluding 'id' and 'text'\n",
    "    merged_label_columns = get_label_columns(merged)\n",
    "    texts = merged['text'].tolist()\n",
    "    labels = merged[merged_label_columns].values.tolist()\n",
    "    texts_train = [texts[i] for i in train_indices]\n",
    "    texts_val = [texts[i] for i in val_indices]\n",
    "    labels_train = [labels[i] for i in train_indices]\n",
    "    labels_val = [labels[i] for i in val_indices]\n",
    "    train_dataset = PolarizationDataset(texts_train, labels_train, tokenizer)\n",
    "    val_dataset = PolarizationDataset(texts_val, labels_val, tokenizer)\n",
    "else:\n",
    "    # Apply the same split to all three datasets\n",
    "    for train in [train_1, train_2, train_3]:\n",
    "        current_label_columns = get_label_columns(train)\n",
    "        texts = train['text'].tolist()\n",
    "        \n",
    "        # Use the same indices for all datasets\n",
    "        texts_train = [texts[i] for i in train_indices]\n",
    "        texts_val = [texts[i] for i in val_indices]\n",
    "        \n",
    "        if current_label_columns:\n",
    "            labels = train[current_label_columns].values.tolist()\n",
    "            labels_train = [labels[i] for i in train_indices]\n",
    "            labels_val = [labels[i] for i in val_indices]\n",
    "        else:\n",
    "            labels_train = [[] for _ in texts_train]\n",
    "            labels_val = [[] for _ in texts_val]\n",
    "        \n",
    "        train_datasets.append(PolarizationDataset(texts_train, labels_train, tokenizer))\n",
    "        val_datasets.append(PolarizationDataset(texts_val, labels_val, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c696cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(labels_matrix):\n",
    "    # labels_matrix is a list of lists or numpy array\n",
    "    labels_np = np.array(labels_matrix)\n",
    "    num_pos = labels_np.sum(axis=0)\n",
    "    num_neg = len(labels_np) - num_pos\n",
    "    \n",
    "    # Simple ratio: if 10 pos and 90 neg, weight is 9.0\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    weights = num_neg / (num_pos + 1e-5)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "pos_weight_2 = torch.ones([NUM_TYPES]) * 5.0  # Penalize missing a type 5x more\n",
    "pos_weight_3 = torch.ones([NUM_MANIFESTATIONS]) * 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gemini.google.com/share/04047aff5ce0\n",
    "class SharedMTLModel(nn.Module):\n",
    "    def __init__(self, model_name, num_types, num_manifestations, pos_weight_2=None, pos_weight_3=None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.num_types = num_types\n",
    "        self.num_manifestations = num_manifestations\n",
    "        \n",
    "        # Increase dropout to fight overfitting\n",
    "        self.dropout = nn.Dropout(0.2) \n",
    "\n",
    "        self.head1 = nn.Linear(hidden_size, 1)\n",
    "        self.head2 = nn.Linear(hidden_size, num_types)\n",
    "        self.head3 = nn.Linear(hidden_size, num_manifestations)\n",
    "        \n",
    "        # Register weights as buffers (part of state_dict but not trainable parameters)\n",
    "        self.register_buffer('pos_weight_2', pos_weight_2)\n",
    "        self.register_buffer('pos_weight_3', pos_weight_3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        H = outputs.last_hidden_state[:, 0, :]\n",
    "        H = self.dropout(H)\n",
    "\n",
    "        logits1 = self.head1(H)\n",
    "        logits2 = self.head2(H)\n",
    "        logits3 = self.head3(H)\n",
    "        logits = torch.cat([logits1, logits2, logits3], dim=-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            y1_true = labels[:, :1]\n",
    "            y2_true = labels[:, 1:1 + self.num_types]\n",
    "            y3_true = labels[:, 1 + self.num_types:]\n",
    "\n",
    "            loss_fct_bin = nn.BCEWithLogitsLoss()\n",
    "            # Apply weights here\n",
    "            loss_fct_2 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_2)\n",
    "            loss_fct_3 = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_3)\n",
    "\n",
    "            loss1 = loss_fct_bin(logits1, y1_true)\n",
    "            loss2 = loss_fct_2(logits2, y2_true)\n",
    "            loss3 = loss_fct_3(logits3, y3_true)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SharedMTLModel(model_name, NUM_TYPES, NUM_MANIFESTATIONS)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    # Extract Binary Classification (Subtask 1)\n",
    "    y1_true = labels[:, 0]\n",
    "    y1_pred = preds[:, 0]\n",
    "\n",
    "    # --- LOGICAL GATING START ---\n",
    "    # Create a mask from Subtask 1 predictions\n",
    "    # Shape becomes (Batch_Size, 1) to broadcast over the other subtasks\n",
    "    mask = y1_pred[:, None] \n",
    "    \n",
    "    # Extract raw predictions for Subtask 2 & 3\n",
    "    y2_pred_raw = preds[:, 1:1+NUM_TYPES]\n",
    "    y3_pred_raw = preds[:, 1+NUM_TYPES:]\n",
    "    \n",
    "    # Apply the mask: If y1_pred is 0, force y2 and y3 to be 0\n",
    "    y2_pred = y2_pred_raw * mask\n",
    "    y3_pred = y3_pred_raw * mask\n",
    "    # --- LOGICAL GATING END ---\n",
    "\n",
    "    # Extract True Labels for Subtask 2 & 3\n",
    "    y2_true = labels[:, 1:1+NUM_TYPES]\n",
    "    y3_true = labels[:, 1+NUM_TYPES:]\n",
    "\n",
    "    return {\n",
    "        \"subtask_1/accuracy\": accuracy_score(y1_true, y1_pred),\n",
    "        \"subtask_1/f1_binary\": f1_score(y1_true, y1_pred, average=\"binary\", zero_division=0),\n",
    "        \"subtask_1/f1_macro\": f1_score(y1_true, y1_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_1/f1_micro\": f1_score(y1_true, y1_pred, average=\"micro\", zero_division=0),\n",
    "\n",
    "        \"subtask_2/f1_macro\": f1_score(y2_true, y2_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_2/f1_micro\": f1_score(y2_true, y2_pred, average=\"micro\", zero_division=0),\n",
    "\n",
    "        \"subtask_3/f1_macro\": f1_score(y3_true, y3_pred, average=\"macro\", zero_division=0),\n",
    "        \"subtask_3/f1_micro\": f1_score(y3_true, y3_pred, average=\"micro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{trial_id}\",\n",
    "    num_train_epochs=15,              # Increase max epochs, let EarlyStopping handle the cut\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,   # 64 might be too stable? 32 adds some noise (good for regularization)\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,                # Add Weight Decay!\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # Must save to load best\n",
    "    load_best_model_at_end=True,      # Automatically load the best checkpoint\n",
    "    metric_for_best_model=\"eval_subtask_1/f1_macro\", # Optimize for the hardest metric or the main task\n",
    "    save_total_limit=2,               # Don't fill disk\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392db14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/410 03:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Subtask 1/accuracy</th>\n",
       "      <th>Subtask 1/f1 Binary</th>\n",
       "      <th>Subtask 1/f1 Macro</th>\n",
       "      <th>Subtask 1/f1 Micro</th>\n",
       "      <th>Subtask 2/f1 Macro</th>\n",
       "      <th>Subtask 2/f1 Micro</th>\n",
       "      <th>Subtask 3/f1 Macro</th>\n",
       "      <th>Subtask 3/f1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.586100</td>\n",
       "      <td>1.024253</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.775465</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.142549</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>0.039816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.039300</td>\n",
       "      <td>0.955948</td>\n",
       "      <td>0.812403</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.812403</td>\n",
       "      <td>0.139667</td>\n",
       "      <td>0.546468</td>\n",
       "      <td>0.198685</td>\n",
       "      <td>0.350515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.845700</td>\n",
       "      <td>1.091587</td>\n",
       "      <td>0.776744</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.766516</td>\n",
       "      <td>0.776744</td>\n",
       "      <td>0.141317</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.272870</td>\n",
       "      <td>0.440678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>1.106296</td>\n",
       "      <td>0.801550</td>\n",
       "      <td>0.716814</td>\n",
       "      <td>0.782035</td>\n",
       "      <td>0.801550</td>\n",
       "      <td>0.139955</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.316205</td>\n",
       "      <td>0.446996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.618200</td>\n",
       "      <td>1.177114</td>\n",
       "      <td>0.812403</td>\n",
       "      <td>0.715294</td>\n",
       "      <td>0.787705</td>\n",
       "      <td>0.812403</td>\n",
       "      <td>0.140618</td>\n",
       "      <td>0.550186</td>\n",
       "      <td>0.299629</td>\n",
       "      <td>0.433526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.528100</td>\n",
       "      <td>1.297464</td>\n",
       "      <td>0.787597</td>\n",
       "      <td>0.704104</td>\n",
       "      <td>0.769222</td>\n",
       "      <td>0.787597</td>\n",
       "      <td>0.137418</td>\n",
       "      <td>0.547038</td>\n",
       "      <td>0.338181</td>\n",
       "      <td>0.472060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>1.389107</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.712695</td>\n",
       "      <td>0.779653</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.140225</td>\n",
       "      <td>0.555160</td>\n",
       "      <td>0.321447</td>\n",
       "      <td>0.467645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>1.401894</td>\n",
       "      <td>0.803101</td>\n",
       "      <td>0.712018</td>\n",
       "      <td>0.781215</td>\n",
       "      <td>0.803101</td>\n",
       "      <td>0.140092</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.336679</td>\n",
       "      <td>0.471404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>1.436210</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.704846</td>\n",
       "      <td>0.772279</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.549645</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>0.484108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>1.433956</td>\n",
       "      <td>0.804651</td>\n",
       "      <td>0.714932</td>\n",
       "      <td>0.783174</td>\n",
       "      <td>0.804651</td>\n",
       "      <td>0.140367</td>\n",
       "      <td>0.553345</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.480207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: \n",
      "subtask_1 accuracy: 0.8047 \n",
      "subtask_1 f1_binary: 0.7149 \n",
      "subtask_1 f1_macro: 0.7832 \n",
      "subtask_1 f1_micro: 0.8047 \n",
      "subtask_2 f1_macro: 0.1404 \n",
      "subtask_2 f1_micro: 0.5533 \n",
      "subtask_3 f1_macro: 0.3439 \n",
      "subtask_3 f1_micro: 0.4802\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\n",
    "    \"Validation Results:\",\n",
    "    f\"\\nsubtask_1 accuracy: {eval_results['eval_subtask_1/accuracy']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_binary: {eval_results['eval_subtask_1/f1_binary']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_macro: {eval_results['eval_subtask_1/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_1 f1_micro: {eval_results['eval_subtask_1/f1_micro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_macro: {eval_results['eval_subtask_2/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_2 f1_micro: {eval_results['eval_subtask_2/f1_micro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_macro: {eval_results['eval_subtask_3/f1_macro']:.4f}\",\n",
    "    f\"\\nsubtask_3 f1_micro: {eval_results['eval_subtask_3/f1_micro']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7c00c",
   "metadata": {},
   "source": [
    "# Log Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4ddef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Experiment results logged to logs.json (trial_id: 0000NG1)\n",
      "  - subtask_1: eng\n",
      "  - subtask_2: eng\n",
      "  - subtask_3: eng\n"
     ]
    }
   ],
   "source": [
    "# Log the experiment results - each subtask separately\n",
    "\n",
    "\n",
    "# Prepare metadata for the experiment\n",
    "experiment_metadata = {\n",
    "    \"approach\": \"MTL_no_gate\",\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,\n",
    "    \"num_types\": NUM_TYPES,\n",
    "    \"num_manifestations\": NUM_MANIFESTATIONS,\n",
    "    \"datasets_merge\": datasets_merge,\n",
    "}\n",
    "\n",
    "# Extract metrics for each subtask\n",
    "subtask_1_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_accuracy\": eval_results.get(\"eval_subtask_1/accuracy\"),\n",
    "    \"eval_f1_binary\": eval_results.get(\"eval_subtask_1/f1_binary\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_1/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_1/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "subtask_2_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_2/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_2/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "subtask_3_results = {\n",
    "    \"eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"eval_f1_macro\": eval_results.get(\"eval_subtask_3/f1_macro\"),\n",
    "    \"eval_f1_micro\": eval_results.get(\"eval_subtask_3/f1_micro\"),\n",
    "    \"eval_runtime\": eval_results.get(\"eval_runtime\"),\n",
    "    \"eval_samples_per_second\": eval_results.get(\"eval_samples_per_second\"),\n",
    "    \"eval_steps_per_second\": eval_results.get(\"eval_steps_per_second\"),\n",
    "    \"epoch\": eval_results.get(\"epoch\")\n",
    "}\n",
    "\n",
    "# To respect pre-existing metadata, update it INSTEAD of replacing it\n",
    "import json\n",
    "\n",
    "# Attempt to load existing logs and merge metadata for this trial if present\n",
    "existing_metadata = {}\n",
    "try:\n",
    "    with open(\"logs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        logs = json.load(f)\n",
    "        if isinstance(logs, dict):\n",
    "            logs = [logs]\n",
    "        for trial in logs:\n",
    "            if trial.get(\"trial_id\") == trial_id and \"metadata\" in trial:\n",
    "                existing_metadata = trial[\"metadata\"].copy()\n",
    "                break\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    pass\n",
    "\n",
    "# Only add/replace model_{lang}, don't overwrite the whole metadata\n",
    "merged_metadata = dict(existing_metadata)\n",
    "merged_metadata.update({\n",
    "    f\"model_{lang}\": model_name,\n",
    "    \"approach\": experiment_metadata[\"approach\"],\n",
    "    \"learning_rate\": experiment_metadata[\"learning_rate\"],\n",
    "    \"num_train_epochs\": experiment_metadata[\"num_train_epochs\"],\n",
    "    \"per_device_train_batch_size\": experiment_metadata[\"per_device_train_batch_size\"],\n",
    "    \"per_device_eval_batch_size\": experiment_metadata[\"per_device_eval_batch_size\"],\n",
    "    \"num_types\": experiment_metadata[\"num_types\"],\n",
    "    \"num_manifestations\": experiment_metadata[\"num_manifestations\"],\n",
    "    \"datasets_merge\": experiment_metadata[\"datasets_merge\"]\n",
    "})\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_1\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_1_results,\n",
    "    metadata=merged_metadata,\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "# Log subtask_2 and subtask_3 using the same trial_id and do not pass metadata to avoid overwrite\n",
    "log(\n",
    "    subtask_name=\"subtask_2\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_2_results,\n",
    "    metadata=None,  # Don't overwrite metadata\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "log(\n",
    "    subtask_name=\"subtask_3\",\n",
    "    language=lang,\n",
    "    eval_results=subtask_3_results,\n",
    "    metadata=None,  # Don't overwrite metadata\n",
    "    trial_id=trial_id\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Experiment results logged to logs.json (trial_id: {trial_id})\")\n",
    "print(f\"  - subtask_1: {lang}\")\n",
    "print(f\"  - subtask_2: {lang}\")\n",
    "print(f\"  - subtask_3: {lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654cdde",
   "metadata": {},
   "source": [
    "# Predict on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe02703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved for all 3 dev sets with Logical Gating applied.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load dev 1 and predict all 3 dev sets\n",
    "dev_1 = pd.read_csv(f\"./dev_phase/subtask1/dev/{lang}.csv\")\n",
    "dev_2 = pd.read_csv(f\"./dev_phase/subtask2/dev/{lang}.csv\")\n",
    "dev_3 = pd.read_csv(f\"./dev_phase/subtask3/dev/{lang}.csv\")\n",
    "\n",
    "# Create dataset from dev 1 texts (all dev sets have same texts)\n",
    "dev_texts = dev_1['text'].tolist()\n",
    "# Dummy labels for prediction\n",
    "dev_dataset = PolarizationDataset(dev_texts, [[0]*12]*len(dev_texts), tokenizer)\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.predict(dev_dataset)\n",
    "logits = predictions.predictions\n",
    "if isinstance(logits, tuple):\n",
    "    logits = logits[0]\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Extract predictions for Subtask 1\n",
    "polarization_preds = preds[:, 0]\n",
    "\n",
    "# --- LOGICAL GATING START ---\n",
    "# Create mask based on Subtask 1 (N, 1)\n",
    "mask = polarization_preds[:, None]\n",
    "\n",
    "# Apply mask to Subtasks 2 and 3\n",
    "# If polarization is 0, these predictions become 0 regardless of model output\n",
    "types_preds = preds[:, 1:1+NUM_TYPES] * mask\n",
    "manifestations_preds = preds[:, 1+NUM_TYPES:] * mask\n",
    "# --- LOGICAL GATING END ---\n",
    "\n",
    "# Create output DataFrames\n",
    "output_1 = dev_1[['id', 'text']].copy()\n",
    "output_1['polarization'] = polarization_preds\n",
    "\n",
    "output_2 = dev_2[['id', 'text']].copy()\n",
    "type_cols = [col for col in dev_2.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(type_cols):\n",
    "    output_2[col] = types_preds[:, i]\n",
    "\n",
    "output_3 = dev_3[['id', 'text']].copy()\n",
    "manifest_cols = [col for col in dev_3.columns if col not in ['id', 'text']]\n",
    "for i, col in enumerate(manifest_cols):\n",
    "    output_3[col] = manifestations_preds[:, i]\n",
    "\n",
    "# Drop the 'text' column before saving\n",
    "output_1 = output_1.drop(columns=['text'])\n",
    "output_2 = output_2.drop(columns=['text'])\n",
    "output_3 = output_3.drop(columns=['text'])\n",
    "\n",
    "# Create dir under results with trial_id\n",
    "os.makedirs(f\"./results/{trial_id}\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_1\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_2\", exist_ok=True)\n",
    "os.makedirs(f\"./results/{trial_id}/subtask_3\", exist_ok=True)\n",
    "\n",
    "# Save predictions to subtask_ directories\n",
    "output_1.to_csv(f\"./results/{trial_id}/subtask_1/pred_{lang}.csv\", index=False)\n",
    "output_2.to_csv(f\"./results/{trial_id}/subtask_2/pred_{lang}.csv\", index=False)\n",
    "output_3.to_csv(f\"./results/{trial_id}/subtask_3/pred_{lang}.csv\", index=False)\n",
    "\n",
    "print(f\"Predictions saved for all 3 dev sets with Logical Gating applied.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
